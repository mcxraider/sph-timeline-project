{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jerryyang/pythonenv/py310/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import sys\n",
    "from sentence_transformers import CrossEncoder\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import json\n",
    "import re\n",
    "from json import JSONDecodeError\n",
    "import torch\n",
    "from torch import nn\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# Import libraries for working with language models and Google Gemini\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Read configuration\n",
    "config_file = '../config.yaml'\n",
    "with open(config_file, 'r') as fin:\n",
    "    config = yaml.safe_load(fin)\n",
    "# end with\n",
    "\n",
    "GROQ_API_KEY = os.environ['GROQ_API_KEY']\n",
    "GEMINI_KEY   = os.environ['GEMINI_KEY']\n",
    "MONGO_URI    = os.environ['MONGO_URI']\n",
    "HF_KEY       = os.environ['HUGGINGFACE_API_KEY']\n",
    "EMBEDDER_API = os.environ[\"HF_EMBEDDING_MODEL_URL\"]\n",
    "\n",
    "chat_model = \"llama3-8b-8192\"\n",
    "genai.configure(api_key=GEMINI_KEY)\n",
    "\n",
    "# Initialise Mongodb client\n",
    "mongo_client = MongoClient(MONGO_URI)\n",
    "\n",
    "# Setup default LLM model\n",
    "default_llm = genai.GenerativeModel('gemini-1.5-flash-latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_database():\n",
    "    \"\"\"\n",
    "    Connects to the MongoDB client, fetches the training and test data collections, \n",
    "    and combines them into a single DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the combined training and test data.\n",
    "    \"\"\"\n",
    "    # Connect to the MongoDB client\n",
    "    try:\n",
    "        db = mongo_client[config[\"database\"][\"name\"]]\n",
    "        train_documents = db[config[\"database\"][\"train_collection\"]].find()\n",
    "        logger.info(\"Train data successfully fetched from MongoDB\\n\")\n",
    "    except Exception as error:\n",
    "        logger.error(f\"Unable to fetch train data from MongoDB. Check your connection to the database...\\nERROR: {error}\\n\")\n",
    "\n",
    "    try:\n",
    "        test_docs = db[config[\"database\"][\"test_collection\"]].find()\n",
    "        logger.info(\"Test data successfully fetched from MongoDB\\n\")\n",
    "    except Exception as error:\n",
    "        logger.error(f\"Unable to fetch test data from MongoDB. Check your connection to the database...\\nERROR: {error}\\n\")\n",
    "\n",
    "    df_train = pd.DataFrame.from_dict(list(train_documents))\n",
    "    df_test = pd.DataFrame.from_dict(list(test_docs))\n",
    "\n",
    "    # Row bind the training and test dataframes\n",
    "    df = pd.concat([df_train, df_test], axis=0).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "# end def\n",
    "\n",
    "def get_text_embeddings(df):\n",
    "    \"\"\"\n",
    "    Extracts and deserializes the text embeddings from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the text data with serialized embeddings.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A NumPy array containing the deserialized embeddings.\n",
    "    \"\"\"\n",
    "    logger.info(\"Fetching embeddings...\\n\")\n",
    "    # Deserializing the embeddings\n",
    "    body_embeddings = np.array(df['embeddings'].apply(ast.literal_eval).tolist())\n",
    "    return body_embeddings\n",
    "# end def\n",
    "\n",
    "def split_data(test_id):\n",
    "    \"\"\"\n",
    "    Splits the data into training and test sets based on the provided test article ID.\n",
    "\n",
    "    Args:\n",
    "        test_id (str or int): The ID of the test article to separate from the training data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the following:\n",
    "            - pd.DataFrame: The combined DataFrame.\n",
    "            - pd.DataFrame: The training data DataFrame.\n",
    "            - pd.DataFrame: The test article DataFrame.\n",
    "    \"\"\"\n",
    "    df = load_database()\n",
    "    test_article = df[df['st_id'] == test_id].reset_index(drop=True)\n",
    "    df_train = df[df[\"st_id\"] != test_id]\n",
    "    return df, df_train, test_article\n",
    "# end def\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_output(output):\n",
    "    \"\"\"\n",
    "    Cleans and parses the JSON output.\n",
    "\n",
    "    Args:\n",
    "        output (str): The JSON string to be cleaned and parsed.\n",
    "\n",
    "    Returns:\n",
    "        dict or list: The parsed JSON object.\n",
    "\n",
    "    Raises:\n",
    "        JSONDecodeError: If the output cannot be parsed as JSON.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        updated_timeline = json.loads(output)\n",
    "        return updated_timeline\n",
    "    except JSONDecodeError:\n",
    "        # Ensure that the string ends with just the open and close list brackets\n",
    "        try:\n",
    "            new_output = re.search(r'\\[[^\\]]*\\]', output).group(0)\n",
    "        except AttributeError:\n",
    "            new_output = re.search(r'\\{.*?\\}', output, re.DOTALL).group(0)  \n",
    "        updated_timeline = json.loads(new_output)\n",
    "        return updated_timeline\n",
    "# end def\n",
    "\n",
    "def clean_sort_timeline(timelines, df_retrieve):\n",
    "    \"\"\"\n",
    "    Cleans and sorts the timeline events.\n",
    "\n",
    "    Args:\n",
    "        timelines (dict): The dictionary containing timelines to be cleaned and sorted.\n",
    "        df_retrieve (pd.DataFrame): DataFrame with the retrieval data.\n",
    "\n",
    "    Returns:\n",
    "        list: A sorted list of timeline events.\n",
    "    \"\"\"\n",
    "    # Expands the list of timelines\n",
    "    generated_timeline = []\n",
    "    for _, line in timelines.items():\n",
    "        # Sieve out the timeline\n",
    "        indiv_timeline = clean_output(line)\n",
    "        if type(indiv_timeline) == list:\n",
    "            # Append each individual event in this timeline from this article\n",
    "            for el in indiv_timeline:\n",
    "                generated_timeline.append(el)\n",
    "        else:\n",
    "            generated_timeline.append(indiv_timeline)\n",
    "        # end if\n",
    "    # end for\n",
    "    \n",
    "    # Code to derive the article id from the article number\n",
    "    unsorted_timeline = []\n",
    "    for event in generated_timeline:\n",
    "        article_index = event[\"Article\"] - 1\n",
    "        event[\"Article_id\"] = df_retrieve.iloc[article_index].id\n",
    "        del event[\"Article\"]\n",
    "        unsorted_timeline.append(event)\n",
    "    # end for\n",
    "    \n",
    "    # Section to remove the parts of the date that LLM were unclear of\n",
    "    timeline = sorted(unsorted_timeline, key=lambda x: x['Date'])\n",
    "    timeline = [event for event in timeline if event['Date'].lower() != 'nan']\n",
    "    \n",
    "    for event in timeline:\n",
    "        date = event['Date']\n",
    "        if date.endswith('-XX-XX'):\n",
    "            event['Date'] = date[:4]\n",
    "        elif date.endswith('-XX'):\n",
    "            event['Date'] = date[:7]\n",
    "        # end if\n",
    "    # end for\n",
    "    return timeline\n",
    "# end def\n",
    "\n",
    "def format_timeline_date(date_str, formats=['%Y', '%Y-%m-%d', '%Y-%m']):\n",
    "    \"\"\"\n",
    "    Formats a date string into a human-readable format.\n",
    "\n",
    "    Args:\n",
    "        date_str (str): The date string to be formatted.\n",
    "        formats (list): A list of date formats to try.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted date string, or the original string if no format matches.\n",
    "    \"\"\"\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str, fmt)\n",
    "            if fmt == '%Y':\n",
    "                return date_obj.strftime('%Y')\n",
    "            elif fmt == '%Y-%m-%d':\n",
    "                return date_obj.strftime('%d %B %Y')\n",
    "            elif fmt == '%Y-%m':\n",
    "                return date_obj.strftime('%B %Y')\n",
    "            # end if\n",
    "        except ValueError:\n",
    "            continue  # If the format doesn't match, try the next one\n",
    "    # end for\n",
    "    \n",
    "    # If no format matches, return the original date string\n",
    "    return date_str\n",
    "# end def\n",
    "\n",
    "def det_generate_timeline(input_data, score_threshold=3, llm=default_llm,safety_settings = {\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "    }):\n",
    "    \"\"\"\n",
    "    Evaluates the necessity of generating a timeline for an article.\n",
    "\n",
    "    Args:\n",
    "        input_data (dict): The input data containing the article title and text.\n",
    "        score_threshold (int, optional): The threshold score to decide if a timeline is needed. Defaults to 3.\n",
    "        llm (object, optional): The language model to use for content generation. Defaults to default_llm.\n",
    "        safety_settings (dict, optional): The safety settings for the language model. Defaults to blocking none for all categories.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the decision on whether a timeline is needed, the score, and the reason if not needed.\n",
    "    \"\"\"\n",
    "    def clean_llm_output(llm_output):\n",
    "        \"\"\"\n",
    "        Cleans the output from the language model.\n",
    "\n",
    "        Args:\n",
    "            llm_output (object): The output from the language model.\n",
    "\n",
    "        Returns:\n",
    "            dict: The cleaned and parsed JSON output.\n",
    "        \"\"\"\n",
    "        text = llm_output.parts[0].text.replace(\"```\", '').replace('json', '')\n",
    "        result = json.loads(text)\n",
    "        return result\n",
    "    \n",
    "    # Initialise Pydantic object to enforce LLM return format\n",
    "    class Event(BaseModel):\n",
    "        score: int = Field(description=\"The need for this article to have a timeline\")\n",
    "        Reason: str = Field(description=\"The main reason for your choice why a timeline is needed or why it is not needed\")\n",
    "    \n",
    "    # Initialise JSON output parser\n",
    "    output_parser = JsonOutputParser(pydantic_object=Event)\n",
    "\n",
    "    # Define the template\n",
    "    template = '''\n",
    "    You are a highly intelligent AI tasked with analyzing articles to determine whether generating a timeline of events leading up to the key event in the article would be beneficial. \n",
    "    Consider the following factors to make your decision:\n",
    "    1. **Significance of the Event**:\n",
    "       - Does the event have a significant impact on a large number of people, industries, or countries?\n",
    "       - Are the potential long-term consequences of the event important?\n",
    "\n",
    "    2. **Controversy or Debate**:\n",
    "       - Is the event highly controversial or has it sparked significant debate?\n",
    "       - Has the event garnered significant media attention and public interest?\n",
    "\n",
    "    3. **Complexity**:\n",
    "       - Does the event involve multiple factors, stakeholders, or causes that make it complex?\n",
    "       - Does the event have deep historical roots or is it the culmination of long-term developments?\n",
    "\n",
    "    4. **Personal Relevance**:\n",
    "       - Does the event directly affect the reader or their community?\n",
    "       - Is the event of particular interest to the reader due to economic implications, political affiliations, or social issues?\n",
    "\n",
    "    5. **Educational Purposes**:\n",
    "       - Would a timeline provide valuable learning or research information?\n",
    "\n",
    "    Here is the information for the article:\n",
    "    Title:{title}\n",
    "    Text: {text}\n",
    "\n",
    "    Based on the factors above, decide whether generating a timeline of events leading up to the key event in this article would be beneficial. \n",
    "    Your answer will include the need for this article to have a timeline with a score 1 - 5, 1 means unnecessary, 5 means necessary. It will also include the main reason for your choice.\n",
    "    {format_instructions}    \n",
    "    ANSWER:\n",
    "    '''\n",
    "    \n",
    "    # Get the prompt template format instructions\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    # Create the prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"text\", \"title\"],\n",
    "        partial_variables={\"format_instructions\": format_instructions},\n",
    "        template=template,\n",
    "    )\n",
    "\n",
    "    # Define the headline and body from input data\n",
    "    headline = input_data[\"Title\"]\n",
    "    body = input_data[\"Text\"]\n",
    "\n",
    "    # Format the prompt\n",
    "    final_prompt = prompt.format(title=headline, text=body)\n",
    "\n",
    "    # Generate content using the language model\n",
    "    response = llm.generate_content(\n",
    "        final_prompt,\n",
    "        safety_settings=safety_settings,\n",
    "    )\n",
    "    final_response = clean_llm_output(response)\n",
    "\n",
    "    score = final_response['score']\n",
    "    \n",
    "    # If LLM approves\n",
    "    if score >= score_threshold:\n",
    "        logger.info(\"Timeline is appropriate for this chosen article.\\n\")\n",
    "        return {\"det\": True, \"score\": final_response['score'], \"reason\": None}\n",
    "    else:\n",
    "        logger.info(\"A timeline for this article is not required. \\n\")\n",
    "        for part in final_response['Reason'].replace(\". \", \".\").split(\". \"):\n",
    "            logger.info(f\"{part}\\n\")\n",
    "        # end for\n",
    "        \n",
    "        logger.info(\"Hence I gave this a required timeline score of \" + str(score))\n",
    "        reason = (\n",
    "            \"A timeline for this article is not required. \\n\"\n",
    "            + \"\\n\" + final_response['Reason'] + \"\\n\"\n",
    "            + \"\\nHence this timeline received a necessity score of \"\n",
    "            + str(final_response['score']) + \"\\n\"\n",
    "        )\n",
    "        return {\"det\": False, \"score\": score, \"reason\": reason}\n",
    "# end def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_similarity(test_embedding, db_embedding):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between the embeddings of a test article and a database article.\n",
    "\n",
    "    Args:\n",
    "        test_embedding (list or np.ndarray): The embedding of the test article.\n",
    "        db_embedding (list or np.ndarray): The embedding of the database article.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The cosine similarity score between the two embeddings.\n",
    "    \"\"\"\n",
    "    cos_sim = nn.CosineSimilarity(dim=0)\n",
    "    \n",
    "    # Convert embeddings to tensors\n",
    "    similarity_score = cos_sim(torch.tensor(test_embedding), torch.tensor(db_embedding))\n",
    "    return similarity_score\n",
    "# end def\n",
    "\n",
    "def articles_ranked_by_text(test_article, database):\n",
    "    \"\"\"\n",
    "    Finds the top 20 most similar articles based on their text embeddings.\n",
    "\n",
    "    Args:\n",
    "        test_article (pd.DataFrame): The test article containing the 'embeddings' column.\n",
    "        database (pd.DataFrame): The database of articles.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of the top 20 most similar articles.\n",
    "    \"\"\"\n",
    "    logger.info(\"Computing similarities between article texts...\")\n",
    "    test_embedding = test_article['embeddings'].apply(ast.literal_eval)[0]\n",
    "\n",
    "    article_collection = []\n",
    "    for i in trange(len(database)):\n",
    "        article = {\n",
    "            'id': database.iloc[i]['st_id'],\n",
    "            'Title': database.iloc[i]['Title'],\n",
    "            'Text': database.iloc[i]['Text'],\n",
    "            'Date': database.iloc[i]['Publication_date'],\n",
    "            'Article_URL': database.iloc[i]['article_url']\n",
    "        }\n",
    "        article_embedding = pd.DataFrame(database.iloc[i]).loc['embeddings'].apply(ast.literal_eval)[i]\n",
    "        article['cosine_score'] = get_text_similarity(test_embedding, article_embedding)\n",
    "        article_collection.append(article)\n",
    "    # end for\n",
    "    \n",
    "    # Sort by cosine similarity in descending order\n",
    "    article_collection.sort(key=lambda x: x['cosine_score'], reverse=True)\n",
    "    \n",
    "    # Return the top 21 most similar articles\n",
    "    collection = article_collection[:21]\n",
    "    \n",
    "    # Remove the first article as it is the test article to get the top 20 articles\n",
    "    collection.pop(0)\n",
    "    return collection\n",
    "# end def\n",
    "\n",
    "def re_rank_articles(unique_articles, test_article, cross_encoder_model=\"cross-encoder/ms-marco-TinyBERT-L-2-v2\", top_k=6):\n",
    "    \"\"\"\n",
    "    Re-ranks articles based on similarity derived from a cross encoder.\n",
    "\n",
    "    Args:\n",
    "        unique_articles (list): The list of unique articles to be re-ranked.\n",
    "        test_article (pd.DataFrame): The test article.\n",
    "        cross_encoder_model (str, optional): The cross encoder model to use. Defaults to \"cross-encoder/ms-marco-TinyBERT-L-2-v2\".\n",
    "        top_k (int, optional): The number of top articles to return. Defaults to 6.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of the top re-ranked articles.\n",
    "    \"\"\"\n",
    "    cross_encoder = CrossEncoder(cross_encoder_model, max_length=512, device=\"cpu\")\n",
    "    test_article_text = test_article['Text'][0]\n",
    "    \n",
    "    # Format the timeline header and article for cross encoder\n",
    "    unranked_articles = [(test_article_text, doc['Text']) for doc in unique_articles]\n",
    "    \n",
    "    # Predict similarity scores\n",
    "    similarity_scores = cross_encoder.predict(unranked_articles).tolist()\n",
    "    \n",
    "    # Assign the list of similarity scores to the individual articles\n",
    "    for i in range(len(unique_articles)):\n",
    "        unique_articles[i]['reranked_score'] = similarity_scores[i]\n",
    "    # end for\n",
    "    \n",
    "    combined_articles = [article for article in unique_articles if 'reranked_score' in article]\n",
    "    best_articles = sorted(combined_articles, key=lambda x: x['reranked_score'], reverse=True)[:top_k]\n",
    "\n",
    "    return best_articles\n",
    "# end def\n",
    "\n",
    "def get_article_dict(test_article, best_articles, df):\n",
    "    \"\"\"\n",
    "    Generates the clustering properties of the test article.\n",
    "\n",
    "    Args:\n",
    "        test_article (pd.DataFrame): The test article DataFrame.\n",
    "        best_articles (list): A list of the best articles.\n",
    "        df (pd.DataFrame): The DataFrame containing all articles.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the clustering properties of the test article.\n",
    "              If there are insufficient relevant articles, returns \"generate_similar_error\".\n",
    "    \"\"\"\n",
    "    # Get the DataFrame IDs of the best articles\n",
    "    ids = [article['id'] for article in best_articles]\n",
    "    \n",
    "    similar_indexes = []\n",
    "    for idx, row in df.iterrows():\n",
    "        if row['st_id'] in ids:\n",
    "            similar_indexes.append(idx)\n",
    "        # end if\n",
    "        if len(similar_indexes) == len(ids):\n",
    "            break\n",
    "        # end if\n",
    "    # end for\n",
    "\n",
    "    similar_articles_dict = {\n",
    "        'Title': test_article['Title'][0],\n",
    "        'indexes': similar_indexes,\n",
    "        'Text': test_article['Text'][0],\n",
    "    }\n",
    "    if len(similar_articles_dict) < 2:\n",
    "        logger.info(\"There are insufficient relevant articles to construct a meaningful timeline. ... Exiting execution now\\n\")\n",
    "        return \"generate_similar_error\"\n",
    "    # end if\n",
    "    \n",
    "    return similar_articles_dict\n",
    "# end def\n",
    "\n",
    "def get_predicted_cluster(dataframe, test_id):\n",
    "    \"\"\"\n",
    "    Generates the predicted cluster label for the test article.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The DataFrame containing all articles.\n",
    "        test_id (str or int): The ID of the test article.\n",
    "\n",
    "    Returns:\n",
    "        int or str: The predicted cluster label for the test article.\n",
    "    \"\"\"\n",
    "    test_article = dataframe[dataframe['st_id'] == test_id].reset_index(drop=True)\n",
    "    predicted_cluster = test_article['Cluster_label'][0]\n",
    "    return predicted_cluster\n",
    "# end def\n",
    "\n",
    "def find_similar_articles(pre_computed_cluster, df, test_article, max_d, test_id):\n",
    "    \"\"\"\n",
    "    Performs hierarchical clustering and re-ranking to find similar articles.\n",
    "\n",
    "    Args:\n",
    "        pre_computed_cluster (np.ndarray): The pre-computed cluster linkage matrix.\n",
    "        df (pd.DataFrame): The DataFrame containing all articles.\n",
    "        test_article (pd.DataFrame): The test article DataFrame.\n",
    "        max_d (float): The maximum distance threshold for clustering.\n",
    "        test_id (str or int): The ID of the test article.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the similar articles and their clustering properties.\n",
    "    \"\"\"\n",
    "    cluster_labels = fcluster(pre_computed_cluster, max_d, criterion='distance')\n",
    "    df['Cluster_label'] = cluster_labels\n",
    "    \n",
    "    # Get the predicted cluster for the test article\n",
    "    predicted_cluster = get_predicted_cluster(df, test_id)\n",
    "    cluster_df = df[df['Cluster_label'] == predicted_cluster].reset_index(drop=True)\n",
    "    \n",
    "    # Get similar articles based on text embeddings\n",
    "    similar_articles_by_text_embedding = articles_ranked_by_text(test_article, cluster_df)\n",
    "    \n",
    "    # Re-rank the top 6 similar articles\n",
    "    best_articles = re_rank_articles(similar_articles_by_text_embedding, test_article)\n",
    "    \n",
    "    # Get the clustering properties for the test article\n",
    "    similar_articles = get_article_dict(test_article, best_articles, df)\n",
    "\n",
    "    return similar_articles\n",
    "# end def\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_sort_timeline(similar_articles_dict, df_train, df_test, llm=default_llm):\n",
    "    \"\"\"\n",
    "    Generates and sorts a timeline of events for an article using a language model.\n",
    "\n",
    "    Args:\n",
    "        similar_articles_dict (dict): Dictionary containing similar articles and their indexes.\n",
    "        df_train (pd.DataFrame): DataFrame containing the training data.\n",
    "        df_test (pd.DataFrame): DataFrame containing the test data.\n",
    "        llm (object, optional): The language model to use for content generation. Defaults to default_llm.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of sorted timeline events.\n",
    "        pd.DataFrame: The DataFrame containing the retrieved articles.\n",
    "    \"\"\"\n",
    "    # Define the event model\n",
    "    class Event(BaseModel):\n",
    "        Date: str = Field(description=\"The date of the event in YYYY-MM-DD format\")\n",
    "        Event: str = Field(description=\"A detailed description of the important event\")\n",
    "        Article: int = Field(description=\"The article number from which the event was extracted\")\n",
    "\n",
    "    output_parser = JsonOutputParser(pydantic_object=Event)\n",
    "\n",
    "    # Get the prompt template format instructions\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    # Define the template\n",
    "    template = '''\n",
    "    Given an article, containing a publication date, title, and content, your task is to construct a detailed timeline of events leading up to the main event described in the article.\n",
    "    Begin by thoroughly analyzing the title, content, and publication date of the article to understand the main event in the article. \n",
    "    The dates are represented in YYYY-MM-DD format. Identify events, context, and any time references such as \"last week,\" \"last month,\" or specific dates. \n",
    "    The article could contain more than one key event. \n",
    "    If the article does not provide a publication date or any events leading up to the main event, return NAN in the Date field, and 0 in the Article field.\n",
    "\n",
    "    Construct the Timeline:\n",
    "    Chronological Order: Organize the events chronologically, using the publication dates and time references within the articles.\n",
    "    Detailed Descriptions: Provide detailed descriptions of each event, explaining how it relates to the main event of the first article.\n",
    "    Contextual Links: Use information from the articles to link events together logically and coherently.\n",
    "    Handle Ambiguities: If an article uses ambiguous time references, infer the date based on the publication date of the article and provide a clear rationale for your inference.\n",
    "\n",
    "    Contextual Links:\n",
    "    External Influences: Mention any external influences (e.g., global conflicts, economic trends, scientific discoveries) that might have indirectly affected the events.\n",
    "    Internal Issues: Highlight any internal issues or developments (e.g., political changes, organizational restructuring, societal movements) within the entities involved that might have impacted the events.\n",
    "    Efforts for Improvement: Note any indications of efforts to improve the situation (e.g., policy changes, strategic initiatives, collaborative projects) despite existing challenges.\n",
    "\n",
    "    Be as thorough and precise as possible, ensuring the timeline accurately reflects the sequence and context of events leading to the main event.\n",
    "\n",
    "    Article:\n",
    "    {text}\n",
    "\n",
    "    {format_instructions}\n",
    "    Check and ensure again that the output follows the format instructions above very strictly.\n",
    "    '''\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        partial_variables={\"format_instructions\": format_instructions},\n",
    "        template=template\n",
    "    )\n",
    "    \n",
    "    def generate_individual_timeline(date_text_triples):\n",
    "        \"\"\"\n",
    "        Generates an individual timeline for an article.\n",
    "\n",
    "        Args:\n",
    "            date_text_triples (tuple): A tuple containing article number, publication date, and article text.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated timeline.\n",
    "        \"\"\"\n",
    "        article_details = f'Article {date_text_triples[0]}: Publication date: {date_text_triples[1]} Article Text: {date_text_triples[2]}'\n",
    "        final_prompt = prompt.format(text=article_details)\n",
    "        response = llm.generate_content(\n",
    "            final_prompt,\n",
    "            safety_settings={\n",
    "                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, \n",
    "                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Check if Model returns correct format \n",
    "        if '[' in response.parts[0].text or '{' in response.parts[0].text:\n",
    "            result = response.parts[0].text\n",
    "        else:\n",
    "            retry_response = llm.generate_content(\n",
    "                final_prompt,\n",
    "                safety_settings={\n",
    "                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, \n",
    "                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "                }\n",
    "            )\n",
    "            try:\n",
    "                result = retry_response.parts[0].text\n",
    "            except ValueError:\n",
    "                logger.error(\"ERROR: There were issues with the generation of the timeline. The timeline could not be generated\")\n",
    "                return\n",
    "        return result\n",
    "    # end def\n",
    "    \n",
    "    def process_articles(df_train, similar_articles_dict, df_test):\n",
    "        \"\"\"\n",
    "        Processes the articles to retrieve and concatenate dataframes.\n",
    "\n",
    "        Args:\n",
    "            df_train (pd.DataFrame): The training data DataFrame.\n",
    "            similar_articles_dict (dict): Dictionary containing similar articles and their indexes.\n",
    "            df_test (pd.DataFrame): The test data DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary of generated timelines.\n",
    "            pd.DataFrame: DataFrame of retrieved articles.\n",
    "        \"\"\"\n",
    "        # Retrieve and concatenate dataframes\n",
    "        df_retrieve = pd.concat([df_train.loc[similar_articles_dict['indexes']], df_test], axis=0).iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "        # Prepare metadata for LLM\n",
    "        date_text_triples = list(zip(range(1, len(df_retrieve) + 1), df_retrieve['Publication_date'].tolist(), df_retrieve['combined'].tolist()))\n",
    "\n",
    "        # Generate timelines using ThreadPoolExecutor\n",
    "        dict_of_timelines = {}\n",
    "        try:\n",
    "            with ThreadPoolExecutor(max_workers=len(date_text_triples)) as executor:\n",
    "                futures = {executor.submit(generate_individual_timeline, triple): i for i, triple in enumerate(date_text_triples)}\n",
    "                for future in as_completed(futures):\n",
    "                    dict_of_timelines[futures[future]] = future.result()\n",
    "        except IndexError:\n",
    "            with ThreadPoolExecutor(max_workers=len(date_text_triples)) as executor:\n",
    "                futures = {executor.submit(generate_individual_timeline, triple): i for i, triple in enumerate(date_text_triples)}\n",
    "                for future in as_completed(futures):\n",
    "                    dict_of_timelines[futures[future]] = future.result()\n",
    "\n",
    "        return dict_of_timelines, df_retrieve\n",
    "    # end def\n",
    "    \n",
    "    # Process the articles to generate timelines\n",
    "    timeline_dic, df_retrieve = process_articles(df_train, similar_articles_dict, df_test)\n",
    "    logger.info(\"The first timeline has been generated\\n\")\n",
    "    \n",
    "    # Clean and sort the generated timelines\n",
    "    generated_timeline = []\n",
    "    for _, line in timeline_dic.items():\n",
    "        indiv_timeline = clean_output(line)\n",
    "        if isinstance(indiv_timeline, list):\n",
    "            for el in indiv_timeline:\n",
    "                generated_timeline.append(el)\n",
    "        else:\n",
    "            generated_timeline.append(indiv_timeline)\n",
    "    \n",
    "    unsorted_timeline = []\n",
    "    for event in generated_timeline:\n",
    "        article_index = event[\"Article\"] - 1\n",
    "        event[\"Article_id\"] = df_retrieve.iloc[article_index].st_id\n",
    "    for event in generated_timeline:\n",
    "        del event[\"Article\"]\n",
    "        unsorted_timeline.append(event)  \n",
    "    \n",
    "    timeline = sorted(unsorted_timeline, key=lambda x: x['Date'])\n",
    "    finished_timeline = [event for event in timeline if event['Date'].lower() != 'nan']\n",
    "    for i in range(len(finished_timeline)):\n",
    "        date = finished_timeline[i]['Date']\n",
    "        if date.endswith('-XX-XX') or date.endswith('00-00'):\n",
    "            finished_timeline[i]['Date'] = date[:4]\n",
    "        elif date.endswith('-XX') or date.endswith('00'):\n",
    "            finished_timeline[i]['Date'] = date[:7]\n",
    "    \n",
    "    return finished_timeline, df_retrieve\n",
    "# end def\n",
    "\n",
    "def reduce_by_date(timeline_list, llm=default_llm):\n",
    "    \"\"\"\n",
    "    Takes in a list of events in one day and returns a list of dicts for events in that day,\n",
    "    filtering out duplicated or redundant events.\n",
    "\n",
    "    Args:\n",
    "        timeline_list (list): List of events for a particular day.\n",
    "        llm (object, optional): The language model to use for content generation. Defaults to default_llm.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries with filtered and possibly merged events.\n",
    "    \"\"\"\n",
    "    def extract_content_from_json(string):\n",
    "        \"\"\"\n",
    "        Extracts content from a JSON string.\n",
    "\n",
    "        Args:\n",
    "            string (str): The JSON string to extract content from.\n",
    "\n",
    "        Returns:\n",
    "            list or None: The extracted JSON data or None if extraction fails.\n",
    "        \"\"\"\n",
    "        # Use a regular expression to find the content within the first and last square brackets\n",
    "        match = re.search(r'\\[.*\\]', string, re.DOTALL)\n",
    "        \n",
    "        if match:\n",
    "            json_content = match.group(0)\n",
    "            try:\n",
    "                # Load the extracted content into a JSON object\n",
    "                json_data = json.loads(json_content)\n",
    "                return json_data\n",
    "            except json.JSONDecodeError as e:\n",
    "                logger.error(\"Failed to decode JSON:\", e)\n",
    "                return None\n",
    "        else:\n",
    "            logger.info(\"No valid JSON content found.\")\n",
    "            return None\n",
    "        # end if\n",
    "    # end def\n",
    "    \n",
    "    timeline_string = json.dumps(timeline_list)\n",
    "    \n",
    "    # Define the event model for structured LLM output\n",
    "    class Event(BaseModel):\n",
    "        Event: str = Field(description=\"A detailed description of the event\")\n",
    "        Article_id: list = Field(description=\"The article id(s) from which the events were extracted\")\n",
    "\n",
    "    parser = JsonOutputParser(pydantic_object=Event)\n",
    "\n",
    "    # Define the template for the prompt\n",
    "    template = '''\n",
    "    You are a news article editor tasked with simplifying a section of a timeline of events on the same day. \n",
    "    Given this snippet a timeline, filter out duplicated events. \n",
    "    IF events convey the same overall meaning, I want you to merge these events into one event to avoid redundancy, and add the article ids to a list. \n",
    "    However, the events are all different, do not combine them, I want you to return it as it is, however, follow the specified format instructions below. \n",
    "    Furthermore, if the event is not considered to be an event worthy of an audience reading the timeline, do not include it.\n",
    "    Take your time and evaluate the timeline slowly to make your decision.\n",
    "\n",
    "    Timeline snippet:\n",
    "    {text}\n",
    "\n",
    "    {format_instructions}\n",
    "    Ensure that the format follows the example output format strictly before returning the output.'''\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template=template,\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "    )\n",
    "    \n",
    "    final_prompt = prompt.format(text=timeline_string)\n",
    "    \n",
    "    response = llm.generate_content(\n",
    "        final_prompt,\n",
    "        safety_settings={\n",
    "            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, \n",
    "            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    data = extract_content_from_json(response.parts[0].text)\n",
    "    \n",
    "    # Limit the number of article links to 2 links\n",
    "    link_limit = 2\n",
    "    \n",
    "    for event_dic in data:\n",
    "        article_id_list = event_dic['Article_id']\n",
    "        if len(article_id_list) > link_limit:\n",
    "            shortened_ids = article_id_list[:link_limit]\n",
    "            event_dic['Article_id'] = shortened_ids\n",
    "        # end if\n",
    "    # end for\n",
    "    \n",
    "    return data\n",
    "# end def\n",
    "\n",
    "def first_timeline_enhancement(timeline):\n",
    "    \"\"\"\n",
    "    Takes in a timeline in list format and enhances it by combining events with the same date.\n",
    "\n",
    "    Args:\n",
    "        timeline (list): A list of events with their dates and descriptions.\n",
    "\n",
    "    Returns:\n",
    "        list: An enhanced timeline with combined and filtered events.\n",
    "    \"\"\"\n",
    "    # Get the unique dates seen in the timeline\n",
    "    unique_dates = sorted(list(set([event['Date'] for event in timeline])))\n",
    "    \n",
    "    # Combine the events that have the same date\n",
    "    dic = {}\n",
    "    for date in unique_dates:\n",
    "        dic[date] = [{'Event': event['Event'], 'Article_id': event['Article_id']} for event in timeline if event['Date'] == date]\n",
    "    # end for\n",
    "    \n",
    "    # Combine the events that have the same date and (only if they have the same event)\n",
    "    new_timeline = {}\n",
    "    for date, snippet in dic.items():\n",
    "        if len(snippet) == 1:\n",
    "            new_timeline[date] = snippet\n",
    "        else:\n",
    "            new_snippet = reduce_by_date(snippet)\n",
    "            new_timeline[date] = new_snippet\n",
    "        # end if\n",
    "    # end for\n",
    "    \n",
    "    enhanced_timeline = []\n",
    "    for date, events in new_timeline.items():\n",
    "        for event in events:\n",
    "            new_event = {\n",
    "                'Date': date,\n",
    "                'Event': event['Event'],\n",
    "                'Article_id': event['Article_id'] if isinstance(event['Article_id'], list) else [event['Article_id']]\n",
    "            }\n",
    "            enhanced_timeline.append(new_event)\n",
    "        # end for\n",
    "    # end for\n",
    "    \n",
    "    return enhanced_timeline\n",
    "# end def\n",
    "\n",
    "def pair_article_urls(enhanced_timeline, df_retrieve):\n",
    "    \"\"\"\n",
    "    Pairs article URLs and titles with events in the enhanced timeline.\n",
    "\n",
    "    Args:\n",
    "        enhanced_timeline (list): List of events with article IDs.\n",
    "        df_retrieve (pd.DataFrame): DataFrame containing article details.\n",
    "\n",
    "    Returns:\n",
    "        list: Enhanced timeline with article URLs and titles.\n",
    "    \"\"\"\n",
    "    def edit_timeline(timeline):\n",
    "        \"\"\"\n",
    "        Formats the dates in the timeline.\n",
    "\n",
    "        Args:\n",
    "            timeline (list): List of events with dates.\n",
    "\n",
    "        Returns:\n",
    "            list: Timeline with formatted dates.\n",
    "        \"\"\"\n",
    "        for event in timeline:\n",
    "            new_date = format_timeline_date(event['Date'])\n",
    "            event['Date'] = new_date\n",
    "        # end for\n",
    "        return timeline\n",
    "    # end def\n",
    "\n",
    "    edited_timeline = edit_timeline(enhanced_timeline)\n",
    "\n",
    "    # Add URLs and titles to events in the timeline\n",
    "    for event in edited_timeline:\n",
    "        id_list = event['Article_id']\n",
    "        url_title_pairs = []\n",
    "        \n",
    "        for i in range(len(id_list)):\n",
    "            id = id_list[i]\n",
    "            url = df_retrieve[df_retrieve['st_id'] == id]['article_url'].values[0]\n",
    "            title = df_retrieve[df_retrieve['st_id'] == id]['Title'].values[0]\n",
    "            url_title_pairs.append({'url': url, 'title': title})\n",
    "        # end for\n",
    "        \n",
    "        event['Article_URL'] = url_title_pairs\n",
    "        event.pop('Article_id')\n",
    "    # end for\n",
    "    return edited_timeline\n",
    "# end def\n",
    "\n",
    "def articles_needing_summary(timeline):\n",
    "    \"\"\"\n",
    "    Identifies events in the timeline that need to be summarized.\n",
    "\n",
    "    Args:\n",
    "        timeline (list): List of events in the timeline.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of events that need to be summarized, indexed by their position in the timeline.\n",
    "    \"\"\"\n",
    "    def get_num_words(event_str):\n",
    "        \"\"\"\n",
    "        Counts the number of words in an event description.\n",
    "\n",
    "        Args:\n",
    "            event_str (str): Event description.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of words in the event description.\n",
    "        \"\"\"\n",
    "        ls = event_str.split()\n",
    "        return len(ls)\n",
    "    # end def\n",
    "\n",
    "    need_summary_timeline = []\n",
    "    for i in range(len(timeline)):\n",
    "        # If the number of words in the event is more than 20\n",
    "        if get_num_words(timeline[i]['Event']) > 20:\n",
    "            need_summary_timeline.append((i, timeline[i]))\n",
    "        # end if\n",
    "    # end for\n",
    "    \n",
    "    # Extract events needing summary\n",
    "    events = {}\n",
    "    for i in range(len(need_summary_timeline)):\n",
    "        events[need_summary_timeline[i][0]] = need_summary_timeline[i][1]\n",
    "    # end for\n",
    "    return events\n",
    "# end def\n",
    "\n",
    "def generate_event_summary(events_ls):\n",
    "    \"\"\"\n",
    "    Summarizes events in the given list of events.\n",
    "\n",
    "    Args:\n",
    "        events_ls (list): List of events to be summarized.\n",
    "\n",
    "    Returns:\n",
    "        list: List of summarized events.\n",
    "    \"\"\"\n",
    "    # Define summarized event output\n",
    "    class summarized_event(BaseModel):\n",
    "        Event: str = Field(description=\"Event in a timeline\")\n",
    "        Event_Summary: str = Field(description=\"Short Summary of event\")\n",
    "    \n",
    "    parser = JsonOutputParser(pydantic_object=summarized_event)\n",
    "\n",
    "    chat = ChatGroq(temperature=0, model_name=chat_model)\n",
    "    \n",
    "    # Template for the prompt\n",
    "    template = '''\n",
    "You are a news article editor.\n",
    "Given a list of events from a timeline, you are tasked to provide a short summary of these series of events. \n",
    "For each event, you should return the event, and the summary.\n",
    "\n",
    "Series of events:\n",
    "{text}\n",
    "\n",
    "{format_instructions}\n",
    "    '''\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"text\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "    \n",
    "    chain = prompt | chat | parser\n",
    "    \n",
    "    event_str = json.dumps(events_ls)\n",
    "    result = chain.invoke({\"text\": event_str})\n",
    "    if isinstance(result, list):\n",
    "        return result\n",
    "    else:\n",
    "        cleaned_result = clean_output(result)\n",
    "    # end if\n",
    "    return cleaned_result\n",
    "# end def\n",
    "\n",
    "def merge_event_summaries(events_ls, llm_answer, timeline, need_summary_timeline):\n",
    "    \"\"\"\n",
    "    Merges summarized events back into the original timeline.\n",
    "\n",
    "    Args:\n",
    "        events_ls (list): List of events needing summary.\n",
    "        llm_answer (list): List of summarized events from the language model.\n",
    "        timeline (list): The original timeline of events.\n",
    "        need_summary_timeline (dict): Dictionary of events that need summarization.\n",
    "\n",
    "    Returns:\n",
    "        list: The updated timeline with summarized events merged in.\n",
    "    \"\"\"\n",
    "    # Check if the length of the summarized events matches the length of events needing summary\n",
    "    if len(llm_answer) != len(need_summary_timeline):\n",
    "        logger.info(\"Groq had an error where timeline summary output length not equal to input but trying to resolve\")\n",
    "        llm_answer = generate_event_summary(events_ls)\n",
    "        logger.info(len(llm_answer) == len(events_ls))\n",
    "    # end if\n",
    "    \n",
    "    # Merge the summarized events back into the need_summary_timeline\n",
    "    i = 0\n",
    "    for k, v in need_summary_timeline.items():\n",
    "        need_summary_timeline[k]['Event_Summary'] = llm_answer[i]['Event_Summary']\n",
    "        i += 1\n",
    "    # end for\n",
    "        \n",
    "    # Update the original timeline with the summarized events\n",
    "    for i in range(len(timeline)):\n",
    "        if i in need_summary_timeline:\n",
    "            timeline[i] = need_summary_timeline[i]\n",
    "        # end if\n",
    "    # end for\n",
    "    \n",
    "    return timeline\n",
    "# end def\n",
    "\n",
    "def second_timeline_enhancement(timeline):\n",
    "    \"\"\"\n",
    "    Enhances the timeline by summarizing lengthy events.\n",
    "\n",
    "    Args:\n",
    "        timeline (list): The original timeline of events.\n",
    "\n",
    "    Returns:\n",
    "        list: The enhanced timeline with summarized events.\n",
    "    \"\"\"\n",
    "    # Identify events that need summarization\n",
    "    need_summary_timeline = articles_needing_summary(timeline)\n",
    "    events_ls = [event['Event'] for _, event in need_summary_timeline.items()]\n",
    "    \n",
    "    # Generate summaries for the identified events\n",
    "    summaries = generate_event_summary(events_ls)\n",
    "    \n",
    "    # Merge the summaries back into the timeline\n",
    "    final_timeline = merge_event_summaries(events_ls, summaries, timeline, need_summary_timeline)\n",
    "    \n",
    "    return final_timeline\n",
    "# end def\n",
    "\n",
    "def generate_save_timeline(similar_articles, df_train, df_test):\n",
    "    \"\"\"\n",
    "    Generates and saves the enhanced timeline.\n",
    "\n",
    "    Args:\n",
    "        similar_articles (dict): Dictionary containing similar articles and their indexes.\n",
    "        df_train (pd.DataFrame): DataFrame containing the training data.\n",
    "        df_test (pd.DataFrame): DataFrame containing the test data.\n",
    "\n",
    "    Returns:\n",
    "        list or str: The final enhanced timeline or an error message.\n",
    "    \"\"\"\n",
    "    if similar_articles == \"generate_similar_error\":\n",
    "        return \"Error02\"\n",
    "    \n",
    "    # Generate and sort the initial timeline\n",
    "    generated_timeline, df_retrieve = generate_and_sort_timeline(similar_articles, df_train, df_test)\n",
    "    logger.info(\"Proceeding to Stage 1/2 of enhancement...\\n\")\n",
    "    \n",
    "    # Perform the first enhancement on the timeline\n",
    "    first_enhanced_timeline = first_timeline_enhancement(generated_timeline)\n",
    "    \n",
    "    # Pair article URLs with the enhanced timeline\n",
    "    second_enhanced_timeline = pair_article_urls(first_enhanced_timeline, df_retrieve)\n",
    "    logger.info(\"Proceeding to Stage 2/2 of enhancement...\\n\")\n",
    "    \n",
    "    # Perform the second enhancement on the timeline\n",
    "    final_timeline = second_timeline_enhancement(second_enhanced_timeline)\n",
    "    logger.info(\"Timeline enhanced..\\n\")\n",
    "    \n",
    "    return final_timeline\n",
    "# end def\n",
    "\n",
    "def generate_timeline_header(final_timeline):\n",
    "    \"\"\"\n",
    "    Generates a header for the timeline based on the events.\n",
    "\n",
    "    Args:\n",
    "        final_timeline (list): The final enhanced timeline of events.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated header for the timeline.\n",
    "    \"\"\"\n",
    "    # Extract event titles for headline generation\n",
    "    event_titles = [event['Event_Summary'] if 'Event_Summary' in event else event['Event'] for event in final_timeline ]\n",
    "    \n",
    "    # Define timeline heading pydantic object\n",
    "    class timeline_heading(BaseModel):\n",
    "        timeline_heading: str = Field(description=\"Heading of timeline\")\n",
    "    \n",
    "    parser = JsonOutputParser(pydantic_object=timeline_heading)\n",
    "\n",
    "    chat = ChatGroq(temperature=0, model_name=chat_model)\n",
    "    \n",
    "    # Template for the prompt\n",
    "    template = '''\n",
    "You are a news article editor.\n",
    "Given a list of events from a timeline, analyze the series of events which are displayed in chronological order. \n",
    "Form a concise and accurate heading for this timeline.\n",
    "\n",
    "Series of events:\n",
    "{text}\n",
    "\n",
    "{format_instructions}\n",
    "    '''\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"text\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "    \n",
    "    chain = prompt | chat | parser\n",
    "    \n",
    "    # Invoke the language model to generate the timeline header\n",
    "    result = chain.invoke({\"text\": event_titles})\n",
    "    \n",
    "    return result\n",
    "# end def\n",
    "\n",
    "def main_hierarchical(df, test_article, df_train, test_id):\n",
    "    \"\"\"\n",
    "    Performs hierarchical clustering and generates a timeline if necessary.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing all articles.\n",
    "        test_article (pd.DataFrame): DataFrame containing the test article.\n",
    "        df_train (pd.DataFrame): DataFrame containing the training data.\n",
    "        test_id (str): The ID of the test article.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the final timeline and a failure reason if applicable.\n",
    "    \"\"\"\n",
    "    embeddings = get_text_embeddings(df)\n",
    "    max_d = 0.58\n",
    "    \n",
    "    # Precomputed hierarchical clustering\n",
    "    Z = linkage(embeddings, method='average', metric='cosine')   \n",
    "    \n",
    "    # Check if a timeline is necessary for the test article\n",
    "    timeline_necessary = det_generate_timeline(test_article)\n",
    "    if timeline_necessary[\"det\"]:\n",
    "        # Find similar articles using clustering and reranking\n",
    "        similar_articles = find_similar_articles(Z, df, test_article, max_d, test_id)\n",
    "        # Generate the final timeline\n",
    "        final_timeline = generate_save_timeline(similar_articles, df_train, test_article)\n",
    "        \n",
    "        if final_timeline == \"Error02\":\n",
    "            reason02 = \"There are insufficient relevant articles to construct a meaningful timeline.\"\n",
    "            return \"generate_similar_articles_error\", reason02\n",
    "        # end if\n",
    "        return final_timeline, None\n",
    "    else:\n",
    "        return \"to_generate_error\", timeline_necessary['reason']\n",
    "    # end if\n",
    "# end def\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timeline(test_id):\n",
    "    \"\"\"\n",
    "    Generates a timeline for the specified test article and saves it to MongoDB.\n",
    "\n",
    "    Args:\n",
    "        test_id (str): The ID of the test article.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the timeline details or an error message.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting Timeline Generation\\n\")\n",
    "    \n",
    "    df, df_train, test_article = split_data(test_id)\n",
    "    \n",
    "    # Generate the timeline using hierarchical clustering\n",
    "    timeline, fail_reason = main_hierarchical(df, test_article, df_train, test_id)\n",
    "    \n",
    "    logger.info(\"Fetching database to store the generated timeline...\\n\")\n",
    "    # Connect to MongoDB\n",
    "    db = mongo_client[config[\"database\"][\"name\"]]\n",
    "    \n",
    "    # Get the timelines collection from the database\n",
    "    gen_timeline_documents = db[config[\"database\"][\"timelines_collection\"]]\n",
    "    \n",
    "    test_article_id = test_article['st_id'][0]\n",
    "    test_article_title = test_article['Title'][0]\n",
    "    \n",
    "    # Handle cases where the timeline should not be generated\n",
    "    if timeline == \"to_generate_error\" or timeline == \"generate_similar_articles_error\":\n",
    "        # Timeline instance to return for error message\n",
    "        timeline_return = {\n",
    "            \"Article_id\": test_article_id,\n",
    "            \"Article_Title\": test_article_title,\n",
    "            \"Timeline_header\": \"null\",\n",
    "            \"error\": fail_reason\n",
    "        }\n",
    "        \n",
    "        # Timeline instance to export to MongoDB\n",
    "        timeline_export = {\n",
    "            \"Article_id\": test_article_id,\n",
    "            \"Article_Title\": test_article_title,\n",
    "            \"Timeline_header\": \"null\",\n",
    "            \"Timeline\": \"null\"\n",
    "        }\n",
    "        try:\n",
    "            # Insert result into collection\n",
    "            gen_timeline_documents.insert_one(timeline_export)\n",
    "            logger.info(f\"Timeline with article id {test_article_id} successfully saved to MongoDB\")\n",
    "        except Exception as error:\n",
    "            logger.error(f\"Unable to save timeline to database. Check your connection to the database...\\nERROR: {error}\\n\")\n",
    "            sys.exit()\n",
    "    else:\n",
    "        # Generate a header for the timeline if no error occurred\n",
    "        logger.info(\"Generating the timeline header...\\n\")\n",
    "        timeline_header = generate_timeline_header(timeline)['timeline_heading']\n",
    "        # Convert the timeline to JSON\n",
    "        timeline_json = json.dumps(timeline)\n",
    "        timeline_return = {\n",
    "            \"Article_id\": test_article_id,\n",
    "            \"Article_Title\": test_article_title,\n",
    "            \"Timeline_header\": timeline_header,\n",
    "            \"Timeline\": timeline_json\n",
    "        }\n",
    "        timeline_export = timeline_return\n",
    "        \n",
    "        # Insert the timeline data into MongoDB\n",
    "        try:\n",
    "            gen_timeline_documents.insert_one(timeline_export)\n",
    "            logger.info(f\"Timeline with article id {test_article_id} successfully saved to MongoDB\")\n",
    "        except Exception as error:\n",
    "            logger.error(f\"Unable to save timeline to database. Check your connection to the database...\\nERROR: {error}\\n\")\n",
    "            sys.exit()\n",
    "    # end if\n",
    "    \n",
    "    return timeline_return\n",
    "# end def\n",
    "\n",
    "def main(test_id): \n",
    "    \"\"\"\n",
    "    Main function to generate a timeline for a specific test article.\n",
    "\n",
    "    Returns:\n",
    "        dict: The generated timeline details.\n",
    "    \"\"\"\n",
    "    timeline_return = generate_timeline(test_id)\n",
    "    return timeline_return\n",
    "# end def\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline = main(test_id=\"st_1155048\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for the gradio interface\n",
    "\n",
    "def display_timeline(timeline_str):\n",
    "    logger.info(\"Displaying timeline on Gradio Interface \\n\")\n",
    "    if isinstance(timeline_str, str):\n",
    "        # Used for testing\n",
    "        timeline_list = json.loads(timeline_str)\n",
    "    else:\n",
    "        timeline_list = timeline_str\n",
    "    display_list= timeline_list[:len(timeline_list)//2]\n",
    "    html_content = '''\n",
    "                    <div style='text-align: center;'><strong>\n",
    "                    First half of events in the timeline:\\n\n",
    "                    </strong></div>\n",
    "                    <div style='padding: 10px;'>'''\n",
    "    for event in display_list:\n",
    "        html_content += f\"<h3>{event['Date']}</h3>\"\n",
    "        html_content += f\"<p><strong>Event:</strong> {event['Event']}</p>\"\n",
    "        #Display only the first 60 chars of the first article url\n",
    "        html_content += \"<p><strong>Article URL:</strong> \" + event['Article_URL'][0][0][:60] + \"</p>\"\n",
    "        html_content += \"<hr>\"\n",
    "    html_content += \"</div>\"\n",
    "    return html_content\n",
    "\n",
    "def user_download_button():\n",
    "    return gr.DownloadButton(visible=True)\n",
    "\n",
    "def display_gradio():\n",
    "    with gr.Blocks(title=\"Article Timeline Generator\", theme='snehilsanyal/scikit-learn') as gradio_timeline:\n",
    "        gr.Markdown(\"\"\"\n",
    "            <h1 style='text-align: center;'>\n",
    "            Timeline Generator\n",
    "            </h1>\n",
    "            <hr>\n",
    "            <h3>\n",
    "            Choose an article index to generate a timeline using the test database.\n",
    "            </h3>\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    input_test_index = gr.Number(label=\"Test Article Index. Choose an index from 1-7 (Number of test articles)\", value=0)\n",
    "                    hidden_article_id = gr.Textbox(visible=False)\n",
    "\n",
    "                    with gr.Row():\n",
    "                        clear_button = gr.Button(\"Reset index\")\n",
    "                        generate_button = gr.Button(\"Generate Timeline\")\n",
    "                    shown_article_title = gr.Textbox(label=\"Title of chosen article\")\n",
    "                    output_timeline = gr.JSON(label=\"Generated Timeline in JSON format\", visible=False)\n",
    "                    gr.Markdown('''\n",
    "                                If Error message is not shown past the 7 second mark, a timeline is necessary for the chosen article. \n",
    "                                ''')\n",
    "                    output_error = gr.Textbox(label=\"Error Message:\")  \n",
    "                    user_download_button = gr.DownloadButton(\"Download Generated Timeline\")\n",
    "                \n",
    "                with gr.Column():\n",
    "                    show_timeline_button = gr.Button(\"Show Generated Timeline\")\n",
    "                    output_timeline_HTML = gr.HTML()\n",
    "        \n",
    "        clear_button.click(lambda: 0, None, input_test_index)\n",
    "        \n",
    "        def handle_generate_timeline(test_id):\n",
    "                result = generate_timeline(test_id)\n",
    "                article_id = result['Article_id']\n",
    "                article_title = result['Article_Title']\n",
    "                if \"error\" in result:\n",
    "                    timeline_error = result[\"error\"]\n",
    "                    return timeline_error, None, article_id, article_title\n",
    "                else:\n",
    "                    \n",
    "                    timeline = result['Timeline']\n",
    "                    return \"NIL, Press Show Generated Timeline to display generated timeline\", timeline, article_id, article_title\n",
    "        generate_button.click(\n",
    "                handle_generate_timeline(test_id = \"st_1155048\"),\n",
    "                inputs=input_test_index,\n",
    "                outputs=[output_error, output_timeline, hidden_article_id, shown_article_title]\n",
    "            )\n",
    "        show_timeline_button.click(\n",
    "                display_timeline,\n",
    "                inputs=output_timeline,\n",
    "                outputs=output_timeline_HTML\n",
    "            )\n",
    "        user_download_button.click(\n",
    "            inputs=output_timeline,\n",
    "            outputs = user_download_button\n",
    "        )\n",
    "    gradio_timeline.launch(inbrowser=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Timeline Generation\n",
      "\n",
      "Fetching embeddings...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:00<00:00, 429.55it/s]\n",
      "/Users/jerryyang/pythonenv/py310/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first timeline has been generated\n",
      "\n",
      "Proceeding to Stage 1/2 of enhancement...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdisplay_gradio\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 73\u001b[0m, in \u001b[0;36mdisplay_gradio\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m             timeline \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimeline\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     71\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNIL, Press Show Generated Timeline to display generated timeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeline, article_id, article_title\n\u001b[1;32m     72\u001b[0m generate_button\u001b[38;5;241m.\u001b[39mclick(\n\u001b[0;32m---> 73\u001b[0m         \u001b[43mhandle_generate_timeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mst_1155048\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     74\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minput_test_index,\n\u001b[1;32m     75\u001b[0m         outputs\u001b[38;5;241m=\u001b[39m[output_error, output_timeline, hidden_article_id, shown_article_title]\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m show_timeline_button\u001b[38;5;241m.\u001b[39mclick(\n\u001b[1;32m     78\u001b[0m         display_timeline,\n\u001b[1;32m     79\u001b[0m         inputs\u001b[38;5;241m=\u001b[39moutput_timeline,\n\u001b[1;32m     80\u001b[0m         outputs\u001b[38;5;241m=\u001b[39moutput_timeline_HTML\n\u001b[1;32m     81\u001b[0m     )\n\u001b[1;32m     82\u001b[0m user_download_button\u001b[38;5;241m.\u001b[39mclick(\n\u001b[1;32m     83\u001b[0m     inputs\u001b[38;5;241m=\u001b[39moutput_timeline,\n\u001b[1;32m     84\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m user_download_button\n\u001b[1;32m     85\u001b[0m )\n",
      "Cell \u001b[0;32mIn[6], line 62\u001b[0m, in \u001b[0;36mdisplay_gradio.<locals>.handle_generate_timeline\u001b[0;34m(test_id)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandle_generate_timeline\u001b[39m(test_id):\n\u001b[0;32m---> 62\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_timeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m         article_id \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArticle_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     64\u001b[0m         article_title \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArticle_Title\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[5], line 74\u001b[0m, in \u001b[0;36mgenerate_timeline\u001b[0;34m(test_id)\u001b[0m\n\u001b[1;32m     71\u001b[0m df, df_train, test_article \u001b[38;5;241m=\u001b[39m load_mongodb(test_id)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Run this after gradio workflow tested\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m timeline, fail_reason \u001b[38;5;241m=\u001b[39m \u001b[43mmain_hierarchical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_article\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching database to store the generated timeline.. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Pull database\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m, in \u001b[0;36mmain_hierarchical\u001b[0;34m(df, test_article, df_train, test_id)\u001b[0m\n\u001b[1;32m     12\u001b[0m     similar_articles \u001b[38;5;241m=\u001b[39m find_similar_articles(Z, df, test_article, max_d, test_id)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Generation of final timeline\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     final_timeline \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_save_timeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimilar_articles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_article\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# return final_timeline\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final_timeline\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError02\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[0;32mIn[4], line 601\u001b[0m, in \u001b[0;36mgenerate_save_timeline\u001b[0;34m(similar_articles, df_train, df_test)\u001b[0m\n\u001b[1;32m    599\u001b[0m generated_timeline, df_retrieve \u001b[38;5;241m=\u001b[39m generate_and_sort_timeline(similar_articles, df_train, df_test)\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProceeding to Stage 1/2 of enhancement...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 601\u001b[0m first_enhanced_timeline \u001b[38;5;241m=\u001b[39m \u001b[43mfirst_timeline_enhancement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_timeline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m second_enhanced_timeline \u001b[38;5;241m=\u001b[39m pair_article_urls(first_enhanced_timeline, df_retrieve)\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProceeding to Stage 2/2 of enhancement...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 446\u001b[0m, in \u001b[0;36mfirst_timeline_enhancement\u001b[0;34m(timeline)\u001b[0m\n\u001b[1;32m    444\u001b[0m         new_timeline[date] \u001b[38;5;241m=\u001b[39m snippet\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 446\u001b[0m         new_snippet \u001b[38;5;241m=\u001b[39m \u001b[43mreduce_by_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43msnippet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m         new_timeline[date] \u001b[38;5;241m=\u001b[39mnew_snippet\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# end if\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# end for\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 401\u001b[0m, in \u001b[0;36mreduce_by_date\u001b[0;34m(timeline_list, llm)\u001b[0m\n\u001b[1;32m    394\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m    395\u001b[0m         input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    396\u001b[0m         template\u001b[38;5;241m=\u001b[39mtemplate,\n\u001b[1;32m    397\u001b[0m         partial_variables\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_instructions\u001b[39m\u001b[38;5;124m\"\u001b[39m: parser\u001b[38;5;241m.\u001b[39mget_format_instructions()}\n\u001b[1;32m    398\u001b[0m     )\n\u001b[1;32m    400\u001b[0m final_prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mformat(text\u001b[38;5;241m=\u001b[39mtimeline_string)\n\u001b[0;32m--> 401\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43mHarmCategory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHARM_CATEGORY_HATE_SPEECH\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mHarmBlockThreshold\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBLOCK_NONE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m            \u001b[49m\u001b[43mHarmCategory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHARM_CATEGORY_HARASSMENT\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mHarmBlockThreshold\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBLOCK_NONE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m            \u001b[49m\u001b[43mHarmCategory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHARM_CATEGORY_SEXUALLY_EXPLICIT\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mHarmBlockThreshold\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBLOCK_NONE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m            \u001b[49m\u001b[43mHarmCategory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHARM_CATEGORY_DANGEROUS_CONTENT\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mHarmBlockThreshold\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBLOCK_NONE\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m data \u001b[38;5;241m=\u001b[39m extract_content_from_json(response\u001b[38;5;241m.\u001b[39mparts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;66;03m# limit the number of article links to 2 links\u001b[39;00m\n",
      "File \u001b[0;32m~/pythonenv/py310/lib/python3.10/site-packages/google/generativeai/generative_models.py:262\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/pythonenv/py310/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:812\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    811\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 812\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/pythonenv/py310/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pythonenv/py310/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pythonenv/py310/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m~/pythonenv/py310/lib/python3.10/site-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pythonenv/py310/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(callable_)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_remapped_callable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[0;32m~/pythonenv/py310/lib/python3.10/site-packages/grpc/_channel.py:1178\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1168\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1174\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1175\u001b[0m     (\n\u001b[1;32m   1176\u001b[0m         state,\n\u001b[1;32m   1177\u001b[0m         call,\n\u001b[0;32m-> 1178\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/pythonenv/py310/lib/python3.10/site-packages/grpc/_channel.py:1162\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1145\u001b[0m state\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target)\n\u001b[1;32m   1146\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[1;32m   1147\u001b[0m     cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registered_call_handle,\n\u001b[1;32m   1161\u001b[0m )\n\u001b[0;32m-> 1162\u001b[0m event \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1163\u001b[0m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:400\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:78\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:42\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "display_gradio()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
