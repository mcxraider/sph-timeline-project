{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jerryyang/pythonenv/py310/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import re\n",
    "from tqdm import trange\n",
    "import torch\n",
    "from torch import nn\n",
    "from json import JSONDecodeError\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sentence_transformers import CrossEncoder\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "import requests\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# Import libraries for working with language models and Google Gemini\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Read configuration\n",
    "config_file = '../config.yaml'\n",
    "with open(config_file, 'r') as fin:\n",
    "    config = yaml.safe_load(fin)\n",
    "# end with\n",
    "\n",
    "GROQ_KEY     = os.environ['GROQ_API_KEY']\n",
    "GEMINI_KEY   = os.environ['GEMINI_KEY']\n",
    "MONGO_URI    = os.environ['MONGO_URI']\n",
    "HF_KEY       = os.environ['HUGGINGFACE_API_KEY']\n",
    "EMBEDDER_API = os.environ[\"HF_EMBEDDING_MODEL_URL\"]\n",
    "\n",
    "genai.configure(api_key=GEMINI_KEY)\n",
    "\n",
    "# Initialise Mongodb client\n",
    "mongo_client = MongoClient(MONGO_URI)\n",
    "\n",
    "# Setup default LLM model\n",
    "default_llm = genai.GenerativeModel('gemini-1.5-flash-latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_database():\n",
    "    # Connect to the MongoDB client\n",
    "    try:\n",
    "        db = mongo_client[config[\"database\"][\"name\"]]\n",
    "        train_documents = db[config[\"database\"][\"train_collection\"]].find()\n",
    "        logger.info(\"Train data successfully fetched from MongoDB\\n\")\n",
    "    except Exception as error: \n",
    "        logger.error(f\"Unable to fetch train data from MongoDB. Check your connection the database...\\nERROR: {error}\\n\")\n",
    "    \n",
    "    try:\n",
    "        test_docs = db[config[\"database\"][\"test_collection\"]].find()\n",
    "        logger.info(\"Test data successfully fetched from MongoDB\\n\")\n",
    "    except:\n",
    "        logger.error(f\"Unable to fetch test data from MongoDB. Check your connection the database...\\nERROR: {error}\\n\")\n",
    "    \n",
    "    df_train = pd.DataFrame.from_dict(list(train_documents))\n",
    "    df_test = pd.DataFrame.from_dict(list(test_docs))\n",
    "    \n",
    "    # Row bind the training and test dataframes \n",
    "    df = pd.concat([df_train, df_test], axis=0)\n",
    "    \n",
    "    # Convert it to a dictionary for O(1) look up\n",
    "    db = df.to_dict(orient='records')\n",
    "    return db\n",
    "# end def\n",
    "\n",
    "def get_test_article(db, test_id):\n",
    "    test_article = []\n",
    "    for article in db:\n",
    "        if article['st_id'] == test_id:\n",
    "            test_article.append(article)\n",
    "            break\n",
    "        # end if\n",
    "    # end for\n",
    "    return test_article[0]\n",
    "# end def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_llm_output(llm_output):\n",
    "        text = llm_output.parts[0].text.replace(\"```\", '').replace('json','')\n",
    "        result = json.loads(text)\n",
    "        return result\n",
    "# end def\n",
    "\n",
    "def det_generate_timeline(input_data,score_threshold=3,llm=default_llm,\n",
    "    safety_settings = {\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "    }):\n",
    "    \n",
    "    \"\"\"Evaluating necessity of Timeline for this article.\"\"\"\n",
    "\n",
    "    # Initialise Pydantic object to force LLM return format\n",
    "    class Event(BaseModel):\n",
    "        score: int = Field(description=\"The need for this article to have a timeline\")\n",
    "        Reason: str = Field(description = \"The main reason for your choice why a timeline is needed or why it is not needed\")\n",
    "    \n",
    "    # Initialise Json output parser\n",
    "    output_parser = JsonOutputParser(pydantic_object=Event)\n",
    "\n",
    "    # Define the template\n",
    "    template = \\\n",
    "    '''\n",
    "    You are a highly intelligent AI tasked with analyzing articles to determine whether generating a timeline of events leading up to the key event in the article would be beneficial. \n",
    "    Consider the following factors to make your decision:\n",
    "    1. **Significance of the Event**:\n",
    "       - Does the event have a significant impact on a large number of people, industries, or countries?\n",
    "       - Are the potential long-term consequences of the event important?\n",
    "\n",
    "    2. **Controversy or Debate**:\n",
    "       - Is the event highly controversial or has it sparked significant debate?\n",
    "       - Has the event garnered significant media attention and public interest?\n",
    "\n",
    "    3. **Complexity**:\n",
    "       - Does the event involve multiple factors, stakeholders, or causes that make it complex?\n",
    "       - Does the event have deep historical roots or is it the culmination of long-term developments?\n",
    "\n",
    "    4. **Personal Relevance**:\n",
    "       - Does the event directly affect the reader or their community?\n",
    "       - Is the event of particular interest to the reader due to economic implications, political affiliations, or social issues?\n",
    "\n",
    "    5. Educational Purposes:\n",
    "       - Would a timeline provide valuable learning or research information?\n",
    "\n",
    "    Here is the information for the article:\n",
    "    Title:{title}\n",
    "    Text: {text}\n",
    "\n",
    "    Based on the factors above, decide whether generating a timeline of events leading up to the key event in this article would be beneficial. \n",
    "    Your answer will include the need for this article to have a timeline with a score 1 - 5, 1 means unnecessary, 5 means necessary. It will also include the main reason for your choice.\n",
    "    {format_instructions}    \n",
    "    ANSWER:\n",
    "    '''\n",
    "    \n",
    "    # See the prompt template you created for formatting\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    # Create the prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables   = [\"text\", \"title\"],\n",
    "        partial_variables = {\"format_instructions\": format_instructions},\n",
    "        template=template,\n",
    "    )\n",
    "\n",
    "    # Define the headline\n",
    "    headline = input_data[\"Title\"]\n",
    "    body     = input_data[\"Text\"]\n",
    "\n",
    "    # Format the prompt\n",
    "    final_prompt = prompt.format(title=headline, text=body)\n",
    "\n",
    "    # Generate content using the generative model\n",
    "    response = llm.generate_content(\n",
    "        final_prompt,\n",
    "        safety_settings=safety_settings,\n",
    "    )\n",
    "    final_response = clean_llm_output(response)\n",
    "\n",
    "    score = final_response['score']\n",
    "    \n",
    "     # If LLM approves\n",
    "    if score >= score_threshold:\n",
    "        logger.info(\"Timeline is appropriate for this chosen article.\\n\")\n",
    "        return {\"det\": True, \"score\": final_response['score'], \"reason\": None}\n",
    "    # end if\n",
    "    else:\n",
    "        logger.info(\"A timeline for this article is not required. \\n\")\n",
    "        for part in final_response['Reason'].replace(\". \", \".\").split(\". \"):\n",
    "            logger.info(f\"{part}\\n\")\n",
    "        # end for\n",
    "        \n",
    "        logger.info(\"Hence I gave this a required timeline score of \" + str(score))\n",
    "        reason = \"A timeline for this article is not required. \\n\" \\\n",
    "                    + \"\\n\" +final_response['Reason'] + \"\\n\"+ \"\\nHence this timeline received a necessity score of \" \\\n",
    "                    + str(final_response['score'])   + \"\\n\"\n",
    "    # end else\n",
    "        return {\"det\": False, \"score\": score, \"reason\": reason}\n",
    "# end def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the header of the timeline of the desired article\n",
    "def generate_timeline_header(article,llm=default_llm,):\n",
    "    \n",
    "    # Define pydantic object to force LLM return output    \n",
    "    class timeline_header(BaseModel):\n",
    "        timeline_header: str = Field(description=\"Suitable header of a timeline for this article\")\n",
    "    \n",
    "    # Define prompt\n",
    "    template = \\\n",
    "'''I would like to create a timeline of events based on the title of an article.\n",
    "Given a list of article titles below, you are tasked with creating an extremely generalised, suitable name for a timeline for this article that will provide a reader contextual information about a timeline of events regarding the article.\n",
    "The header should be something that can be generalised to other similar articles.\n",
    "For instance, if a title is \"Sâ€™pore Red Cross gives $270k worth of relief aid to victims of Hamas-Israel war in Gaza\", the header should be \"Relief Aid for Gaza Conflict Victims\"\n",
    "\n",
    "Article Title:\n",
    "{title}\n",
    "\n",
    "{format_instructions}\n",
    "Before you return the answer, ensure and double check that you have adhered the answer format instructions strictly.'''\n",
    "    \n",
    "    # Initialise Json output parser\n",
    "    output_parser = JsonOutputParser(pydantic_object=timeline_header)\n",
    "\n",
    "    # Create prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"title\"],\n",
    "        partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    "    )\n",
    "    \n",
    "    # Format the prompt\n",
    "    final_prompt = prompt.format(title=article['Title'])\n",
    "    # Generate response from LLM\n",
    "    response = llm.generate_content(final_prompt,\n",
    "                                    safety_settings= {\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE})\n",
    "    \n",
    "    # Post process LLM output \n",
    "    cleaned_output = clean_llm_output(response)\n",
    "    extracted_header= list(cleaned_output.values())[0]\n",
    "    return extracted_header\n",
    "# end def\n",
    "\n",
    "# Initialise dense embedder model from huggingface\n",
    "def dense_embedder(payload):\n",
    "    # The embedder API is the model loaded from Hugging face Inference API.\n",
    "    response = requests.post(EMBEDDER_API, headers={\"Authorization\": f\"Bearer {HF_KEY}\"}, json=payload)\n",
    "    return response.json()\n",
    "# end def\n",
    "\n",
    "# Generate the cosine similarity between texts of 2 articles\n",
    "def get_text_similarity(timeline_embedding,train_article):\n",
    "    \n",
    "    cos_sim = nn.CosineSimilarity(dim=0)\n",
    "    \n",
    "    text_embedding = torch.tensor(eval(train_article['embeddings']))\n",
    "    timeline_embedding = torch.tensor(timeline_embedding)\n",
    "    \n",
    "    similarity_score = cos_sim(timeline_embedding, text_embedding)\n",
    "    return similarity_score\n",
    "# end def\n",
    "\n",
    "# Generate the cosine similarity between titles of 2 articles\n",
    "def get_title_similarity(timeline_embedding,train_article):\n",
    "    \n",
    "    cos_sim = nn.CosineSimilarity(dim=0)\n",
    "    \n",
    "    title_embedding = torch.tensor(eval(train_article['Title_embeddings']))\n",
    "    timeline_embedding = torch.tensor(timeline_embedding)\n",
    "    \n",
    "    similarity_score = cos_sim(timeline_embedding, title_embedding)\n",
    "    return similarity_score\n",
    "# end def\n",
    "\n",
    "# Find the top 20 most similar articles based on their text\n",
    "def articles_ranked_by_text(test_article,timeline_header,database):\n",
    "    \n",
    "    logger.info(\"Title of test article: \" + test_article['Title'] + \"\\n\")\n",
    "    logger.info(\"Computing similarities between article texts...\")\n",
    "    \n",
    "    timeline_heading_embedding = dense_embedder(timeline_header)\n",
    "    \n",
    "    article_collection = []\n",
    "    for i in trange(len(database)):\n",
    "        article = {}\n",
    "        article['id'] = database[i]['st_id']\n",
    "        article['Title'] = database[i]['Title']\n",
    "        article['Text'] = database[i]['Text']\n",
    "        article['Date'] = database[i]['Publication_date']\n",
    "        article['Article_URL'] = database[i]['article_url']\n",
    "        article['cosine_score'] = get_text_similarity(timeline_heading_embedding, database[i])\n",
    "        article_collection.append(article)\n",
    "    # end for\n",
    "    \n",
    "    # Sort by cosine similarity in descending order\n",
    "    article_collection.sort(key = lambda x: x['cosine_score'], reverse=True)\n",
    "    \n",
    "    # Returns the top 20 most similar articles\n",
    "    return article_collection[:20]\n",
    "# end def\n",
    "\n",
    "# Find the top 20 most similar articles based on their title\n",
    "def articles_ranked_by_titles(timeline_header,database):\n",
    "    \n",
    "    logger.info(\"Computing similarities between article titles...\")\n",
    "    \n",
    "    timeline_heading_embedding = dense_embedder(timeline_header)\n",
    "    \n",
    "    article_collection = []\n",
    "    for i in trange(len(database)):\n",
    "        article = {}\n",
    "        article['id'] = database[i]['st_id']\n",
    "        article['Title'] = database[i]['Title']\n",
    "        article['Text'] = database[i]['Text']\n",
    "        article['Date'] = database[i]['Publication_date']\n",
    "        article['Article_URL'] = database[i]['article_url']\n",
    "        article['cosine_score'] = get_title_similarity(timeline_heading_embedding, database[i])\n",
    "        article_collection.append(article)\n",
    "    \n",
    "    # Sort by cosine similarity in descending order\n",
    "    article_collection.sort(key = lambda x: x['cosine_score'], reverse=True)\n",
    "    \n",
    "    # Returns the top 20 most similar articles (might not always need top 10)\n",
    "    return article_collection[:20]\n",
    "# end def\n",
    "\n",
    "# Combine the similar articles retrieved by text and title embeddings\n",
    "def combine_similar_articles(similar_by_titles,similar_by_text):\n",
    "    \n",
    "    all_articles = []\n",
    "    # Combine the results by appending one article from each list simultaneously\n",
    "    for i in range(len(similar_by_text)):\n",
    "        all_articles.append(similar_by_text[i])\n",
    "        all_articles.append(similar_by_titles[i])\n",
    "    # end for\n",
    "    \n",
    "    # Initialize a set to track seen titles\n",
    "    seen_titles = set()\n",
    "\n",
    "    # List comprehension to remove duplicates based on the title of the article\n",
    "    unique_articles = []\n",
    "    for item in all_articles:\n",
    "        title = item[\"Title\"]\n",
    "        if title not in seen_titles:\n",
    "            seen_titles.add(title)\n",
    "            unique_articles.append(item)\n",
    "        # end if\n",
    "    # end for\n",
    "    return unique_articles\n",
    "# end def\n",
    "\n",
    "# Re rank articles based on similarity derived from cross encoder\n",
    "def re_rank_articles(unique_articles,timeline_header,cross_encoder_model=\"cross-encoder/ms-marco-TinyBERT-L-2-v2\"):\n",
    " \n",
    "    cross_encoder = CrossEncoder(\n",
    "        cross_encoder_model, max_length=512, device=\"cpu\"\n",
    "    )\n",
    "\n",
    "    # Format the timeline header and article for cross encoder\n",
    "    unranked_articles = [(timeline_header, doc['Text']) for doc in unique_articles]\n",
    "    \n",
    "    # Predict similarity_scores\n",
    "    similarity_scores = cross_encoder.predict(unranked_articles).tolist()\n",
    "\n",
    "    # Assign the list of similarity scores to the inidividual articles \n",
    "    for i in range(len(unique_articles)):\n",
    "        # Only assign the score if there is a positive relationship between the timeline header and the article\n",
    "        if similarity_scores[i]>0:\n",
    "            unique_articles[i]['reranked_score'] = similarity_scores[i]\n",
    "        # end if\n",
    "    # end for\n",
    "    combined_articles = [article for article in unique_articles if 'reranked_score' in article]\n",
    "    return combined_articles\n",
    "# end def    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the main event of each article by using its contents\n",
    "def generate_event_and_date(article,llm=default_llm,):\n",
    "    \n",
    "    article_text = article['Text']\n",
    "    article_title = article['Title']\n",
    "    article_date = article['Date']\n",
    "        \n",
    "    class main_event(BaseModel):\n",
    "        main_event: str = Field(description=\"Main event of the article\")\n",
    "        event_date: str = Field(description=\"Date which the main event occured in YYYY-MM-DD\")\n",
    "        \n",
    "    template = \\\n",
    "    '''\n",
    "    You are a news article editor. Analyse the article deeply, and describe the main event of the article below in one short sentence.\n",
    "    Using this main event and the publication date, identify the date at when this main event occured.\n",
    "    You should use any time references such as \"last week,\" \"last month,\" or specific dates. \n",
    "    If the article does not specify the exact date, save the date in the YYYY-MM-XX or YYYY-XX-XX format.\n",
    "    Do not provide any explanations for your answer.\n",
    "\n",
    "    Publication Date:\n",
    "    {date}\n",
    "    Article Title:\n",
    "    {title}\n",
    "    Article Text:\n",
    "    {text}\n",
    "\n",
    "    {format_instructions}\n",
    "    Before you return the answer, ensure and double check that you have adhered the answer format instructions strictly.\n",
    "    '''\n",
    "    \n",
    "    # Initialise Json output parser\n",
    "    output_parser = JsonOutputParser(pydantic_object=main_event)\n",
    "\n",
    "    # Define prompt \n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"date\", \"title\", \"text\"],\n",
    "        partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    "    )\n",
    "    \n",
    "    # Format prompt\n",
    "    final_prompt = prompt.format(date=article_date,\n",
    "                                 title=article_title,\n",
    "                                 text=article_text)\n",
    "    \n",
    "    # Generate LLM response \n",
    "    response = llm.generate_content(final_prompt,\n",
    "                                    safety_settings={\n",
    "                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, \n",
    "                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "    })\n",
    "    \n",
    "    # Post process LLM output\n",
    "    cleaned_output = clean_llm_output(response)\n",
    "    \n",
    "    generated_main_event = cleaned_output['main_event']\n",
    "    generated_date = cleaned_output['event_date']\n",
    "    return generated_main_event, generated_date\n",
    "# end def\n",
    "\n",
    "# Filter out only the top 12 articles \n",
    "def filter_ranked_articles(combined_articles,top_k = 12):\n",
    "    sorted_articles = sorted(combined_articles, key=lambda x: x['reranked_score'], reverse=True)\n",
    "    \n",
    "    # Filter out top 12 articles \n",
    "    if len(sorted_articles) > top_k:\n",
    "        sorted_articles = sorted_articles[:top_k]\n",
    "    # end if\n",
    "    \n",
    "    logging.info(\"Generating main events from each article\\n\")\n",
    "    \n",
    "    # Add generated main event for each article\n",
    "    for i in trange(len(sorted_articles)):\n",
    "        article = sorted_articles[i]\n",
    "        main_event, event_date = generate_event_and_date(article)\n",
    "        sorted_articles[i]['Event'] = main_event\n",
    "        sorted_articles[i]['Event_date'] = event_date\n",
    "        \n",
    "        # Remove the cosine score for cleanliness \n",
    "        sorted_articles[i].pop(\"cosine_score\")\n",
    "    # end for\n",
    "    \n",
    "    return sorted_articles\n",
    "# end def\n",
    "\n",
    "# Format the dates into human readable format and sort by date \n",
    "def generate_timeline(filtered_articles):\n",
    "    \n",
    "    def format_timeline_date(date_str, formats=['%Y', '%Y-%m-%d', '%Y-%m']):\n",
    "        \"\"\"Formats a date string into a human-readable format.\n",
    "        \n",
    "        Args:\n",
    "            date_str (str): The date string to be formatted.\n",
    "            formats (list): A list of date formats to try.\n",
    "        \n",
    "        Returns:\n",
    "            str: The formatted date string, or the original string if no format matches.\n",
    "        \"\"\"\n",
    "        for fmt in formats:\n",
    "            try:\n",
    "                date_obj = datetime.strptime(date_str, fmt)\n",
    "                if fmt == '%Y':\n",
    "                    return date_obj.strftime('%Y')\n",
    "                elif fmt == '%Y-%m-%d':\n",
    "                    return date_obj.strftime('%d %B %Y')\n",
    "                elif fmt == '%Y-%m':\n",
    "                    return date_obj.strftime('%B %Y')\n",
    "                # end if\n",
    "            except ValueError:\n",
    "                continue  # If the format doesn't match, try the next one\n",
    "        # end for\n",
    "        \n",
    "        # If no format matches, return the original date string\n",
    "        return date_str\n",
    "    # end def\n",
    "    \n",
    "    \n",
    "    timeline_data = sorted([{\"Event\": event['Event'], \"Date\": event['Event_date'], \"Article_URL\": event['Article_URL'], \"Article_title\": event['Title']} for event in filtered_articles],\n",
    "                           key= lambda x: x['Date'])\n",
    "    \n",
    "    for event in timeline_data:\n",
    "        # Format data into human readable format\n",
    "        event['Date'] = format_timeline_date(event['Date'])\n",
    "        \n",
    "        # Generate URL-title pair for easy access in displaying the timeline\n",
    "        event['Article_URL'] = [{\"url\": event['Article_URL'], \"title\": event['Article_title']}]\n",
    "    # end for\n",
    "    return timeline_data\n",
    "# end def\n",
    "\n",
    "def export_hybrid_timeline(test_article,timeline_data,timeline_header):\n",
    "    \n",
    "    logging.info(\"Fetching database to store the generated timeline.. \\n\")\n",
    "    \n",
    "    # Pull database from MongoDB\n",
    "    database = mongo_client[config[\"database\"][\"name\"]]\n",
    "        \n",
    "    # Get collection from database that stores the hybrid timeline\n",
    "    timeline_documents = database[config[\"database\"][\"hybrid_timeline_collection\"]]\n",
    "        \n",
    "    article_id    = test_article['st_id']\n",
    "    article_title = test_article['Title']\n",
    "\n",
    "    # Modify timeline header for readability\n",
    "    display_header = \"Timeline: \" + timeline_header\n",
    "    \n",
    "    timeline_export = {\"Article_id\": article_id, \n",
    "                        \"Article_Title\": article_title, \n",
    "                        \"Timeline_header\": display_header,\n",
    "                        \"Timeline\": json.dumps(timeline_data)}\n",
    "                \n",
    "    # Insert the timeline data to MongoDB\n",
    "    try:\n",
    "        timeline_documents.insert_one(timeline_export)\n",
    "        logging.info(f\"Timeline with article id {article_id} successfully saved to MongoDB\")\n",
    "    except Exception as error:\n",
    "        logging.error(f\"Unable to save timeline to database. Check your connection the database...\\nERROR: {error}\\n\")\n",
    "# end def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(test_id):\n",
    "    # Load database \n",
    "    db= load_database()\n",
    "\n",
    "    # Article to generate timeline for\n",
    "    test_article = get_test_article(db, test_id)    \n",
    "        \n",
    "    # Let LLM decide if a timeline is necessary\n",
    "    timeline_necessary = det_generate_timeline(test_article)\n",
    "    if timeline_necessary[\"det\"]:\n",
    "            timeline_header = generate_timeline_header(test_article)\n",
    "            articles_by_text = articles_ranked_by_text(test_article,timeline_header,db)\n",
    "            artiles_by_titles = articles_ranked_by_titles(timeline_header,db)\n",
    "            unique_articles = combine_similar_articles(artiles_by_titles, articles_by_text)\n",
    "            re_ranked_articles = re_rank_articles(unique_articles, timeline_header)\n",
    "            sorted_articles = filter_ranked_articles(re_ranked_articles)\n",
    "            generated_timeline = generate_timeline(sorted_articles)\n",
    "            export_hybrid_timeline(test_article, generated_timeline, timeline_header)\n",
    "    else:\n",
    "            logging.info(timeline_necessary[\"reason\"])\n",
    "    # end if\n",
    "\n",
    "# end def\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
