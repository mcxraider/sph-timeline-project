{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import ast\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pymongo import MongoClient\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import trange\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# Import libraries for working with language models and Google Gemini\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "\n",
    "# Install the google-generativeai package (uncomment the line below to run the installation)\n",
    "!pip install -U -q google-generativeai\n",
    "\n",
    "# Set up the environment for plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_upload/cluster_labels1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     merged_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dataframes, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m merged_df\n\u001b[0;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_merge_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_upload/cluster_labels\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36mload_and_merge_csv\u001b[0;34m(file_pattern, num_files)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_merge_csv\u001b[39m(file_pattern, num_files):\n\u001b[1;32m      3\u001b[0m     file_names \u001b[38;5;241m=\u001b[39m [file_pattern\u001b[38;5;241m.\u001b[39mformat(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_files \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m----> 4\u001b[0m     dataframes \u001b[38;5;241m=\u001b[39m [pd\u001b[38;5;241m.\u001b[39mread_csv(filename) \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m file_names]\n\u001b[1;32m      5\u001b[0m     merged_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dataframes, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m merged_df\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_merge_csv\u001b[39m(file_pattern, num_files):\n\u001b[1;32m      3\u001b[0m     file_names \u001b[38;5;241m=\u001b[39m [file_pattern\u001b[38;5;241m.\u001b[39mformat(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_files \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m----> 4\u001b[0m     dataframes \u001b[38;5;241m=\u001b[39m [\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m file_names]\n\u001b[1;32m      5\u001b[0m     merged_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dataframes, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m merged_df\n",
      "File \u001b[0;32m~/Desktop/timeline project/timeline/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/timeline project/timeline/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Desktop/timeline project/timeline/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/timeline project/timeline/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Desktop/timeline project/timeline/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_upload/cluster_labels1.csv'"
     ]
    }
   ],
   "source": [
    "# Function to load and combine the split dataframes\n",
    "def load_and_merge_csv(file_pattern, num_files):\n",
    "    file_names = [file_pattern.format(i) for i in range(1, num_files + 1)]\n",
    "    dataframes = [pd.read_csv(filename) for filename in file_names]\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return merged_df\n",
    "\n",
    "df = load_and_merge_csv(\"../data_upload/cluster_labels{}.csv\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2019 entries, 0 to 2018\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   id            2019 non-null   object\n",
      " 1   Text          2019 non-null   object\n",
      " 2   Title         2018 non-null   object\n",
      " 3   embeddings    2019 non-null   object\n",
      " 4   Cluster       2019 non-null   int64 \n",
      " 5   combined      2018 non-null   object\n",
      " 6   Common_Theme  2019 non-null   object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 110.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_KEY = os.environ.get('GEMINI_KEY')\n",
    "genai.configure(api_key=GEMINI_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- multi processing of inputs using multiprocessing library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing for Tag Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 100 entries, 0 to 99\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   id            100 non-null    object\n",
      " 1   Text          100 non-null    object\n",
      " 2   Title         100 non-null    object\n",
      " 3   embeddings    100 non-null    object\n",
      " 4   Cluster       100 non-null    int64 \n",
      " 5   combined      100 non-null    object\n",
      " 6   Common_Theme  100 non-null    object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 6.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df = df.iloc[range(100)]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.loc[range(25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 25 entries, 0 to 24\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   id            25 non-null     object\n",
      " 1   Text          25 non-null     object\n",
      " 2   Title         25 non-null     object\n",
      " 3   embeddings    25 non-null     object\n",
      " 4   Cluster       25 non-null     int64 \n",
      " 5   combined      25 non-null     object\n",
      " 6   Common_Theme  25 non-null     object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 1.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25 articles with thread pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = genai.GenerativeModel('gemini-1.0-pro')\n",
    "\n",
    "template = '''\n",
    "    Task Description: Given the following news article, identify and suggest 3 to 5 relevant tags that categorize the main themes, \n",
    "    topics, entities, and geographical locations mentioned. \n",
    "    The tags should be concise, informative, and reflect the content accurately to facilitate effective searching and organization within a database.\n",
    "    \n",
    "    Combined Title and Summaries:\n",
    "    {text}\n",
    "    \n",
    "    Formatting convention: List the tags to me in this example format:\n",
    "    Singapore, Big family, climbing, Baby, crying, hungry\n",
    "    \n",
    "    Ensure that the tags generated follow the formatting convention very closely. \n",
    "    Generated tags:\n",
    "    \n",
    "    Check again that the format follows the formatting convention stated above\n",
    "        '''\n",
    "                    \n",
    "prompt = PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=template)\n",
    "        \n",
    "def fetch_tags(article_pair):\n",
    "    article_text, article_id = article_pair\n",
    "    final_prompt = prompt.format(text=article_text)\n",
    "    response = llm.generate_content(final_prompt, safety_settings={\n",
    "                                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, \n",
    "                                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "                                    })\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        return article_id, response.text.strip().split(\", \")\n",
    "    except ValueError:\n",
    "        return article_id, response.prompt_feedback\n",
    "\n",
    "def process_articles(df):\n",
    "    results = {}\n",
    "    max_workers = 10\n",
    "    batch_size = 100\n",
    "    cooldown_period = 90\n",
    "\n",
    "    articles = df['Text'].tolist()\n",
    "    ids = df['id'].tolist()\n",
    "    article_id_pairs = list(zip(articles, ids))\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for i in range(0, len(article_id_pairs), batch_size):\n",
    "            current_batch = article_id_pairs[i:i+batch_size]\n",
    "            print(f\"Starting batch processing for articles {i+1} to {min(i+batch_size, len(article_id_pairs))}\")\n",
    "            futures = {executor.submit(fetch_tags, pair): pair for pair in current_batch}\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                article_id, tags = future.result()\n",
    "                results[article_id] = tags\n",
    "\n",
    "            if i + batch_size < len(article_id_pairs):\n",
    "                print(f\"All tasks in batch {i//batch_size + 1} completed, cooling down for {cooldown_period} seconds...\")\n",
    "                time.sleep(cooldown_period)\n",
    "\n",
    "    return results\n",
    "\n",
    "articles = df1['combined'].tolist()\n",
    "article_ids = df1['id'].tolist()\n",
    "tags = process_articles(articles, article_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_tags = tags\n",
    "copy_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning and filtering \n",
    "def map_to_id(df1, tags):\n",
    "    ids = df1.id.to_list()\n",
    "    ordered_tags = []\n",
    "    for id in ids:\n",
    "        # Clean each tag by stripping extra spaces, removing '*', replacing newlines and dashes, and capitalizing\n",
    "        clean_tag = [tag.strip().replace('#', '').replace('*', '').replace('\\n', ',').replace('-', '').title() for tag in tags[id] if tag.strip()]\n",
    "        ordered_tags.append(clean_tag)\n",
    "    return ordered_tags\n",
    "\n",
    "# USE COPY _TAGS\n",
    "clean_and_ordered_tags_list = map_to_id(df1,copy_tags)\n",
    "clean_and_ordered_tags_list\n",
    "#strs = list(map(lambda x: ', '.join(x), copy_tags))\n",
    "# df1['tags'] = pd.DataFrame(strs)\n",
    "# df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 300 entries, 0 to 299\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   id            300 non-null    object\n",
      " 1   Text          300 non-null    object\n",
      " 2   Title         300 non-null    object\n",
      " 3   embeddings    300 non-null    object\n",
      " 4   Cluster       300 non-null    int64 \n",
      " 5   combined      300 non-null    object\n",
      " 6   Common_Theme  300 non-null    object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 18.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df1 = df.loc[range(400)]\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GENEVA – The remains of a climber discovered in the Swiss Alps in 2022 have been identified as those of a British mountaineer who went missing 52 years ago, local police '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['Text'] = df1['Text'].apply(lambda x: x[:130])\n",
    "df1.Text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = genai.GenerativeModel('gemini-1.0-pro')\n",
    "\n",
    "template = '''\n",
    "    Task Description: Given the following news article, identify and suggest 3 to 5 relevant tags that categorize the main themes, \n",
    "    topics, entities, and geographical locations mentioned. \n",
    "    The tags should be concise, informative, and reflect the content accurately to facilitate effective searching and organization within a database.\n",
    "    \n",
    "    Combined Title and Summaries:\n",
    "    {text}\n",
    "    \n",
    "    Formatting convention: List the tags to me in this example format:\n",
    "    Singapore, Big family, climbing, Baby, crying, hungry\n",
    "    \n",
    "    Ensure that the tags generated follow the formatting convention very closely. \n",
    "    Generated tags:\n",
    "    \n",
    "    Check again that the format follows the formatting convention stated above\n",
    "        '''\n",
    "                    \n",
    "prompt = PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=template)\n",
    "        \n",
    "def fetch_tags(article_pair):\n",
    "    article_text, article_id = article_pair\n",
    "    final_prompt = prompt.format(text=article_text)\n",
    "    response = llm.generate_content(final_prompt, safety_settings={\n",
    "                                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, \n",
    "                                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "                                    })\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        return article_id, response.text.strip().split(\", \")\n",
    "    except ValueError:\n",
    "        return article_id, response.prompt_feedback\n",
    "\n",
    "def process_articles(df):\n",
    "    results = {}\n",
    "    max_workers = 10\n",
    "    batch_size = 100\n",
    "    cooldown_period = 90\n",
    "\n",
    "    articles = df1['combined'].tolist()\n",
    "    article_ids = df1['id'].tolist()\n",
    "    article_id_pairs = list(zip(articles, article_ids))\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for i in range(0, len(article_id_pairs), batch_size):\n",
    "            current_batch = article_id_pairs[i:i+batch_size]\n",
    "            print(f\"Starting batch processing for articles {i+1} to {min(i+batch_size, len(article_id_pairs))}\")\n",
    "            futures = {executor.submit(fetch_tags, pair): pair for pair in current_batch}\n",
    "\n",
    "            processed_count = i\n",
    "            for future in as_completed(futures):\n",
    "                article_id, tags = future.result()\n",
    "                results[article_id] = tags\n",
    "                processed_count += 1\n",
    "                print(f\"Processed article {processed_count} in Batch {(i//100)+1} \")\n",
    "                \n",
    "            if processed_count >= len(article_id_pairs):\n",
    "                return results\n",
    "            \n",
    "            print(f\"All tasks in batch {i//batch_size + 1} completed, cooling down for {cooldown_period} seconds...\")\n",
    "            time.sleep(cooldown_period)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = process_articles(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:08<03:30,  2.19s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Missing climbers',\n",
       "  'Global warming',\n",
       "  'Swiss Alps',\n",
       "  'Saas Valley',\n",
       "  'British mountaineer'],\n",
       " ['AI', 'Ethics', 'Climate change', 'Youth involvement', 'Generative AI'],\n",
       " ['Tennis', 'US Open', 'Novak Djokovic', 'Iga Swiatek', 'Grand Slam'],\n",
       " ['Myanmar', 'Political Crisis', 'UN', 'ASEAN', 'Rohingya Refugees'],\n",
       " ['', 'Judicial Crisis', 'Israel', 'Shekel', 'Politics']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run this tomorrow:\n",
    "llm = genai.GenerativeModel('gemini-1.0-pro')\n",
    "\n",
    "template = '''\n",
    "    Task Description: Given the following news article, identify and suggest 3 to 5 relevant tags that categorize the main themes, \n",
    "    topics, entities, and geographical locations mentioned. \n",
    "    The tags should be concise, informative, and reflect the content accurately to facilitate effective searching and organization within a database.\n",
    "    \n",
    "    Combined Title and Summaries:\n",
    "    {text}\n",
    "    \n",
    "    Formatting convention: List the tags to me in this example format:\n",
    "    Singapore, Big family, climbing, Baby, crying, hungry\n",
    "    \n",
    "    Ensure that the tags generated follow the formatting convention very closely. \n",
    "    Generated tags:\n",
    "    \n",
    "    Check again that the format follows the formatting convention stated above\n",
    "        '''\n",
    "            \n",
    "prompt = PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=template)\n",
    "\n",
    "all_tags = []\n",
    "for i in trange(len(df)):\n",
    "    article = df.combined[i]\n",
    "    final_prompt  = prompt.format(text=article)\n",
    "    article_tags = llm.generate_content(final_prompt, safety_settings={\n",
    "                                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, \n",
    "                                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "                                    })\n",
    "    all_tags.append(article_tags.text.strip().split(\", \"))\n",
    "    \n",
    "all_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Swiss Alps',\n",
       "  'Missing Climber',\n",
       "  'Glacial Melt',\n",
       "  'Dna Identification',\n",
       "  'Climate Change'],\n",
       " ['Artificial Intelligence',\n",
       "  'Generative Ai',\n",
       "  'Ethics',\n",
       "  'Risk Management',\n",
       "  'Youth Engagement'],\n",
       " ['Tennis', 'U.S. Open', 'Grand Slam', 'Novak Djokovic', 'Iga Swiatek'],\n",
       " ['Myanmar', 'United Nations', 'Military Coup', 'Asean', 'Rohingya Crisis'],\n",
       " ['Israel', 'Politics', 'Currency', 'Economy', 'Judicial Reform']]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_tags(nested_tags):\n",
    "    cleaned_nested_tags = []\n",
    "    for tag_list in nested_tags:\n",
    "        # Include condition to filter out empty or whitespace-only tags\n",
    "        cleaned_tags = [tag.strip().replace('*', '').title() for tag in tag_list if tag.strip()]\n",
    "        cleaned_nested_tags.append(cleaned_tags)\n",
    "    return cleaned_nested_tags\n",
    "\n",
    "cleaned_tags = clean_tags(all_tags)\n",
    "cleaned_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the enew article that come sin, use the generate_tag function, (create one),\\\n",
    "    # then find similar articles that have like 2 or 3 of the same tags, means relevant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
