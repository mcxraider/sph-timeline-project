{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully inserted into MongoDB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json \n",
    "import yaml\n",
    "from pymongo import MongoClient\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def combine_json(files):\n",
    "    combined_data = []\n",
    "    for file in files:\n",
    "        with open(file, 'r', encoding='utf-8') as fin:\n",
    "            # Load data from the file and append it to the combined list\n",
    "            data = json.load(fin)\n",
    "            combined_data.extend(data)\n",
    "    return combined_data\n",
    "\n",
    "\n",
    "initial_files = [f'../data/previous_dbs/final_db{i}.json' for i in range(1,5)]\n",
    "initial_db = combine_json(initial_files)\n",
    "cleaned_db_file = [\"../data/cleaned_articles.json\"]\n",
    "cleaned_db = combine_json(cleaned_db_file)\n",
    "phrase_bert_file = ['../data/final_db_pb_tags.json']\n",
    "phrase_bert_db = combine_json(phrase_bert_file)\n",
    "\n",
    "for initial in initial_db:\n",
    "    initial_id = initial['id']\n",
    "    for clean in cleaned_db:\n",
    "        if clean['_id'] == initial_id:\n",
    "            initial['article_url'] = clean['_source']['content_metadata']['article_url']\n",
    "            try:\n",
    "                initial['st_id'] = clean['_source']['identification']['drupal']['id']\n",
    "            except KeyError:\n",
    "                initial['st_id'] = clean['_source']['identification']['cue']['id']\n",
    "    for pb in phrase_bert_db:\n",
    "        if pb['id'] == initial_id:\n",
    "            initial['phrase_Bert_tags_embeddings'] = pb['phrase_Bert_tags_embeddings']\n",
    "for initial in initial_db:\n",
    "    initial.pop('id', None)\n",
    "filtered_initial = [d for d in initial_db if all(value is not None for value in d.values())]\n",
    "def convert_lists_to_string(dicts, keys_to_convert):\n",
    "    \"\"\"\n",
    "    Convert lists to string representations for specified keys in each dictionary.\n",
    "    \n",
    "    :param dicts: List of dictionaries to process.\n",
    "    :param keys_to_convert: List of keys whose values need to be converted from lists to strings.\n",
    "    :return: List of dictionaries with the specified keys' values converted to strings.\n",
    "    \"\"\"\n",
    "    for d in dicts:\n",
    "        for key in keys_to_convert:\n",
    "            if key in d and isinstance(d[key], list):\n",
    "                try:\n",
    "                    # Convert the list to a string representation\n",
    "                    d[key] = json.dumps(d[key], ensure_ascii=False)\n",
    "                except (TypeError, ValueError) as e:\n",
    "                    pass  # Handle or log the error as needed\n",
    "    return dicts\n",
    "\n",
    "keys_to_convert = ['Title_embeddings', 'tags_embeddings', 'phrase_Bert_tags_embeddings']\n",
    "filtered_initial = convert_lists_to_string(filtered_initial, keys_to_convert)\n",
    "filtered_initial[0]\n",
    "final_df = pd.DataFrame(filtered_initial)\n",
    "df_train, df_test = train_test_split(final_df, test_size=7)\n",
    "\n",
    "# Load configuration\n",
    "with open('../gradio_config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Extract MongoDB configuration\n",
    "mongo_uri = config['database']['uri']\n",
    "db_name = config['database']['name']\n",
    "train_collection_name = config['database']['train_collection']\n",
    "test_collection_name = config['database']['test_collection']\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(mongo_uri)\n",
    "db = client[db_name]\n",
    "train_collection = db[train_collection_name]\n",
    "test_collection = db[test_collection_name]\n",
    "\n",
    "# Delete all documents in the collections\n",
    "train_collection.delete_many({})\n",
    "test_collection.delete_many({})\n",
    "\n",
    "# Convert DataFrames to list of dictionaries\n",
    "train_records = df_train.to_dict(orient='records')\n",
    "test_records = df_test.to_dict(orient='records')\n",
    "\n",
    "# Insert records into MongoDB\n",
    "train_collection.insert_many(train_records)\n",
    "test_collection.insert_many(test_records)\n",
    "\n",
    "print(\"New Data successfully deleted and inserted into MongoDB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import re\n",
    "from json import JSONDecodeError\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# Import libraries for working with language models and Google Gemini\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "GEMINI_KEY = os.environ.get('GEMINI_KEY')\n",
    "genai.configure(api_key=GEMINI_KEY)\n",
    "\n",
    "# Normally where to do this? (in which function?)\n",
    "with open(\"../gradio_config.yaml\", \"r\") as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "# Initialise mongo client.\n",
    "mongo_client = MongoClient(config[\"database\"][\"uri\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching article data from MongoDB...\n",
      "\n",
      "Data successfully fetched from MongoDB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_mongo_test():\n",
    "    print(\"Fetching article data from MongoDB...\\n\")\n",
    "    # Connect to the MongoDB client\n",
    "    try:\n",
    "        db = mongo_client[config[\"database\"][\"name\"]]\n",
    "        train_docs = db[config[\"database\"][\"test_collection\"]].find()\n",
    "        print(\"Data successfully fetched from MongoDB\\n\")\n",
    "    except Exception as error: \n",
    "        print(f\"Unable to fetch data from MongoDB. Check your connection the database...\\n\")\n",
    "        print(f\"ERROR: {error}\\n\")\n",
    "        sys.exit()\n",
    "    return train_docs\n",
    "\n",
    "test_database = load_mongo_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "def count_test_length(test_database):\n",
    "    count = 0\n",
    "    for doc in test_database:\n",
    "        count += 1\n",
    "    return count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
