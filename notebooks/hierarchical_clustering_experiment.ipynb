{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "import re\n",
    "import json\n",
    "from tqdm import trange\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Import libraries for working with language models and Google Gemini\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "chat_model = \"llama3-8b-8192\"\n",
    "groq_api_key = os.environ['GROQ_API_KEY']\n",
    "GEMINI_KEY   = os.environ['GEMINI_KEY']\n",
    "genai.configure(api_key=GEMINI_KEY)\n",
    "\n",
    "\n",
    "# Normally where to do this? (in which function?)\n",
    "with open(\"../config.yaml\", \"r\") as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "# Initialise mongo client.\n",
    "mongo_client = MongoClient(config[\"database\"][\"uri\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data successfully fetched from MongoDB\n",
      "\n",
      "Test data successfully fetched from MongoDB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_mongodb():\n",
    "    \"\"\"\n",
    "    Connects to the MongoDB client and fetches train and test data from specified collections.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: A concatenated DataFrame containing both train and test data.\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If there is an issue connecting to the MongoDB or fetching data.\n",
    "    \"\"\"\n",
    "    # Attempt to connect to MongoDB and fetch train data\n",
    "    try:\n",
    "        db = mongo_client[config[\"database\"][\"name\"]]\n",
    "        train_documents = db[config[\"database\"][\"train_collection\"]].find()\n",
    "        print(\"Train data successfully fetched from MongoDB\\n\")\n",
    "    except Exception as error: \n",
    "        print(f\"Unable to fetch train data from MongoDB. Check your connection to the database...\\nERROR: {error}\\n\")\n",
    "        sys.exit()  # Exit the program if train data cannot be fetched\n",
    "\n",
    "    # Attempt to fetch test data from MongoDB\n",
    "    try:\n",
    "        test_docs = db[config[\"database\"][\"test_collection\"]].find()\n",
    "        print(\"Test data successfully fetched from MongoDB\\n\")\n",
    "    except Exception as error: \n",
    "        print(f\"Unable to fetch test data from MongoDB. Check your connection to the database...\\nERROR: {error}\\n\")\n",
    "        sys.exit()  # Exit the program if test data cannot be fetched\n",
    "\n",
    "    # Convert MongoDB documents to DataFrames\n",
    "    df_train = pd.DataFrame.from_dict(list(train_documents))\n",
    "    df_test = pd.DataFrame.from_dict(list(test_docs))\n",
    "    \n",
    "    # Concatenate train and test DataFrames\n",
    "    df = pd.concat([df_train, df_test], axis=0)\n",
    "    return df\n",
    "\n",
    "def scale_body_embeddings(df):\n",
    "    \"\"\"\n",
    "    Processes and scales the embedding data from a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): A DataFrame containing the embedding data.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Scaled embedding data as a NumPy array.\n",
    "    \"\"\"\n",
    "    print.info(\"Processing embedding data and scaling data...\\n\")\n",
    "    \n",
    "    # Deserialize the embeddings from strings to lists\n",
    "    body_embeddings = np.array(df['embeddings'].apply(ast.literal_eval).tolist())\n",
    "    return body_embeddings\n",
    "\n",
    "def groq_header(article_titles):\n",
    "    \"\"\"\n",
    "    Generates a common theme for a list of article titles using a language model.\n",
    "    \n",
    "    Args:\n",
    "        article_titles (str): A string containing a list of article titles.\n",
    "        \n",
    "    Returns:\n",
    "        str: A title that represents the common theme of the provided article titles.\n",
    "    \"\"\"\n",
    "    # Define your desired data structure for the common theme\n",
    "    class common_theme(BaseModel):\n",
    "        Title: str = Field(description=\"Common theme of article titles\")\n",
    "    \n",
    "    # Initialize the ChatGroq model\n",
    "    chat = ChatGroq(temperature=0, model_name=chat_model)\n",
    "    \n",
    "    # Define the prompt template for the language model\n",
    "    template = '''\n",
    "    You are a news article editor. Given a list of article titles, you are to form a common theme for them in one sentence. \n",
    "    I do not require an explanation, just the title that you deem well represents the common theme.\n",
    "\n",
    "    Series of article titles:\n",
    "    {text}\n",
    "\n",
    "    Answer format example:\n",
    "    {{\"Title\": \"Israel-Hamas Conflict and Gaza Crisis\"}}\n",
    "    {{\"Title\": \"Tennis players throw rackets\"}}\n",
    "\n",
    "    Before you return the answer, ensure and double check that you have adhered the answer format instructions strictly.\n",
    "    '''\n",
    "    \n",
    "    # Create a PromptTemplate with the defined template and input variable\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"text\"],\n",
    "    )\n",
    "    \n",
    "    # Chain the prompt with the chat model\n",
    "    chain = prompt | chat \n",
    "    \n",
    "    # Invoke the chain with the article titles and get the result\n",
    "    result = chain.invoke({\"text\": article_titles}).content\n",
    "    result = json.loads(result)\n",
    "    return result['Title']\n",
    "\n",
    "def obtain_distribution_table(pre_comuted_cluster):\n",
    "    \"\"\"\n",
    "    Computes a distribution table of cluster statistics for various distance thresholds.\n",
    "\n",
    "    Args:\n",
    "        pre_comuted_cluster (ndarray): Precomputed hierarchical clustering linkage matrix.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with statistics (mean, standard deviation, and percentiles) \n",
    "                   for each distance threshold.\n",
    "        dict: A dictionary with detailed statistics for each distance threshold.\n",
    "    \"\"\"\n",
    "    dic = {}  # Dictionary to store the statistics for each distance threshold\n",
    "    max_d_range = np.arange(0.4, 0.82, 0.02).tolist()  # Range of distance thresholds\n",
    "\n",
    "    for d in max_d_range:\n",
    "        d = round(d, 2)  # Round the distance threshold to two decimal places\n",
    "        clusters_train = fcluster(pre_comuted_cluster, d, criterion='distance')\n",
    "        \n",
    "        # Get unique cluster labels and their counts\n",
    "        unique_elements, counts = np.unique(clusters_train, return_counts=True)\n",
    "\n",
    "        # Combine unique elements and counts into a dictionary for better readability\n",
    "        cluster_counts = dict(zip(unique_elements, counts))  \n",
    "        \n",
    "        # Extract the counts (number of articles in each cluster)\n",
    "        article_counts = np.array(list(cluster_counts.values()))\n",
    "\n",
    "        # Calculate mean, median, and standard deviation\n",
    "        mean_articles = np.mean(article_counts)\n",
    "        stddev_articles = np.std(article_counts)\n",
    "\n",
    "        # Specify the percentiles you want to calculate\n",
    "        percentiles = sorted(list(np.arange(1, 101)))\n",
    "        percentile_values = np.percentile(article_counts, percentiles)\n",
    "\n",
    "        # Initialize the dictionary for the current distance threshold\n",
    "        dic[d] = {}\n",
    "        dic[d]['num_clusters'] = len(article_counts)\n",
    "\n",
    "        # Add mean, median, and standard deviation to the dictionary\n",
    "        dic[d]['mean'] = round(mean_articles, 2)\n",
    "        dic[d]['std'] = round(stddev_articles, 2)\n",
    "\n",
    "        # Add the percentiles to the dictionary\n",
    "        for i, percentile in enumerate(percentiles):\n",
    "            dic[d][f'{percentile}'] = round(percentile_values[i], 2)\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    data = []\n",
    "    for d, stats in dic.items():\n",
    "        for key, value in stats.items():\n",
    "            if key in ['mean', 'std']:\n",
    "                data.append({'max_d': d, 'statistic': key, 'value': value})\n",
    "            elif key not in ['num_clusters']:\n",
    "                percentile = key\n",
    "                data.append({'max_d': d, 'statistic': f'{percentile}', 'value': value})\n",
    "\n",
    "    # Create DataFrame with all values\n",
    "    distribution_df = pd.DataFrame(data)\n",
    "\n",
    "    # Pivot the DataFrame to have percentiles and other statistics as columns\n",
    "    pivot_df = distribution_df.pivot(index='max_d', columns='statistic', values='value')\n",
    "    pivot_df.reset_index(inplace=True)\n",
    "\n",
    "    # Rename the columns for better readability (optional)\n",
    "    pivot_df.columns.name = None\n",
    "    pivot_df.rename(columns={col: f'{col}' if isinstance(col, int) else col for col in pivot_df.columns}, inplace=True)\n",
    "    return pivot_df, dic\n",
    "\n",
    "def plot_distribution(dic, max_d):\n",
    "    \"\"\"\n",
    "    Plots the distribution of article counts for a specified distance threshold.\n",
    "\n",
    "    Args:\n",
    "        dic (dict): Dictionary containing cluster statistics for various distance thresholds.\n",
    "        max_d (float): The specific distance threshold to plot.\n",
    "    \"\"\"\n",
    "    data = dic[max_d]\n",
    "    \n",
    "    # Extract percentile values (excluding 'num_clusters', 'mean', 'std')\n",
    "    percentiles = {int(k): v for k, v in data.items() if k not in ['num_clusters', 'mean', 'std']}\n",
    "\n",
    "    # Create lists for x (percentiles) and y (article counts)\n",
    "    x = list(percentiles.keys())\n",
    "    y = list(percentiles.values())\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    plt.bar(x, y, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel('Percentiles')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.title('Distribution of Articles at Each Percentile')\n",
    "    plt.xticks(ticks=range(0, 101, 10))\n",
    "    plt.grid(axis='y')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "def plot_stats(dic):\n",
    "    \"\"\"\n",
    "    Plots the mean and standard deviation of article counts across different distance thresholds.\n",
    "\n",
    "    Args:\n",
    "        dic (dict): Dictionary containing cluster statistics for various distance thresholds.\n",
    "    \"\"\"\n",
    "    # Prepare data for plotting\n",
    "    datasets = [{'max_d': max_d, 'num_clusters': props['num_clusters'], 'mean': props['mean'], 'std': props['std']} for max_d, props in dic.items()]\n",
    "\n",
    "    # Extract max_d, mean, and std values\n",
    "    max_ds = [data['max_d'] for data in datasets]\n",
    "    means = [data['mean'] for data in datasets]\n",
    "    stds = [data['std'] for data in datasets]\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(20, 12))\n",
    "\n",
    "    # Plot mean values with error bars for standard deviation\n",
    "    plt.errorbar(max_ds, means, yerr=stds, fmt='-o', capsize=5, capthick=2, ecolor='red', color='blue', label='Mean with Std Dev')\n",
    "\n",
    "    # Labeling the plot\n",
    "    plt.xlabel('max_d')\n",
    "    plt.ylabel('Mean Number of Articles')\n",
    "    plt.title('Mean and Standard Deviation of Articles Across Datasets')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def find_clusters_in_percentile(df, cluster_counts, pivot_df, max_d, percentile):\n",
    "    \"\"\"\n",
    "    Identifies and labels clusters in the specified percentile based on article count.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame containing article data with cluster labels.\n",
    "        cluster_counts (dict): Dictionary containing cluster labels and their counts.\n",
    "        pivot_df (DataFrame): DataFrame containing percentile statistics for various distance thresholds.\n",
    "        max_d (float): The distance threshold to consider.\n",
    "        percentile (int): The percentile to find clusters for (e.g., 99 for the 99th percentile).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing details of clusters in the specified percentile.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame for the specified distance threshold\n",
    "    distri_df = pivot_df[round(pivot_df[\"max_d\"], 2) == max_d].reset_index()\n",
    "\n",
    "    # Determine the article count threshold for the specified percentile\n",
    "    percentile_article_num = distri_df[str(percentile)][0]\n",
    "    next_percentile = percentile + 1 if percentile < 99 else percentile\n",
    "\n",
    "    # Identify clusters that meet the article count criteria for the specified percentile\n",
    "    clusters_at_percentile = [\n",
    "        cluster for cluster, cluster_count in cluster_counts.items()\n",
    "        if (percentile == 99 and cluster_count >= percentile_article_num) or\n",
    "        (cluster_count >= percentile_article_num and cluster_count < distri_df[str(next_percentile)][0])\n",
    "    ]\n",
    "\n",
    "    # Limit the number of clusters to process if there are more than 30\n",
    "    if len(clusters_at_percentile) > 30:\n",
    "        clusters_at_percentile = clusters_at_percentile[:25]\n",
    "        print(\"More than 30 articles in this percentile, shortening it to 25 to account for Groq limits...\\n\")\n",
    "\n",
    "    percentile_articles = []\n",
    "\n",
    "    # Generate AI labels and details for each identified cluster\n",
    "    for i in trange(len(clusters_at_percentile)):\n",
    "        label = list(cluster_counts.keys())[clusters_at_percentile[i] - 1]\n",
    "        s = \"\"\n",
    "        count = 0\n",
    "\n",
    "        # Accumulate article titles for the current cluster\n",
    "        for t in df[df['Cluster_labels'] == label]['Title'].values.tolist():\n",
    "            s += f\"- {t}\\n\"\n",
    "            count += 1\n",
    "\n",
    "        # Generate an AI label for the cluster\n",
    "        generated_label = groq_header(s)\n",
    "\n",
    "        # Create a dictionary with cluster details\n",
    "        cluster_details = {\n",
    "            'Cluster number': clusters_at_percentile[i],\n",
    "            'Cluster label': generated_label,\n",
    "            'Number of articles': count\n",
    "        }\n",
    "        percentile_articles.append(cluster_details)\n",
    "\n",
    "    # Convert numeric types in the cluster details to standard Python types\n",
    "    for d in percentile_articles:\n",
    "        for key, value in d.items():\n",
    "            if isinstance(value, (np.int32, np.int64)):\n",
    "                d[key] = int(value)\n",
    "            elif isinstance(value, (np.float32, np.float64)):\n",
    "                d[key] = float(value)\n",
    "\n",
    "    # Convert the list of dictionaries to a JSON string for display\n",
    "    json_string = json.dumps(percentile_articles, indent=4)\n",
    "\n",
    "    # Print the generated labels and details of the clusters\n",
    "    print(f\"Generated Labels of clusters in the {percentile} percentile:\\n\")\n",
    "    for d in percentile_articles:\n",
    "        print(f\"- {d['Cluster label']}\\n\")\n",
    "    print(f\"Details of clusters in the {percentile} percentile:\\n\")\n",
    "    print(json_string)\n",
    "    \n",
    "    return percentile_articles\n",
    "\n",
    "def view_cluster(df, cluster_counts, cluster_num):\n",
    "    \"\"\"\n",
    "    Displays the titles and AI-generated label of a specified cluster.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame containing article data with cluster labels.\n",
    "        cluster_counts (dict): Dictionary containing cluster labels and their counts.\n",
    "        cluster_num (int): The cluster number to view (1-based index).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Get the actual label of the cluster from the dictionary using the 1-based index\n",
    "    label = list(cluster_counts.keys())[cluster_num - 1]\n",
    "    \n",
    "    # Initialize a string to accumulate article titles\n",
    "    s = \"\"\n",
    "    count = 0  # Counter for the number of articles in the cluster\n",
    "\n",
    "    # Retrieve article titles for the specified cluster label and concatenate them into the string\n",
    "    for t in df[df['Cluster_labels'] == label]['Title'].values.tolist():\n",
    "        s += f\"- {t}\\n\"\n",
    "        count += 1  # Increment the article count\n",
    "\n",
    "    # Generate an AI label for the cluster using the concatenated titles\n",
    "    generated_label = groq_header(s)\n",
    "    \n",
    "    # Print the cluster number and AI-generated label\n",
    "    print(f\"Cluster No: {label}\\n\")\n",
    "    print(f\"AI generated label of cluster: \\n{generated_label}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Print each article title in the cluster\n",
    "    for t in df[df['Cluster_labels'] == label]['Title'].values.tolist():\n",
    "        print(f\"- {t}\\n\")\n",
    "    \n",
    "    # Print the total number of articles in the cluster\n",
    "    print(f\"Number of articles in cluster: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration to see the clusters that are in the different percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load database\n",
    "df= load_mongodb()\n",
    "\n",
    "# Change type of embeddings accordingly\n",
    "train_embeddings = scale_body_embeddings(df)\n",
    "\n",
    "# Pre computed hierarchical clustering\n",
    "Z = linkage(train_embeddings, method='average', metric='cosine')\n",
    "\n",
    "# Obtain distribution table\n",
    "pivot_df, dic = obtain_distribution_table(Z)\n",
    "\n",
    "# Pretty Print distribution table\n",
    "print(\"Distribution for Body embeddings:\")\n",
    "print(tabulate(pivot_df, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(dic, 0.58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pre computed cluster to predict at particular max_d\n",
    "# max_d of 0.56 to 0.6 seem to be the optimal \n",
    "max_d = 0.58\n",
    "clusters_train = fcluster(Z, max_d, criterion='distance')\n",
    "    \n",
    "# Get unique elements and their counts\n",
    "unique_elements, counts = np.unique(clusters_train, return_counts=True)\n",
    "\n",
    "# Combine unique elements and counts into a dictionary for better readability\n",
    "cluster_counts = dict(zip(unique_elements, counts))  \n",
    "    \n",
    "# Extract the counts (number of articles in each cluster)\n",
    "article_counts = np.array(list(cluster_counts.values()))\n",
    "\n",
    "labels = clusters_train.tolist()\n",
    "df['Cluster_labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = find_clusters_in_percentile(df, pivot_df, 0.58, 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_cluster(df, cluster_counts, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting these details into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = len(set(labels))\n",
    "cluster = df[df[\"Cluster_labels\"] == 1].reset_index()\n",
    "cluster_title_ls = cluster['Title'].values.tolist()\n",
    "\n",
    "def get_cluster_details(df, test_cluster):\n",
    "    cluster = df[df[\"Cluster_labels\"] == test_cluster].reset_index()\n",
    "    cluster_title_ls = cluster['Title'].values.tolist()\n",
    "    cluster_size = len(cluster_title_ls)\n",
    "    if cluster_size > 1:\n",
    "        titles = \"\"\n",
    "        # Generate the list\n",
    "        for i in range(len(cluster_title_ls)):\n",
    "            titles += f\"- {cluster_title_ls[i]}\\n\"\n",
    "        # generate label for this cluster\n",
    "        cluster_label = groq_header(titles)\n",
    "    else:\n",
    "        cluster_label = \"SAME AS TITLE: \" + cluster_title_ls[0]\n",
    "    return cluster_size, cluster_label\n",
    "\n",
    "details = {}\n",
    "batch_size = 30\n",
    "total = num_clusters + 1  # Update this to the total number of iterations you need\n",
    "\n",
    "# Calculate the number of full batches\n",
    "num_batches = (total + batch_size - 1) // batch_size\n",
    "\n",
    "# Process each full batch\n",
    "k = 0\n",
    "for batch in range(num_batches):\n",
    "    start = batch * batch_size + 1\n",
    "    end = min(start + batch_size, total + 1)\n",
    "    for i in trange(start, end):\n",
    "        details[i] = {}\n",
    "        size, label = get_cluster_details(df, i)\n",
    "        details[i]['Cluster_size'] = size\n",
    "        details[i]['Cluster_label'] = label\n",
    "    if batch < num_batches - 1:\n",
    "        print(f\"Batch {k+1}/{num_batches} completed, cooling down for 60 seconds...\\n\")\n",
    "        time.sleep(60)  # Pause for 40 seconds after each batch\n",
    "    k += 1\n",
    "\n",
    "# Process any remaining items that don't form a full batch\n",
    "remainder = total % batch_size\n",
    "if remainder > 0 and num_batches * batch_size + 1 <= total:\n",
    "    start = num_batches * batch_size + 1\n",
    "    end = total + 1\n",
    "    for i in trange(start, end):\n",
    "        details[i] = {}\n",
    "        size, label = get_cluster_details(df, i)\n",
    "        details[i]['Cluster_size'] = size\n",
    "        details[i]['Cluster_label'] = label\n",
    "\n",
    "details.popitem()\n",
    "new_df = df.drop(['_id', 'embeddings', 'combined', 'tags_embeddings', 'Title_embeddings', 'phrase_Bert_tags_embeddings'], axis=1)\n",
    "details_df = pd.DataFrame.from_dict(details, orient='index').reset_index()\n",
    "details_df.rename(columns={'index': 'Cluster_id', 'Cluster_label': 'Generated_labels'}, inplace=True)\n",
    "merged_df = pd.merge(new_df, details_df, left_on='Cluster_labels', right_on='Cluster_id', how='left')\n",
    "merged_df.drop(columns=['Cluster_labels'], inplace=True)\n",
    "merged_df = merged_df[['st_id', 'Publication_date', 'Text', 'Title', 'tags', 'Cluster_id', 'Cluster_size', 'Generated_labels', 'article_url']]\n",
    "merged_df.to_csv(\"../data/clustering_results_with_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising sub clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cluster(target_cluster):\n",
    "    target_data = df[df[\"Cluster_labels\"] == target_cluster].reset_index()\n",
    "    subset_train_embeddings = scale_body_embeddings(target_data)\n",
    "    # Perform hierarchical clustering on the subset\n",
    "    Z = linkage(subset_train_embeddings, method='average', metric='cosine')\n",
    "    return Z\n",
    "\n",
    "\n",
    "def show_subcluster(df, target_cluster):\n",
    "    Z_target = compute_cluster(target_cluster)\n",
    "    # Plot the dendrogram for the sub-cluster\n",
    "    plt.figure(figsize=(20, 7))\n",
    "    plt.title(f\"Dendrogram for Sub-Clusters within Cluster {target_cluster}\")\n",
    "    dendrogram(Z_target)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_subcluster(df, target_cluster=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cluster = 3\n",
    "i = 0\n",
    "dic = {}\n",
    "max_d_range = np.arange(0.8, 1.2,0.02).tolist()\n",
    "for d in max_d_range:\n",
    "        clusters_train = fcluster(compute_cluster(target_cluster), d, criterion='distance')\n",
    "        \n",
    "        # Get unique elements and their counts\n",
    "        unique_elements, counts = np.unique(clusters_train, return_counts=True)\n",
    "\n",
    "        # Combine unique elements and counts into a dictionary for better readability\n",
    "        cluster_counts = dict(zip(unique_elements, counts))  \n",
    "        \n",
    "        # Extract the counts (number of articles in each cluster)\n",
    "        article_counts = np.array(list(cluster_counts.values()))\n",
    "\n",
    "        # Calculate mean, median, and standard deviation\n",
    "        mean_articles = np.mean(article_counts)\n",
    "        stddev_articles = np.std(article_counts)\n",
    "\n",
    "        # Specify the percentiles you want to calculate\n",
    "        percentiles = [10, 25, 33, 65, 50, 70, 75, 80,81,82,83,84,85, 86,87,88,89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
    "        percentile_values = np.percentile(article_counts, percentiles)\n",
    "\n",
    "        # Initialize the dictionary for the cluster\n",
    "        dic[d] = {}\n",
    "        dic[d]['num_clusters'] = len(article_counts)\n",
    "\n",
    "        # Add mean, median, and standard deviation\n",
    "        dic[d]['mean'] = round(mean_articles, 2)\n",
    "        dic[d]['std'] = round(stddev_articles, 2)\n",
    "\n",
    "        # Add the percentiles\n",
    "        for i, percentile in enumerate(percentiles):\n",
    "            dic[d][f'{percentile}th'] = round(percentile_values[i], 2)\n",
    "        i += 1\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "data = []\n",
    "\n",
    "for d, stats in dic.items():\n",
    "        for key, value in stats.items():\n",
    "            if key in ['mean', 'std']:\n",
    "                data.append({'max_d': d, 'statistic': key, 'value': value})\n",
    "            elif key not in ['num_clusters']:\n",
    "                percentile = int(key.replace('th', ''))\n",
    "                data.append({'max_d': d, 'statistic': f'{percentile}', 'value': value})\n",
    "\n",
    "# Create DataFrame with all values\n",
    "distribution_df = pd.DataFrame(data)\n",
    "\n",
    "# Pivot the DataFrame to have percentiles and other statistics as columns\n",
    "pivot_df = distribution_df.pivot(index='max_d', columns='statistic', values='value')\n",
    "pivot_df.reset_index(inplace=True)\n",
    "\n",
    "# Rename the columns for better readability (optional)\n",
    "pivot_df.columns.name = None\n",
    "pivot_df.rename(columns={col: f'{col}' if isinstance(col, int) else col for col in pivot_df.columns}, inplace=True)    \n",
    "print(\"Distribution for Body embeddings:\")\n",
    "print(tabulate(pivot_df, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_d = 1.0\n",
    "# Plot the dendrogram for the sub-cluster\n",
    "plt.figure(figsize=(20, 7))\n",
    "plt.title(f\"Dendrogram for Sub-Clusters within Cluster {target_cluster}\")\n",
    "dendrogram(compute_cluster(target_cluster))\n",
    "plt.axhline(y=chosen_d, color='r', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_d = chosen_d\n",
    "clusters_train = fcluster(Z_target, max_d, criterion='distance')\n",
    "    \n",
    "# Get unique elements and their counts\n",
    "unique_elements, counts = np.unique(clusters_train, return_counts=True)\n",
    "\n",
    "# Combine unique elements and counts into a dictionary for better readability\n",
    "cluster_counts = dict(zip(unique_elements, counts))  \n",
    "    \n",
    "# Extract the counts (number of articles in each cluster)\n",
    "article_counts = np.array(list(cluster_counts.values()))\n",
    "\n",
    "labels = clusters_train.tolist()\n",
    "target_data = df[df[\"Cluster_labels\"] == target_cluster].reset_index()\n",
    "\n",
    "target_data['Cluster_labels'] = labels\n",
    "\n",
    "# WHat i want to find out is what the labels of each cluster are in each of these percentiles. \n",
    "def view_cluster(df, cluster_num):\n",
    "    label = list(cluster_counts.keys())[cluster_num-1]\n",
    "    print(f\"Cluster {label}\\n\")\n",
    "    print(\"-\" * 80)\n",
    "    s = \"\"\n",
    "    count = 0\n",
    "    for t in df[df['Cluster_labels'] == label]['Title'].values.tolist():\n",
    "        s += f\"- {t}\\n\" \n",
    "        print(f\"- {t}\\n\")\n",
    "        count += 1\n",
    "    print(f\"Number of articles: {count}\")\n",
    "    generated_label = groq_header(s)\n",
    "    print(f\"Generated label: {generated_label}\\n\")\n",
    "    return generated_label\n",
    "\n",
    "sub_cluster_labels = []\n",
    "for i in range(len(set(labels))):\n",
    "    sub_cluster_label = view_cluster(target_data, i)\n",
    "    sub_cluster_labels.append(sub_cluster_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
