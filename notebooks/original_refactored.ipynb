{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "from sentence_transformers import CrossEncoder\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import json\n",
    "import re\n",
    "from json import JSONDecodeError\n",
    "import torch\n",
    "from torch import nn\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# Import libraries for working with language models and Google Gemini\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Read configuration\n",
    "config_file = '../config.yaml'\n",
    "with open(config_file, 'r') as fin:\n",
    "    config = yaml.safe_load(fin)\n",
    "# end with\n",
    "\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "chat_model = \"llama3-8b-8192\"\n",
    "\n",
    "GEMINI_KEY   = os.environ['GEMINI_KEY']\n",
    "MONGO_URI    = os.environ['MONGO_URI']\n",
    "HF_KEY       = os.environ['HUGGINGFACE_API_KEY']\n",
    "EMBEDDER_API = os.environ[\"HF_EMBEDDING_MODEL_URL\"]\n",
    "\n",
    "genai.configure(api_key=GEMINI_KEY)\n",
    "\n",
    "# Initialise Mongodb client\n",
    "mongo_client = MongoClient(MONGO_URI)\n",
    "\n",
    "# Setup default LLM model\n",
    "default_llm = genai.GenerativeModel('gemini-1.5-flash-latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_database():\n",
    "    # Connect to the MongoDB client\n",
    "    try:\n",
    "        db = mongo_client[config[\"database\"][\"name\"]]\n",
    "        train_documents = db[config[\"database\"][\"train_collection\"]].find()\n",
    "        logger.info(\"Train data successfully fetched from MongoDB\\n\")\n",
    "    except Exception as error: \n",
    "        logger.error(f\"Unable to fetch train data from MongoDB. Check your connection the database...\\nERROR: {error}\\n\")\n",
    "    \n",
    "    try:\n",
    "        test_docs = db[config[\"database\"][\"test_collection\"]].find()\n",
    "        logger.info(\"Test data successfully fetched from MongoDB\\n\")\n",
    "    except:\n",
    "        logger.error(f\"Unable to fetch test data from MongoDB. Check your connection the database...\\nERROR: {error}\\n\")\n",
    "    \n",
    "    df_train = pd.DataFrame.from_dict(list(train_documents))\n",
    "    df_test = pd.DataFrame.from_dict(list(test_docs))\n",
    "    \n",
    "    # Row bind the training and test dataframes \n",
    "    df = pd.concat([df_train, df_test], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "# end def\n",
    "\n",
    "def get_test_article(db, test_id):\n",
    "    test_article = []\n",
    "    for article in db:\n",
    "        if article['st_id'] == test_id:\n",
    "            test_article.append(article)\n",
    "            break\n",
    "        # end if\n",
    "    # end for\n",
    "    return test_article[0]\n",
    "# end def\n",
    "\n",
    "def get_text_embeddings(df):\n",
    "    print(\"Fetching embeddings...\\n\")\n",
    "    #Deserializing the embeddings\n",
    "    body_embeddings = np.array(df['embeddings'].apply(ast.literal_eval).tolist())\n",
    "    return body_embeddings\n",
    "\n",
    "def get_predicted_cluster(dataframe, test_id):\n",
    "    test_article = dataframe[dataframe['st_id'] == test_id].reset_index(drop=True)\n",
    "    predicted_cluster = test_article['Cluster_label'][0]\n",
    "    return predicted_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, compare similarity from test article to that of articles in the cluster.\n",
    "# Use cross encoder for most similar by text, filter out top 7 articles, to be in \"relevant articles\"\n",
    "\n",
    "\n",
    "# Generate the cosine similarity between texts of 2 articles\n",
    "def get_text_similarity(timeline_embedding,train_article):\n",
    "    \n",
    "    cos_sim = nn.CosineSimilarity(dim=0)\n",
    "    \n",
    "    text_embedding = torch.tensor(eval(train_article['embeddings']))\n",
    "    timeline_embedding = torch.tensor(timeline_embedding)\n",
    "    \n",
    "    similarity_score = cos_sim(timeline_embedding, text_embedding)\n",
    "    return similarity_score\n",
    "# end def\n",
    "\n",
    "# Generate the cosine similarity between texts of 2 articles\n",
    "def get_text_similarity(test_embedding, db_embedding):\n",
    "    \n",
    "    cos_sim = nn.CosineSimilarity(dim=0)\n",
    "    similarity_score = cos_sim(torch.tensor(test_embedding), torch.tensor(db_embedding))\n",
    "    return similarity_score\n",
    "# end def\n",
    "\n",
    "\n",
    "# Find the top 20 most similar articles based on their text embedding\n",
    "def articles_ranked_by_text(test_article, database):\n",
    "    logger.info(\"Computing similarities between article texts...\")\n",
    "    test_embedding = test_article['embeddings'].apply(ast.literal_eval)[0]  \n",
    "\n",
    "    article_collection = []\n",
    "    for i in trange(len(database)):\n",
    "        article = {}\n",
    "        article['id'] = database.iloc[i]['st_id']\n",
    "        article['Title'] = database.iloc[i]['Title']\n",
    "        article['Text'] = database.iloc[i]['Text']\n",
    "        article['Date'] = database.iloc[i]['Publication_date']\n",
    "        article['Article_URL'] = database.iloc[i]['article_url']\n",
    "        article_embedding =  pd.DataFrame(database.iloc[i]).loc['embeddings'].apply(ast.literal_eval)[i]\n",
    "        article['cosine_score'] = get_text_similarity(test_embedding, article_embedding)\n",
    "        article_collection.append(article)\n",
    "    # end for\n",
    "    \n",
    "    # Sort by cosine similarity in descending order\n",
    "    article_collection.sort(key = lambda x: x['cosine_score'], reverse=True)\n",
    "    \n",
    "    # Returns the top 20 most similar articles\n",
    "    collection = article_collection[:21]\n",
    "    \n",
    "    # remove the first article as it is the test article\n",
    "    collection.pop(0)\n",
    "    return collection\n",
    "# end def\n",
    "\n",
    "# use cross encoder to filter out the top 7 articles\n",
    "# Re rank articles based on similarity derived from cross encoder\n",
    "def re_rank_articles(unique_articles, test_article, cross_encoder_model=\"cross-encoder/ms-marco-TinyBERT-L-2-v2\", top_k=6):\n",
    " \n",
    "    cross_encoder = CrossEncoder(\n",
    "        cross_encoder_model, max_length=512, device=\"cpu\"\n",
    "    )\n",
    "    test_article_text = test_article['Text'][0]\n",
    "    # Format the timeline header and article for cross encoder\n",
    "    unranked_articles = [(test_article_text, doc['Text']) for doc in unique_articles]\n",
    "    \n",
    "    # Predict similarity_scores\n",
    "    similarity_scores = cross_encoder.predict(unranked_articles).tolist()\n",
    "    # Assign the list of similarity scores to the inidividual articles \n",
    "    for i in range(len(unique_articles)):\n",
    "        # if similarity_scores[i]:\n",
    "        unique_articles[i]['reranked_score'] = similarity_scores[i]\n",
    "        # end if\n",
    "    # end for\n",
    "    combined_articles = [article for article in unique_articles if 'reranked_score' in article]\n",
    "    best_articles = sorted(combined_articles, key=lambda x: x['reranked_score'], reverse=True)[:top_k]\n",
    "\n",
    "    return best_articles\n",
    "\n",
    "def get_article_dict(test_article, best_articles, df):\n",
    "    ids = [article['id'] for article in best_articles]\n",
    "    similar_indexes = []\n",
    "    for idx, row in df.iterrows():\n",
    "        if row['st_id'] in ids:\n",
    "            similar_indexes.append(idx)\n",
    "        if len(similar_indexes)==len(ids):\n",
    "            break\n",
    "\n",
    "    similar_articles_dict = {\n",
    "                    'Title': test_article['Title'][0],\n",
    "                    'indexes': similar_indexes,\n",
    "                    'Text': test_article['Text'][0],\n",
    "                }\n",
    "    return similar_articles_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_llm_score(output):\n",
    "    text = output.parts[0].text.replace(\"```\", '').replace('json','')\n",
    "    result = json.loads(text)\n",
    "    return result\n",
    "\n",
    "def clean_output(output):\n",
    "    try:\n",
    "        updated_timeline = json.loads(output)\n",
    "        return updated_timeline\n",
    "    except JSONDecodeError:\n",
    "        #try 1: Ensuring that the string ends with just the open and close lists brackets\n",
    "        try:\n",
    "            new_output = re.search(r'\\[[^\\]]*\\]', output).group(0)\n",
    "        except AttributeError:\n",
    "            new_output = re.search(r'\\{.*?\\}', output, re.DOTALL).group(0)  \n",
    "        updated_timeline = json.loads(new_output)\n",
    "        return updated_timeline\n",
    "\n",
    "def clean_llm_output(llm_output):\n",
    "        text = llm_output.parts[0].text.replace(\"```\", '').replace('json','')\n",
    "        result = json.loads(text)\n",
    "        return result\n",
    "    \n",
    "def generate_and_sort_timeline(similar_articles_dict, df_train, df_test):\n",
    "    llm = genai.GenerativeModel('gemini-1.5-flash-latest' )\n",
    "    \n",
    "    class Event(BaseModel):\n",
    "        Date: str = Field(description=\"The date of the event in YYYY-MM-DD format\")\n",
    "        Event: str = Field(description=\"A detailed description of the important event\")\n",
    "        Article: int = Field(description=\"The article number from which the event was extracted\")\n",
    "\n",
    "    output_parser = JsonOutputParser(pydantic_object=Event)\n",
    "\n",
    "    # See the prompt template you created for formatting\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    template = '''\n",
    "    Given an article, containing a publication date, title, and content, your task is to construct a detailed timeline of events leading up to the main event described in the article.\n",
    "    Begin by thoroughly analyzing the title, content, and publication date of the article to understand the main event in the article. \n",
    "    the dates are represented in YYYY-MM-DD format. Identify events, context, and any time references such as \"last week,\" \"last month,\" or specific dates. \n",
    "    The article could contain more or one key events. \n",
    "    If the article does not provide a publication date or any events leading up to the main event, return NAN in the Date field, and 0 i the Article Field\n",
    "\n",
    "    Construct the Timeline:\n",
    "    Chronological Order: Organize the events chronologically, using the publication dates and time references within the articles.\n",
    "    Detailed Descriptions: Provide detailed descriptions of each event, explaining how it relates to the main event of the first article.\n",
    "    Contextual Links: Use information from the articles to link events together logically and coherently.\n",
    "    Handle Ambiguities: If an article uses ambiguous time references, infer the date based on the publication date of the article and provide a clear rationale for your inference.\n",
    "\n",
    "    Contextual Links:\n",
    "    External Influences: Mention any external influences (e.g., global conflicts, economic trends, scientific discoveries) that might have indirectly affected the events.\n",
    "    Internal Issues: Highlight any internal issues or developments (e.g., political changes, organizational restructuring, societal movements) within the entities involved that might have impacted the events.\n",
    "    Efforts for Improvement: Note any indications of efforts to improve the situation (e.g., policy changes, strategic initiatives, collaborative projects) despite existing challenges.\n",
    "\n",
    "    Be as thorough and precise as possible, ensuring the timeline accurately reflects the sequence and context of events leading to the main event.\n",
    "\n",
    "    Article:\n",
    "    {text}\n",
    "\n",
    "    {format_instructions}\n",
    "    Check and ensure again that the output follows the format instructions above very strictly. \n",
    "    '''\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        partial_variables={\"format_instructions\": format_instructions},\n",
    "        template=template\n",
    "    )\n",
    "    \n",
    "    def generate_individual_timeline(date_text_triples):\n",
    "        s =  f'Article {date_text_triples[0]}: Publication date: {date_text_triples[1]} Article Text: {date_text_triples[2]}'\n",
    "        final_prompt = prompt.format(text=s)\n",
    "        response = llm.generate_content(final_prompt,\n",
    "                                        safety_settings={\n",
    "                                            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, \n",
    "                                            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "                                            })\n",
    "        # Check if Model returns correct format \n",
    "        if '[' in response.parts[0].text or '{' in response.parts[0].text:\n",
    "            result = response.parts[0].text\n",
    "        else:\n",
    "            retry_response = llm.generate_content(final_prompt,\n",
    "                                        safety_settings={\n",
    "                                            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, \n",
    "                                            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "                                            })\n",
    "            try:\n",
    "                result = retry_response.parts[0].text\n",
    "            except ValueError:\n",
    "                print(\"ERROR: There were issues with the generation of the timeline. The timeline could not be generated\")\n",
    "                return\n",
    "        return result\n",
    "    \n",
    "    def process_articles(df_train):\n",
    "        df_retrieve = df_train.loc[similar_articles_dict['indexes']]\n",
    "        df_retrieve = pd.concat([df_retrieve, df_test], axis=0).iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "        # Prepare texts and publication dates\n",
    "        indiv_numbers = list(range(1,len(df_retrieve)+1))\n",
    "        indiv_text = df_retrieve['combined'].tolist()\n",
    "        indiv_dates = df_retrieve['Publication_date'].tolist()\n",
    "        date_text_triples = list(zip(indiv_numbers, indiv_text, indiv_dates))\n",
    "\n",
    "        dict_of_timelines = {}\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=len(date_text_triples)) as executor:\n",
    "            futures = {executor.submit(generate_individual_timeline, date_text_triple): date_text_triple for date_text_triple in date_text_triples}\n",
    "            i = 0\n",
    "            for future in as_completed(futures):\n",
    "                dict_of_timelines[i] = future.result()\n",
    "                i += 1\n",
    "        return dict_of_timelines, df_retrieve\n",
    "    \n",
    "    timeline_dic, df_retrieve = process_articles(df_train)\n",
    "    print(\"The first timeline has been generated\\n\")\n",
    "    generated_timeline = []\n",
    "    for _, line in timeline_dic.items():\n",
    "        indiv_timeline = clean_output(line)\n",
    "        if type(indiv_timeline) == list:\n",
    "            for el in indiv_timeline:\n",
    "                generated_timeline.append(el)\n",
    "        else:\n",
    "            generated_timeline.append(indiv_timeline)\n",
    "    \n",
    "    unsorted_timeline = []\n",
    "    for event in generated_timeline:\n",
    "        article_index = event[\"Article\"] - 1\n",
    "        event[\"Article_id\"] = df_retrieve.iloc[article_index].st_id\n",
    "    for event in generated_timeline:\n",
    "        del event[\"Article\"]\n",
    "        unsorted_timeline.append(event)  \n",
    "        \n",
    "    timeline = sorted(unsorted_timeline, key=lambda x:x['Date'])\n",
    "    finished_timeline = [event for event in timeline if event['Date'].lower()!= 'nan']\n",
    "    for i in range(len(finished_timeline)):\n",
    "        date = finished_timeline[i]['Date']\n",
    "        if date.endswith('-XX-XX') or date.endswith('00-00'):\n",
    "            finished_timeline[i]['Date'] = date[:4]\n",
    "        elif date.endswith('-XX') or date.endswith('00'):\n",
    "            finished_timeline[i]['Date'] = date[:7]\n",
    "    return finished_timeline, df_retrieve\n",
    "\n",
    "def reduce_by_date(timeline_list):\n",
    "    '''\n",
    "    Takes in a list of events in one day, and returns a list of dicts for events in that day\n",
    "    \n",
    "    '''\n",
    "    timeline_string = json.dumps(timeline_list)\n",
    "    \n",
    "    llm = genai.GenerativeModel(model_name='gemini-1.5-flash-latest')\n",
    "\n",
    "    class Event(BaseModel):\n",
    "            Event: str = Field(description=\"A detailed description of the event\")\n",
    "            Article_id: list = Field(description=\"The article id(s) from which the events were extracted\")\n",
    "\n",
    "    parser = JsonOutputParser(pydantic_object=Event)\n",
    "\n",
    "    template = '''You are a news article editor tasked with simplifying a section of a timeline of events on the same day. \n",
    "Given this snippet a timeline, filter out duplicated events. \n",
    "IF events convey the same overall meaning, I want you to merge these events into one event to avoid redundancy, and add the article ids to a list. \n",
    "However, the events are all different, do not combine them, I want you to return it as it is, however, follow the specified format instructions below. \n",
    "Furthermore, if the event is not considered to be an event worthy of an audience reading the timeline, do not include it.\n",
    "Take your time and evaluate the timeline slowly to make your decision.\n",
    "\n",
    "Timeline snippet:\n",
    "{text}\n",
    "\n",
    "{format_instructions}\n",
    "Ensure that the format follows the example output format strictly before returning the output.'''\n",
    "    prompt = PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=template,\n",
    "            partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "        )\n",
    "    final_prompt = prompt.format(text=timeline_string)\n",
    "    response = llm.generate_content(final_prompt,\n",
    "            safety_settings={\n",
    "                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, \n",
    "                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "            }\n",
    "        )\n",
    "    data = extract_json_from_string(response.parts[0].text)\n",
    "    for event_dic in data:\n",
    "        article_id_list = event_dic['Article_id']\n",
    "        if len(article_id_list)> 2:\n",
    "            shortened_ids = article_id_list\n",
    "            event_dic['Article_id'] = shortened_ids\n",
    "    return data\n",
    "\n",
    "# Creating a set of unique days. This will reduce the number of tokens to the model, and makes it easier to handle in the output\n",
    "def first_timeline_enhancement(timeline):\n",
    "    '''\n",
    "    This function takes in a timeline in list format \n",
    "    '''\n",
    "    uniq_dates = sorted(list(set([event['Date'] for event in timeline])))\n",
    "    dic = {}\n",
    "    for i in range(len(uniq_dates)):\n",
    "        dic[uniq_dates[i]] = [{'Event':event['Event'], 'Article_id': event['Article_id']} for event in timeline if event['Date'] == uniq_dates[i]]\n",
    "     \n",
    "    new_timeline = {}\n",
    "    for date, snippet in dic.items():\n",
    "        if len(snippet) == 1:\n",
    "            new_timeline[date] = snippet\n",
    "        else:\n",
    "            new_snippet = reduce_by_date(snippet)\n",
    "            new_timeline[date] =new_snippet\n",
    "    enhanced_timeline = []\n",
    "    for date, events in new_timeline.items():\n",
    "            for event in events:\n",
    "                new_event = {}\n",
    "                new_event['Date'] = date\n",
    "                new_event['Event'] = event['Event']\n",
    "                article_id = event['Article_id']\n",
    "                if isinstance(article_id, str):\n",
    "                    new_event['Article_id'] = [article_id]\n",
    "                else:\n",
    "                    new_event['Article_id'] = event['Article_id']\n",
    "                enhanced_timeline.append(new_event)\n",
    "    return enhanced_timeline\n",
    "\n",
    "def pair_article_urls(enhanced_timeline, df_retrieve):\n",
    "    \"\"\"\n",
    "    Save the enhanced timeline to a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    enhanced_timeline (list): The enhanced timeline data.\n",
    "    output_path (str): The file path where the JSON will be saved.\n",
    "    \"\"\"\n",
    "    \n",
    "    def edit_timeline(timeline):\n",
    "        for event in timeline:\n",
    "            new_date = format_timeline_date(event['Date'])\n",
    "            event['Date'] = new_date\n",
    "        return timeline\n",
    "\n",
    "    edited_timeline = edit_timeline(enhanced_timeline)\n",
    "\n",
    "    for event in edited_timeline:\n",
    "        id_list = event['Article_id']\n",
    "        url_title_pairs = []\n",
    "        \n",
    "        for i in range(len(id_list)):\n",
    "            id = id_list[i]\n",
    "            url = df_retrieve[df_retrieve['st_id'] == id]['article_url'].values[0]\n",
    "            title = df_retrieve[df_retrieve['st_id'] == id]['Title'].values[0]\n",
    "            url_title_pairs.append({'url': url, 'title': title})\n",
    "        \n",
    "        event['Article_URL'] = url_title_pairs\n",
    "        event.pop('Article_id')  \n",
    "    return edited_timeline\n",
    "\n",
    "def num_words(event_str):\n",
    "    ls = event_str.split()\n",
    "    return len(ls)\n",
    "\n",
    "def get_needed_summaries(timeline):\n",
    "    need_summary_timeline = []\n",
    "    for i in range(len(timeline)):\n",
    "        if num_words(timeline[i]['Event']) > 20:\n",
    "            need_summary_timeline.append((i,timeline[i]))\n",
    "    \n",
    "    # Get out events\n",
    "    events = {}\n",
    "    for i in range(len(need_summary_timeline)):\n",
    "        events[need_summary_timeline[i][0]] = need_summary_timeline[i][1]\n",
    "    return events\n",
    "\n",
    "def groq_summariser(events_ls):\n",
    "    # Define your desired data structure.\n",
    "    class summarized_event(BaseModel):\n",
    "        Event: str = Field(description=\"Event in a timeline\")\n",
    "        Event_Summary: str = Field(description=\"Short Summary of event\")\n",
    "    \n",
    "    parser = JsonOutputParser(pydantic_object=summarized_event)\n",
    "\n",
    "    chat = ChatGroq(temperature=0, model_name=chat_model)\n",
    "    \n",
    "    template = '''\n",
    "You are a news article editor.\n",
    "Given a list of events from a timeline, you are tasked to provide a short summary of these series of events. \n",
    "For each event, you should return the event, and the summary.\n",
    "\n",
    "Series of events:\n",
    "{text}\n",
    "\n",
    "{format_instructions}\n",
    "    '''\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"text\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "    \n",
    "    chain = prompt | chat | parser\n",
    "    \n",
    "    event_str = json.dumps(events_ls)\n",
    "    result = chain.invoke({\"text\": event_str})\n",
    "    if isinstance(result, list):\n",
    "        return result\n",
    "    else:\n",
    "        cleaned_result = clean_output(result)\n",
    "        return cleaned_result\n",
    "\n",
    "def merge_event_summaries(events_ls, llm_answer, timeline, need_summary_timeline):\n",
    "    # Need to improve error handling for this section....\n",
    "    if len(llm_answer) != len(need_summary_timeline):\n",
    "        print(\"Groq had an error where timeline summary output length not equal to input but trying to resolve\")\n",
    "        llm_answer = groq_summariser(events_ls)\n",
    "        print(len(llm_answer) == len(events_ls))\n",
    "    i = 0\n",
    "    for k,v in need_summary_timeline.items():\n",
    "        need_summary_timeline[k]['Event_Summary'] = llm_answer[i]['Event_Summary']\n",
    "        i += 1\n",
    "        \n",
    "    for i in range(len(timeline)):\n",
    "            if i in need_summary_timeline:\n",
    "                timeline[i] = need_summary_timeline[i]\n",
    "    return timeline\n",
    "\n",
    "def second_timeline_enhancement(timeline):\n",
    "    need_summary_timeline = get_needed_summaries(timeline)\n",
    "    events_ls = [event['Event'] for _, event in need_summary_timeline.items()]\n",
    "    summaries = groq_summariser(events_ls)\n",
    "    final_timeline = merge_event_summaries(events_ls, summaries, timeline, need_summary_timeline)\n",
    "    return final_timeline\n",
    "\n",
    "def generate_save_timeline(relevant_articles, df_train, df_test):\n",
    "    similar_articles = get_article_dict(relevant_articles, df_train, df_test)\n",
    "    if similar_articles == \"generate_similar_error\":\n",
    "        return \"Error02\"\n",
    "    generated_timeline, df_retrieve = generate_and_sort_timeline(similar_articles, df_train, df_test)\n",
    "    print(\"Proceeding to Stage 1/2 of enhancement...\\n\")\n",
    "    first_enhanced_timeline = first_timeline_enhancement(generated_timeline)\n",
    "    second_enhanced_timeline = pair_article_urls(first_enhanced_timeline, df_retrieve)\n",
    "    print(\"Proceeding to Stage 2/2 of enhancement...\\n\")\n",
    "    final_timeline = second_timeline_enhancement(second_enhanced_timeline)\n",
    "    print(\"Timeline enhanced.. \\n\")\n",
    "    return final_timeline\n",
    "\n",
    "def det_generate_timeline(input_data,score_threshold=3,llm=default_llm,\n",
    "    safety_settings = {\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "    }):\n",
    "    \n",
    "    \"\"\"Evaluating necessity of Timeline for this article.\"\"\"\n",
    "\n",
    "    # Initialise Pydantic object to force LLM return format\n",
    "    class Event(BaseModel):\n",
    "        score: int = Field(description=\"The need for this article to have a timeline\")\n",
    "        Reason: str = Field(description = \"The main reason for your choice why a timeline is needed or why it is not needed\")\n",
    "    \n",
    "    # Initialise Json output parser\n",
    "    output_parser = JsonOutputParser(pydantic_object=Event)\n",
    "\n",
    "    # Define the template\n",
    "    template = \\\n",
    "    '''\n",
    "    You are a highly intelligent AI tasked with analyzing articles to determine whether generating a timeline of events leading up to the key event in the article would be beneficial. \n",
    "    Consider the following factors to make your decision:\n",
    "    1. **Significance of the Event**:\n",
    "       - Does the event have a significant impact on a large number of people, industries, or countries?\n",
    "       - Are the potential long-term consequences of the event important?\n",
    "\n",
    "    2. **Controversy or Debate**:\n",
    "       - Is the event highly controversial or has it sparked significant debate?\n",
    "       - Has the event garnered significant media attention and public interest?\n",
    "\n",
    "    3. **Complexity**:\n",
    "       - Does the event involve multiple factors, stakeholders, or causes that make it complex?\n",
    "       - Does the event have deep historical roots or is it the culmination of long-term developments?\n",
    "\n",
    "    4. **Personal Relevance**:\n",
    "       - Does the event directly affect the reader or their community?\n",
    "       - Is the event of particular interest to the reader due to economic implications, political affiliations, or social issues?\n",
    "\n",
    "    5. Educational Purposes:\n",
    "       - Would a timeline provide valuable learning or research information?\n",
    "\n",
    "    Here is the information for the article:\n",
    "    Title:{title}\n",
    "    Text: {text}\n",
    "\n",
    "    Based on the factors above, decide whether generating a timeline of events leading up to the key event in this article would be beneficial. \n",
    "    Your answer will include the need for this article to have a timeline with a score 1 - 5, 1 means unnecessary, 5 means necessary. It will also include the main reason for your choice.\n",
    "    {format_instructions}    \n",
    "    ANSWER:\n",
    "    '''\n",
    "    \n",
    "    # See the prompt template you created for formatting\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    # Create the prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables   = [\"text\", \"title\"],\n",
    "        partial_variables = {\"format_instructions\": format_instructions},\n",
    "        template=template,\n",
    "    )\n",
    "\n",
    "    # Define the headline\n",
    "    headline = input_data[\"Title\"]\n",
    "    body     = input_data[\"Text\"]\n",
    "\n",
    "    # Format the prompt\n",
    "    final_prompt = prompt.format(title=headline, text=body)\n",
    "\n",
    "    # Generate content using the generative model\n",
    "    response = llm.generate_content(\n",
    "        final_prompt,\n",
    "        safety_settings=safety_settings,\n",
    "    )\n",
    "    final_response = clean_llm_output(response)\n",
    "\n",
    "    score = final_response['score']\n",
    "    \n",
    "     # If LLM approves\n",
    "    if score >= score_threshold:\n",
    "        logger.info(\"Timeline is appropriate for this chosen article.\\n\")\n",
    "        return {\"det\": True, \"score\": final_response['score'], \"reason\": None}\n",
    "    # end if\n",
    "    else:\n",
    "        logger.info(\"A timeline for this article is not required. \\n\")\n",
    "        for part in final_response['Reason'].replace(\". \", \".\").split(\". \"):\n",
    "            logger.info(f\"{part}\\n\")\n",
    "        # end for\n",
    "        \n",
    "        logger.info(\"Hence I gave this a required timeline score of \" + str(score))\n",
    "        reason = \"A timeline for this article is not required. \\n\" \\\n",
    "                    + \"\\n\" +final_response['Reason'] + \"\\n\"+ \"\\nHence this timeline received a necessity score of \" \\\n",
    "                    + str(final_response['score'])   + \"\\n\"\n",
    "    # end else\n",
    "        return {\"det\": False, \"score\": score, \"reason\": reason}\n",
    "# end def\n",
    "\n",
    "def extract_json_from_string(string):\n",
    "    # Use a regular expression to find the content within the first and last square brackets\n",
    "    match = re.search(r'\\[.*\\]', string, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        json_content = match.group(0)\n",
    "        try:\n",
    "            # Load the extracted content into a JSON object\n",
    "            json_data = json.loads(json_content)\n",
    "            return json_data\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"Failed to decode JSON:\", e)\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No valid JSON content found.\")\n",
    "        return None\n",
    "\n",
    "def clean_sort_timeline(timelines, df_retrieve):  \n",
    "    generated_timeline = []\n",
    "    for idx, line in timelines.items():\n",
    "        indiv_timeline = clean_output(line)\n",
    "        if type(indiv_timeline) == list:\n",
    "            for el in indiv_timeline:\n",
    "                generated_timeline.append(el)\n",
    "        else:\n",
    "            generated_timeline.append(indiv_timeline)\n",
    "    unsorted_timeline = []\n",
    "\n",
    "    for event in generated_timeline:\n",
    "        article_index = event[\"Article\"] - 1\n",
    "        event[\"Article_id\"] = df_retrieve.iloc[article_index].id\n",
    "    for event in generated_timeline:\n",
    "        del event[\"Article\"]\n",
    "        unsorted_timeline.append(event)  \n",
    "        \n",
    "    timeline = sorted(unsorted_timeline, key=lambda x:x['Date'])\n",
    "    timeline = [event for event in timeline if event['Date'].lower()!= 'nan']\n",
    "    for event in timeline:\n",
    "        date = event['Date']\n",
    "        if date.endswith('-XX-XX'):\n",
    "            event['Date'] = date[:4]\n",
    "        elif date.endswith('-XX'):\n",
    "            event['Date'] = date[:7]\n",
    "    return timeline\n",
    "\n",
    "def format_timeline_date(date_str, formats=['%Y', '%Y-%m-%d', '%Y-%m']):\n",
    "        \"\"\"Formats a date string into a human-readable format.\n",
    "        \n",
    "        Args:\n",
    "            date_str (str): The date string to be formatted.\n",
    "            formats (list): A list of date formats to try.\n",
    "        \n",
    "        Returns:\n",
    "            str: The formatted date string, or the original string if no format matches.\n",
    "        \"\"\"\n",
    "        for fmt in formats:\n",
    "            try:\n",
    "                date_obj = datetime.strptime(date_str, fmt)\n",
    "                if fmt == '%Y':\n",
    "                    return date_obj.strftime('%Y')\n",
    "                elif fmt == '%Y-%m-%d':\n",
    "                    return date_obj.strftime('%d %B %Y')\n",
    "                elif fmt == '%Y-%m':\n",
    "                    return date_obj.strftime('%B %Y')\n",
    "                # end if\n",
    "            except ValueError:\n",
    "                continue  # If the format doesn't match, try the next one\n",
    "        # end for\n",
    "        \n",
    "        # If no format matches, return the original date string\n",
    "        return date_str\n",
    "    # end def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching embeddings...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:00<00:00, 612.33it/s]\n",
      "/Users/jerryyang/pythonenv/py310/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first timeline has been generated\n",
      "\n",
      "Proceeding to Stage 1/2 of enhancement...\n",
      "\n",
      "Proceeding to Stage 2/2 of enhancement...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Timeline enhanced.. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_id = \"st_1155048\"\n",
    "df= load_database()\n",
    "test_article = df[df['st_id']==test_id].reset_index(drop=True)\n",
    "df_train = filtered_df = df[df[\"st_id\"] != test_id]\n",
    "\n",
    "\n",
    "\n",
    "timeline_necessary = det_generate_timeline(test_article)\n",
    "if timeline_necessary[\"det\"]:\n",
    "    embeddings = get_text_embeddings(df)\n",
    "    max_d = 0.58\n",
    "\n",
    "    # Pre computed hierarchical clustering\n",
    "    Z = linkage(embeddings, method='average', metric='cosine')\n",
    "    cluster_labels = fcluster(Z, max_d, criterion='distance')\n",
    "    df['Cluster_label'] = cluster_labels\n",
    "    predicted_cluster = get_predicted_cluster(df, test_id)\n",
    "    cluster_df = df[df['Cluster_label'] == predicted_cluster].reset_index(drop=True)\n",
    "    similar_articles_by_text_embedding = articles_ranked_by_text(test_article, cluster_df)\n",
    "    # Here only top 6 selected\n",
    "    best_articles = re_rank_articles(similar_articles_by_text_embedding, test_article)\n",
    "    similar_articles = get_article_dict(test_article, best_articles, df)\n",
    "\n",
    "    generated_timeline, df_retrieve = generate_and_sort_timeline(similar_articles, df_train, test_article)\n",
    "    print(\"Proceeding to Stage 1/2 of enhancement...\\n\")\n",
    "    first_enhanced_timeline = first_timeline_enhancement(generated_timeline)\n",
    "    second_enhanced_timeline = pair_article_urls(first_enhanced_timeline, df_retrieve)\n",
    "    print(\"Proceeding to Stage 2/2 of enhancement...\\n\")\n",
    "    final_timeline = second_timeline_enhancement(second_enhanced_timeline)\n",
    "    print(\"Timeline enhanced.. \\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
