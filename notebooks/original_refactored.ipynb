{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jerryyang/pythonenv/py310/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import sys\n",
    "from sentence_transformers import CrossEncoder\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import json\n",
    "import re\n",
    "from json import JSONDecodeError\n",
    "import torch\n",
    "from torch import nn\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# Import libraries for working with language models and Google Gemini\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Read configuration\n",
    "config_file = '../config.yaml'\n",
    "with open(config_file, 'r') as fin:\n",
    "    config = yaml.safe_load(fin)\n",
    "# end with\n",
    "\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "chat_model = \"llama3-8b-8192\"\n",
    "\n",
    "GEMINI_KEY   = os.environ['GEMINI_KEY']\n",
    "MONGO_URI    = os.environ['MONGO_URI']\n",
    "HF_KEY       = os.environ['HUGGINGFACE_API_KEY']\n",
    "EMBEDDER_API = os.environ[\"HF_EMBEDDING_MODEL_URL\"]\n",
    "\n",
    "genai.configure(api_key=GEMINI_KEY)\n",
    "\n",
    "# Initialise Mongodb client\n",
    "mongo_client = MongoClient(MONGO_URI)\n",
    "\n",
    "# Setup default LLM model\n",
    "default_llm = genai.GenerativeModel('gemini-1.5-flash-latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_database():\n",
    "    # Connect to the MongoDB client\n",
    "    try:\n",
    "        db = mongo_client[config[\"database\"][\"name\"]]\n",
    "        train_documents = db[config[\"database\"][\"train_collection\"]].find()\n",
    "        logger.info(\"Train data successfully fetched from MongoDB\\n\")\n",
    "    except Exception as error: \n",
    "        logger.error(f\"Unable to fetch train data from MongoDB. Check your connection the database...\\nERROR: {error}\\n\")\n",
    "    \n",
    "    try:\n",
    "        test_docs = db[config[\"database\"][\"test_collection\"]].find()\n",
    "        logger.info(\"Test data successfully fetched from MongoDB\\n\")\n",
    "    except:\n",
    "        logger.error(f\"Unable to fetch test data from MongoDB. Check your connection the database...\\nERROR: {error}\\n\")\n",
    "    \n",
    "    df_train = pd.DataFrame.from_dict(list(train_documents))\n",
    "    df_test = pd.DataFrame.from_dict(list(test_docs))\n",
    "    \n",
    "    # Row bind the training and test dataframes \n",
    "    df = pd.concat([df_train, df_test], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "# end def\n",
    "\n",
    "def get_text_embeddings(df):\n",
    "    print(\"Fetching embeddings...\\n\")\n",
    "    #Deserializing the embeddings\n",
    "    body_embeddings = np.array(df['embeddings'].apply(ast.literal_eval).tolist())\n",
    "    return body_embeddings\n",
    "# end def\n",
    "\n",
    "def get_predicted_cluster(dataframe, test_id):\n",
    "    test_article = dataframe[dataframe['st_id'] == test_id].reset_index(drop=True)\n",
    "    predicted_cluster = test_article['Cluster_label'][0]\n",
    "    return predicted_cluster\n",
    "# end def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def clean_output(output):\n",
    "    try:\n",
    "        updated_timeline = json.loads(output)\n",
    "        return updated_timeline\n",
    "    except JSONDecodeError:\n",
    "        #try 1: Ensuring that the string ends with just the open and close lists brackets\n",
    "        try:\n",
    "            new_output = re.search(r'\\[[^\\]]*\\]', output).group(0)\n",
    "        except AttributeError:\n",
    "            new_output = re.search(r'\\{.*?\\}', output, re.DOTALL).group(0)  \n",
    "        updated_timeline = json.loads(new_output)\n",
    "        return updated_timeline\n",
    "\n",
    "def clean_sort_timeline(timelines, df_retrieve):  \n",
    "    # Expands the list of timelines\n",
    "    generated_timeline = []\n",
    "    for _, line in timelines.items():\n",
    "        # Sieve out the timeline\n",
    "        indiv_timeline = clean_output(line)\n",
    "        if type(indiv_timeline) == list:\n",
    "            # append each individual event in this timeline from this article\n",
    "            for el in indiv_timeline:\n",
    "                generated_timeline.append(el)\n",
    "        else:\n",
    "            generated_timeline.append(indiv_timeline)\n",
    "        # end if\n",
    "    # end for\n",
    "    \n",
    "    # Code to derive the article id from the article number \n",
    "    unsorted_timeline = []\n",
    "    for event in generated_timeline:\n",
    "        article_index = event[\"Article\"] - 1\n",
    "        event[\"Article_id\"] = df_retrieve.iloc[article_index].id\n",
    "        del event[\"Article\"]\n",
    "        unsorted_timeline.append(event) \n",
    "    # end for \n",
    "    \n",
    "    # section to remove the parts of the date that LLM were unclear of     \n",
    "    timeline = sorted(unsorted_timeline, key=lambda x:x['Date'])\n",
    "    timeline = [event for event in timeline if event['Date'].lower()!= 'nan']\n",
    "    \n",
    "    for event in timeline:\n",
    "        date = event['Date']\n",
    "        if date.endswith('-XX-XX'):\n",
    "            event['Date'] = date[:4]\n",
    "        elif date.endswith('-XX'):\n",
    "            event['Date'] = date[:7]\n",
    "        # end if\n",
    "    # end for\n",
    "    return timeline\n",
    "# end def\n",
    "\n",
    "def format_timeline_date(date_str, formats=['%Y', '%Y-%m-%d', '%Y-%m']):\n",
    "        \"\"\"Formats a date string into a human-readable format.\n",
    "        \n",
    "        Args:\n",
    "            date_str (str): The date string to be formatted.\n",
    "            formats (list): A list of date formats to try.\n",
    "        \n",
    "        Returns:\n",
    "            str: The formatted date string, or the original string if no format matches.\n",
    "        \"\"\"\n",
    "        for fmt in formats:\n",
    "            try:\n",
    "                date_obj = datetime.strptime(date_str, fmt)\n",
    "                if fmt == '%Y':\n",
    "                    return date_obj.strftime('%Y')\n",
    "                elif fmt == '%Y-%m-%d':\n",
    "                    return date_obj.strftime('%d %B %Y')\n",
    "                elif fmt == '%Y-%m':\n",
    "                    return date_obj.strftime('%B %Y')\n",
    "                # end if\n",
    "            except ValueError:\n",
    "                continue  # If the format doesn't match, try the next one\n",
    "        # end for\n",
    "        \n",
    "        # If no format matches, return the original date string\n",
    "        return date_str\n",
    "# end def\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the cosine similarity between texts of 2 articles\n",
    "def get_text_similarity(timeline_embedding,train_article):\n",
    "    \n",
    "    cos_sim = nn.CosineSimilarity(dim=0)\n",
    "    \n",
    "    text_embedding = torch.tensor(eval(train_article['embeddings']))\n",
    "    timeline_embedding = torch.tensor(timeline_embedding)\n",
    "    \n",
    "    similarity_score = cos_sim(timeline_embedding, text_embedding)\n",
    "    return similarity_score\n",
    "# end def\n",
    "\n",
    "# Generate the cosine similarity between texts of 2 articles\n",
    "def get_text_similarity(test_embedding, db_embedding):\n",
    "    \n",
    "    cos_sim = nn.CosineSimilarity(dim=0)\n",
    "    similarity_score = cos_sim(torch.tensor(test_embedding), torch.tensor(db_embedding))\n",
    "    return similarity_score\n",
    "# end def\n",
    "\n",
    "# Find the top 20 most similar articles based on their text embedding\n",
    "def articles_ranked_by_text(test_article, database):\n",
    "    logger.info(\"Computing similarities between article texts...\")\n",
    "    test_embedding = test_article['embeddings'].apply(ast.literal_eval)[0]  \n",
    "\n",
    "    article_collection = []\n",
    "    for i in trange(len(database)):\n",
    "        article = {}\n",
    "        article['id'] = database.iloc[i]['st_id']\n",
    "        article['Title'] = database.iloc[i]['Title']\n",
    "        article['Text'] = database.iloc[i]['Text']\n",
    "        article['Date'] = database.iloc[i]['Publication_date']\n",
    "        article['Article_URL'] = database.iloc[i]['article_url']\n",
    "        article_embedding =  pd.DataFrame(database.iloc[i]).loc['embeddings'].apply(ast.literal_eval)[i]\n",
    "        article['cosine_score'] = get_text_similarity(test_embedding, article_embedding)\n",
    "        article_collection.append(article)\n",
    "    # end for\n",
    "    \n",
    "    # Sort by cosine similarity in descending order\n",
    "    article_collection.sort(key = lambda x: x['cosine_score'], reverse=True)\n",
    "    \n",
    "    # Returns the top 20 most similar articles\n",
    "    collection = article_collection[:21]\n",
    "    \n",
    "    # remove the first article as it is the test article\n",
    "    collection.pop(0)\n",
    "    return collection\n",
    "# end def\n",
    "\n",
    "# use cross encoder to filter out the top 7 articles\n",
    "# Re rank articles based on similarity derived from cross encoder\n",
    "def re_rank_articles(unique_articles, test_article, cross_encoder_model=\"cross-encoder/ms-marco-TinyBERT-L-2-v2\", top_k=6):\n",
    " \n",
    "    cross_encoder = CrossEncoder(\n",
    "        cross_encoder_model, max_length=512, device=\"cpu\"\n",
    "    )\n",
    "    test_article_text = test_article['Text'][0]\n",
    "    # Format the timeline header and article for cross encoder\n",
    "    unranked_articles = [(test_article_text, doc['Text']) for doc in unique_articles]\n",
    "    \n",
    "    # Predict similarity_scores\n",
    "    similarity_scores = cross_encoder.predict(unranked_articles).tolist()\n",
    "    # Assign the list of similarity scores to the inidividual articles \n",
    "    for i in range(len(unique_articles)):\n",
    "        # if similarity_scores[i]:\n",
    "        unique_articles[i]['reranked_score'] = similarity_scores[i]\n",
    "        # end if\n",
    "    # end for\n",
    "    combined_articles = [article for article in unique_articles if 'reranked_score' in article]\n",
    "    best_articles = sorted(combined_articles, key=lambda x: x['reranked_score'], reverse=True)[:top_k]\n",
    "\n",
    "    return best_articles\n",
    "\n",
    "# Generates the clustering properties of the test article\n",
    "def get_article_dict(test_article, best_articles, df):\n",
    "    # Get the df ids of the best articles \n",
    "    ids = [article['id'] for article in best_articles]\n",
    "    \n",
    "    similar_indexes = []\n",
    "    for idx, row in df.iterrows():\n",
    "        if row['st_id'] in ids:\n",
    "            similar_indexes.append(idx)\n",
    "        # end if\n",
    "        if len(similar_indexes)==len(ids):\n",
    "            break\n",
    "        # end if\n",
    "    # end for\n",
    "\n",
    "    similar_articles_dict = {\n",
    "                    'Title': test_article['Title'][0],\n",
    "                    'indexes': similar_indexes,\n",
    "                    'Text': test_article['Text'][0],\n",
    "                }\n",
    "    if len(similar_articles_dict) < 2:\n",
    "        print(\"There are insufficient relevant articles to construct a meaningful timeline. ... Exiting execution now\\n\")\n",
    "        return \"generate_similar_error\"\n",
    "    # end if\n",
    "    \n",
    "    return similar_articles_dict\n",
    "# end def\n",
    "\n",
    "# version 1 timeline generation\n",
    "def det_generate_timeline(input_data,score_threshold=3,llm=default_llm,\n",
    "    safety_settings = {\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "    }):\n",
    "    \n",
    "    \"\"\"Evaluating necessity of Timeline for this article.\"\"\"\n",
    "\n",
    "    def clean_llm_output(llm_output):\n",
    "        text = llm_output.parts[0].text.replace(\"```\", '').replace('json','')\n",
    "        result = json.loads(text)\n",
    "        return result\n",
    "    \n",
    "    # Initialise Pydantic object to force LLM return format\n",
    "    class Event(BaseModel):\n",
    "        score: int = Field(description=\"The need for this article to have a timeline\")\n",
    "        Reason: str = Field(description = \"The main reason for your choice why a timeline is needed or why it is not needed\")\n",
    "    \n",
    "    # Initialise Json output parser\n",
    "    output_parser = JsonOutputParser(pydantic_object=Event)\n",
    "\n",
    "    # Define the template\n",
    "    template = \\\n",
    "    '''\n",
    "    You are a highly intelligent AI tasked with analyzing articles to determine whether generating a timeline of events leading up to the key event in the article would be beneficial. \n",
    "    Consider the following factors to make your decision:\n",
    "    1. **Significance of the Event**:\n",
    "       - Does the event have a significant impact on a large number of people, industries, or countries?\n",
    "       - Are the potential long-term consequences of the event important?\n",
    "\n",
    "    2. **Controversy or Debate**:\n",
    "       - Is the event highly controversial or has it sparked significant debate?\n",
    "       - Has the event garnered significant media attention and public interest?\n",
    "\n",
    "    3. **Complexity**:\n",
    "       - Does the event involve multiple factors, stakeholders, or causes that make it complex?\n",
    "       - Does the event have deep historical roots or is it the culmination of long-term developments?\n",
    "\n",
    "    4. **Personal Relevance**:\n",
    "       - Does the event directly affect the reader or their community?\n",
    "       - Is the event of particular interest to the reader due to economic implications, political affiliations, or social issues?\n",
    "\n",
    "    5. Educational Purposes:\n",
    "       - Would a timeline provide valuable learning or research information?\n",
    "\n",
    "    Here is the information for the article:\n",
    "    Title:{title}\n",
    "    Text: {text}\n",
    "\n",
    "    Based on the factors above, decide whether generating a timeline of events leading up to the key event in this article would be beneficial. \n",
    "    Your answer will include the need for this article to have a timeline with a score 1 - 5, 1 means unnecessary, 5 means necessary. It will also include the main reason for your choice.\n",
    "    {format_instructions}    \n",
    "    ANSWER:\n",
    "    '''\n",
    "    \n",
    "    # See the prompt template you created for formatting\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    # Create the prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables   = [\"text\", \"title\"],\n",
    "        partial_variables = {\"format_instructions\": format_instructions},\n",
    "        template=template,\n",
    "    )\n",
    "\n",
    "    # Define the headline\n",
    "    headline = input_data[\"Title\"]\n",
    "    body     = input_data[\"Text\"]\n",
    "\n",
    "    # Format the prompt\n",
    "    final_prompt = prompt.format(title=headline, text=body)\n",
    "\n",
    "    # Generate content using the generative model\n",
    "    response = llm.generate_content(\n",
    "        final_prompt,\n",
    "        safety_settings=safety_settings,\n",
    "    )\n",
    "    final_response = clean_llm_output(response)\n",
    "\n",
    "    score = final_response['score']\n",
    "    \n",
    "     # If LLM approves\n",
    "    if score >= score_threshold:\n",
    "        logger.info(\"Timeline is appropriate for this chosen article.\\n\")\n",
    "        return {\"det\": True, \"score\": final_response['score'], \"reason\": None}\n",
    "    # end if\n",
    "    else:\n",
    "        logger.info(\"A timeline for this article is not required. \\n\")\n",
    "        for part in final_response['Reason'].replace(\". \", \".\").split(\". \"):\n",
    "            logger.info(f\"{part}\\n\")\n",
    "        # end for\n",
    "        \n",
    "        logger.info(\"Hence I gave this a required timeline score of \" + str(score))\n",
    "        reason = \"A timeline for this article is not required. \\n\" \\\n",
    "                    + \"\\n\" +final_response['Reason'] + \"\\n\"+ \"\\nHence this timeline received a necessity score of \" \\\n",
    "                    + str(final_response['score'])   + \"\\n\"\n",
    "    # end else\n",
    "        return {\"det\": False, \"score\": score, \"reason\": reason}\n",
    "# end def\n",
    "\n",
    "def generate_and_sort_timeline(similar_articles_dict, df_train, df_test, llm=default_llm):    \n",
    "    \n",
    "    class Event(BaseModel):\n",
    "        Date: str = Field(description=\"The date of the event in YYYY-MM-DD format\")\n",
    "        Event: str = Field(description=\"A detailed description of the important event\")\n",
    "        Article: int = Field(description=\"The article number from which the event was extracted\")\n",
    "\n",
    "    output_parser = JsonOutputParser(pydantic_object=Event)\n",
    "\n",
    "    # See the prompt template you created for formatting\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    template = '''\n",
    "    Given an article, containing a publication date, title, and content, your task is to construct a detailed timeline of events leading up to the main event described in the article.\n",
    "    Begin by thoroughly analyzing the title, content, and publication date of the article to understand the main event in the article. \n",
    "    the dates are represented in YYYY-MM-DD format. Identify events, context, and any time references such as \"last week,\" \"last month,\" or specific dates. \n",
    "    The article could contain more or one key events. \n",
    "    If the article does not provide a publication date or any events leading up to the main event, return NAN in the Date field, and 0 i the Article Field\n",
    "\n",
    "    Construct the Timeline:\n",
    "    Chronological Order: Organize the events chronologically, using the publication dates and time references within the articles.\n",
    "    Detailed Descriptions: Provide detailed descriptions of each event, explaining how it relates to the main event of the first article.\n",
    "    Contextual Links: Use information from the articles to link events together logically and coherently.\n",
    "    Handle Ambiguities: If an article uses ambiguous time references, infer the date based on the publication date of the article and provide a clear rationale for your inference.\n",
    "\n",
    "    Contextual Links:\n",
    "    External Influences: Mention any external influences (e.g., global conflicts, economic trends, scientific discoveries) that might have indirectly affected the events.\n",
    "    Internal Issues: Highlight any internal issues or developments (e.g., political changes, organizational restructuring, societal movements) within the entities involved that might have impacted the events.\n",
    "    Efforts for Improvement: Note any indications of efforts to improve the situation (e.g., policy changes, strategic initiatives, collaborative projects) despite existing challenges.\n",
    "\n",
    "    Be as thorough and precise as possible, ensuring the timeline accurately reflects the sequence and context of events leading to the main event.\n",
    "\n",
    "    Article:\n",
    "    {text}\n",
    "\n",
    "    {format_instructions}\n",
    "    Check and ensure again that the output follows the format instructions above very strictly. \n",
    "    '''\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        partial_variables={\"format_instructions\": format_instructions},\n",
    "        template=template\n",
    "    )\n",
    "    \n",
    "    def generate_individual_timeline(date_text_triples):\n",
    "        article_details=  f'Article {date_text_triples[0]}: Publication date: {date_text_triples[1]} Article Text: {date_text_triples[2]}'\n",
    "        final_prompt = prompt.format(text=article_details)\n",
    "        response = llm.generate_content(final_prompt,\n",
    "                                        safety_settings={\n",
    "                                            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, \n",
    "                                            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "                                            })\n",
    "        \n",
    "        # Check if Model returns correct format \n",
    "        if '[' in response.parts[0].text or '{' in response.parts[0].text:\n",
    "            result = response.parts[0].text\n",
    "        else:\n",
    "            retry_response = llm.generate_content(final_prompt,\n",
    "                                        safety_settings={\n",
    "                                            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, \n",
    "                                            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "                                            })\n",
    "            try:\n",
    "                result = retry_response.parts[0].text\n",
    "            except ValueError:\n",
    "                print(\"ERROR: There were issues with the generation of the timeline. The timeline could not be generated\")\n",
    "                return\n",
    "        # end if\n",
    "        return result\n",
    "    # end def\n",
    "    \n",
    "    def process_articles(df_train, similar_articles_dict, df_test):\n",
    "        # Retrieve and concatenate dataframes\n",
    "        df_retrieve = pd.concat([df_train.loc[similar_articles_dict['indexes']], df_test], axis=0).iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "        # Prepare metadata for LLM\n",
    "        date_text_triples = list(zip(range(1, len(df_retrieve) + 1), df_retrieve['combined'].tolist(), df_retrieve['Publication_date'].tolist()))\n",
    "\n",
    "        # Generate timelines using ThreadPoolExecutor\n",
    "        dict_of_timelines = {}\n",
    "        try:\n",
    "            with ThreadPoolExecutor(max_workers=len(date_text_triples)) as executor:\n",
    "                futures = {executor.submit(generate_individual_timeline, triple): i for i, triple in enumerate(date_text_triples)}\n",
    "                for future in as_completed(futures):\n",
    "                    dict_of_timelines[futures[future]] = future.result()\n",
    "        except IndexError:\n",
    "            with ThreadPoolExecutor(max_workers=len(date_text_triples)) as executor:\n",
    "                futures = {executor.submit(generate_individual_timeline, triple): i for i, triple in enumerate(date_text_triples)}\n",
    "                for future in as_completed(futures):\n",
    "                    dict_of_timelines[futures[future]] = future.result()\n",
    "\n",
    "        return dict_of_timelines, df_retrieve\n",
    "    # end def\n",
    "    \n",
    "    timeline_dic, df_retrieve = process_articles(df_train, similar_articles_dict, df_test)\n",
    "    print(\"The first timeline has been generated\\n\")\n",
    "    generated_timeline = []\n",
    "    for _, line in timeline_dic.items():\n",
    "        indiv_timeline = clean_output(line)\n",
    "        if type(indiv_timeline) == list:\n",
    "            for el in indiv_timeline:\n",
    "                generated_timeline.append(el)\n",
    "        else:\n",
    "            generated_timeline.append(indiv_timeline)\n",
    "    \n",
    "    unsorted_timeline = []\n",
    "    for event in generated_timeline:\n",
    "        article_index = event[\"Article\"] - 1\n",
    "        event[\"Article_id\"] = df_retrieve.iloc[article_index].st_id\n",
    "    for event in generated_timeline:\n",
    "        del event[\"Article\"]\n",
    "        unsorted_timeline.append(event)  \n",
    "        \n",
    "    timeline = sorted(unsorted_timeline, key=lambda x:x['Date'])\n",
    "    finished_timeline = [event for event in timeline if event['Date'].lower()!= 'nan']\n",
    "    for i in range(len(finished_timeline)):\n",
    "        date = finished_timeline[i]['Date']\n",
    "        if date.endswith('-XX-XX') or date.endswith('00-00'):\n",
    "            finished_timeline[i]['Date'] = date[:4]\n",
    "        elif date.endswith('-XX') or date.endswith('00'):\n",
    "            finished_timeline[i]['Date'] = date[:7]\n",
    "    return finished_timeline, df_retrieve\n",
    "    # end def\n",
    "# end def\n",
    "\n",
    "def reduce_by_date(timeline_list, llm=default_llm):\n",
    "    '''\n",
    "    Takes in a list of events in one day, and returns a list of dicts for events in that day\n",
    "    '''\n",
    "    def extract_content_from_json(string):\n",
    "        # Use a regular expression to find the content within the first and last square brackets\n",
    "        match = re.search(r'\\[.*\\]', string, re.DOTALL)\n",
    "        \n",
    "        if match:\n",
    "            json_content = match.group(0)\n",
    "            try:\n",
    "                # Load the extracted content into a JSON object\n",
    "                json_data = json.loads(json_content)\n",
    "                return json_data\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(\"Failed to decode JSON:\", e)\n",
    "                return None\n",
    "        else:\n",
    "            print(\"No valid JSON content found.\")\n",
    "            return None\n",
    "        # end if\n",
    "    # end def\n",
    "    \n",
    "    timeline_string = json.dumps(timeline_list)\n",
    "    \n",
    "    class Event(BaseModel):\n",
    "            Event: str = Field(description=\"A detailed description of the event\")\n",
    "            Article_id: list = Field(description=\"The article id(s) from which the events were extracted\")\n",
    "\n",
    "    parser = JsonOutputParser(pydantic_object=Event)\n",
    "\n",
    "    template = '''You are a news article editor tasked with simplifying a section of a timeline of events on the same day. \n",
    "Given this snippet a timeline, filter out duplicated events. \n",
    "IF events convey the same overall meaning, I want you to merge these events into one event to avoid redundancy, and add the article ids to a list. \n",
    "However, the events are all different, do not combine them, I want you to return it as it is, however, follow the specified format instructions below. \n",
    "Furthermore, if the event is not considered to be an event worthy of an audience reading the timeline, do not include it.\n",
    "Take your time and evaluate the timeline slowly to make your decision.\n",
    "\n",
    "Timeline snippet:\n",
    "{text}\n",
    "\n",
    "{format_instructions}\n",
    "Ensure that the format follows the example output format strictly before returning the output.'''\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=template,\n",
    "            partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "        )\n",
    "    \n",
    "    final_prompt = prompt.format(text=timeline_string)\n",
    "    response = llm.generate_content(final_prompt,\n",
    "            safety_settings={\n",
    "                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, \n",
    "                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    data = extract_content_from_json(response.parts[0].text)\n",
    "    \n",
    "    # limit the number of article links to 2 links\n",
    "    link_limit =  2\n",
    "    \n",
    "    for event_dic in data:\n",
    "        article_id_list = event_dic['Article_id']\n",
    "        if len(article_id_list)> link_limit:\n",
    "            shortened_ids = article_id_list\n",
    "            event_dic['Article_id'] = shortened_ids\n",
    "        # end if\n",
    "    # end for\n",
    "    return data\n",
    "# end def\n",
    "\n",
    "# Creating a set of unique days. This will reduce the number of tokens to the model, and makes it easier to handle in the output\n",
    "def first_timeline_enhancement(timeline):\n",
    "    '''\n",
    "    This function takes in a timeline in list format \n",
    "    '''\n",
    "    \n",
    "    # Get the unique dates seen in the timeline\n",
    "    unique_dates = sorted(list(set([event['Date'] for event in timeline])))\n",
    "    \n",
    "    # Combine the events that have the same date\n",
    "    dic = {}\n",
    "    for i in range(len(unique_dates)):\n",
    "        dic[unique_dates[i]] = [{'Event':event['Event'], 'Article_id': event['Article_id']} for event in timeline if event['Date'] == unique_dates[i]]\n",
    "    # end for\n",
    "    \n",
    "    # Combine the events that have the same date and (only if they have the same event)\n",
    "    new_timeline = {}\n",
    "    for date, snippet in dic.items():\n",
    "        if len(snippet) == 1:\n",
    "            new_timeline[date] = snippet\n",
    "        else:\n",
    "            new_snippet = reduce_by_date(snippet)\n",
    "            new_timeline[date] =new_snippet\n",
    "        # end if\n",
    "    # end for\n",
    "    \n",
    "    enhanced_timeline = []\n",
    "    for date, events in new_timeline.items():\n",
    "            for event in events:\n",
    "                new_event = {}\n",
    "                new_event['Date'] = date\n",
    "                new_event['Event'] = event['Event']\n",
    "                article_id = event['Article_id']\n",
    "                if isinstance(article_id, str):\n",
    "                    new_event['Article_id'] = [article_id]\n",
    "                else:\n",
    "                    new_event['Article_id'] = event['Article_id']\n",
    "                # end if\n",
    "                enhanced_timeline.append(new_event)\n",
    "            # end for\n",
    "    # end for\n",
    "    return enhanced_timeline\n",
    "# end def\n",
    "\n",
    "def pair_article_urls(enhanced_timeline, df_retrieve):\n",
    "    \n",
    "    def edit_timeline(timeline):\n",
    "        for event in timeline:\n",
    "            new_date = format_timeline_date(event['Date'])\n",
    "            event['Date'] = new_date\n",
    "        # end for\n",
    "        return timeline\n",
    "    # end def\n",
    "\n",
    "    edited_timeline = edit_timeline(enhanced_timeline)\n",
    "\n",
    "    # Get out the article id and URL into suitable data structure for being displayed on the webpage\n",
    "    for event in edited_timeline:\n",
    "        id_list = event['Article_id']\n",
    "        url_title_pairs = []\n",
    "        \n",
    "        for i in range(len(id_list)):\n",
    "            id = id_list[i]\n",
    "            url = df_retrieve[df_retrieve['st_id'] == id]['article_url'].values[0]\n",
    "            title = df_retrieve[df_retrieve['st_id'] == id]['Title'].values[0]\n",
    "            url_title_pairs.append({'url': url, 'title': title})\n",
    "        # end for\n",
    "        \n",
    "        event['Article_URL'] = url_title_pairs\n",
    "        event.pop('Article_id')  \n",
    "    # end for\n",
    "    return edited_timeline\n",
    "# end def\n",
    "\n",
    "# Function to get out events that need to be summarised\n",
    "def get_needed_summaries(timeline):\n",
    "    def get_num_words(event_str):\n",
    "        ls = event_str.split()\n",
    "        return len(ls)\n",
    "    # end def\n",
    "\n",
    "    need_summary_timeline = []\n",
    "    for i in range(len(timeline)):\n",
    "        # If the number of words in the event is more than 20\n",
    "        if get_num_words(timeline[i]['Event']) > 20:\n",
    "            need_summary_timeline.append((i,timeline[i]))\n",
    "        # end if\n",
    "    # end for\n",
    "    \n",
    "    # Get out events\n",
    "    events = {}\n",
    "    for i in range(len(need_summary_timeline)):\n",
    "        events[need_summary_timeline[i][0]] = need_summary_timeline[i][1]\n",
    "    # end for\n",
    "    return events\n",
    "# end def\n",
    "\n",
    "# Function to summarise the events \n",
    "def groq_summariser(events_ls):\n",
    "    \n",
    "    # Define summarised event output\n",
    "    class summarized_event(BaseModel):\n",
    "        Event: str = Field(description=\"Event in a timeline\")\n",
    "        Event_Summary: str = Field(description=\"Short Summary of event\")\n",
    "    \n",
    "    parser = JsonOutputParser(pydantic_object=summarized_event)\n",
    "\n",
    "    chat = ChatGroq(temperature=0, model_name=chat_model)\n",
    "    \n",
    "    template = '''\n",
    "You are a news article editor.\n",
    "Given a list of events from a timeline, you are tasked to provide a short summary of these series of events. \n",
    "For each event, you should return the event, and the summary.\n",
    "\n",
    "Series of events:\n",
    "{text}\n",
    "\n",
    "{format_instructions}\n",
    "    '''\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"text\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "    \n",
    "    chain = prompt | chat | parser\n",
    "    \n",
    "    event_str = json.dumps(events_ls)\n",
    "    result = chain.invoke({\"text\": event_str})\n",
    "    if isinstance(result, list):\n",
    "        return result\n",
    "    else:\n",
    "        cleaned_result = clean_output(result)\n",
    "    # end if\n",
    "    return cleaned_result\n",
    "# end def\n",
    "\n",
    "# function to combine the events that were just summarised\n",
    "def merge_event_summaries(events_ls, llm_answer, timeline, need_summary_timeline):\n",
    "    \n",
    "    if len(llm_answer) != len(need_summary_timeline):\n",
    "        print(\"Groq had an error where timeline summary output length not equal to input but trying to resolve\")\n",
    "        llm_answer = groq_summariser(events_ls)\n",
    "        print(len(llm_answer) == len(events_ls))\n",
    "    # end if\n",
    "    \n",
    "    i = 0\n",
    "    for k,v in need_summary_timeline.items():\n",
    "        need_summary_timeline[k]['Event_Summary'] = llm_answer[i]['Event_Summary']\n",
    "        i += 1\n",
    "    # end for\n",
    "        \n",
    "    for i in range(len(timeline)):\n",
    "            if i in need_summary_timeline:\n",
    "                timeline[i] = need_summary_timeline[i]\n",
    "            # end if\n",
    "    # end for\n",
    "    return timeline\n",
    "# end def\n",
    "\n",
    "# Combining the functions for the second enhancement (event summary)\n",
    "def second_timeline_enhancement(timeline):\n",
    "    need_summary_timeline = get_needed_summaries(timeline)\n",
    "    events_ls = [event['Event'] for _, event in need_summary_timeline.items()]\n",
    "    summaries = groq_summariser(events_ls)\n",
    "    final_timeline = merge_event_summaries(events_ls, summaries, timeline, need_summary_timeline)\n",
    "    return final_timeline\n",
    "# end def\n",
    "\n",
    "# Function to generate and save the timeline\n",
    "def generate_save_timeline(relevant_articles, df_train, df_test):\n",
    "    similar_articles = get_article_dict(relevant_articles, df_train, df_test)\n",
    "    if similar_articles == \"generate_similar_error\":\n",
    "        return \"Error02\"\n",
    "    generated_timeline, df_retrieve = generate_and_sort_timeline(similar_articles, df_train, df_test)\n",
    "    print(\"Proceeding to Stage 1/2 of enhancement...\\n\")\n",
    "    first_enhanced_timeline = first_timeline_enhancement(generated_timeline)\n",
    "    second_enhanced_timeline = pair_article_urls(first_enhanced_timeline, df_retrieve)\n",
    "    print(\"Proceeding to Stage 2/2 of enhancement...\\n\")\n",
    "    final_timeline = second_timeline_enhancement(second_enhanced_timeline)\n",
    "    print(\"Timeline enhanced.. \\n\")\n",
    "    return final_timeline\n",
    "# end def\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching embeddings...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:00<00:00, 407.43it/s]\n",
      "/Users/jerryyang/pythonenv/py310/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first timeline has been generated\n",
      "\n",
      "Proceeding to Stage 1/2 of enhancement...\n",
      "\n",
      "Proceeding to Stage 2/2 of enhancement...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeline enhanced.. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_id = \"st_1155048\"\n",
    "df= load_database()\n",
    "test_article = df[df['st_id']==test_id].reset_index(drop=True)\n",
    "df_train = filtered_df = df[df[\"st_id\"] != test_id]\n",
    "\n",
    "timeline_necessary = det_generate_timeline(test_article)\n",
    "if timeline_necessary[\"det\"]:\n",
    "    embeddings = get_text_embeddings(df)\n",
    "    max_d = 0.58\n",
    "\n",
    "    # Pre computed hierarchical clustering\n",
    "    Z = linkage(embeddings, method='average', metric='cosine')\n",
    "    cluster_labels = fcluster(Z, max_d, criterion='distance')\n",
    "    df['Cluster_label'] = cluster_labels\n",
    "    predicted_cluster = get_predicted_cluster(df, test_id)\n",
    "    cluster_df = df[df['Cluster_label'] == predicted_cluster].reset_index(drop=True)\n",
    "    similar_articles_by_text_embedding = articles_ranked_by_text(test_article, cluster_df)\n",
    "    # Here only top 6 selected\n",
    "    best_articles = re_rank_articles(similar_articles_by_text_embedding, test_article)\n",
    "    similar_articles = get_article_dict(test_article, best_articles, df)\n",
    "\n",
    "    generated_timeline, df_retrieve = generate_and_sort_timeline(similar_articles, df_train, test_article)\n",
    "    print(\"Proceeding to Stage 1/2 of enhancement...\\n\")\n",
    "    first_enhanced_timeline = first_timeline_enhancement(generated_timeline)\n",
    "    second_enhanced_timeline = pair_article_urls(first_enhanced_timeline, df_retrieve)\n",
    "    print(\"Proceeding to Stage 2/2 of enhancement...\\n\")\n",
    "    final_timeline = second_timeline_enhancement(second_enhanced_timeline)\n",
    "    print(\"Timeline enhanced.. \\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
