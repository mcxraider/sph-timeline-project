{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import re\n",
    "from json import JSONDecodeError\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# Import libraries for working with language models and Google Gemini\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "GEMINI_KEY = os.environ.get('GEMINI_KEY')\n",
    "genai.configure(api_key=GEMINI_KEY)\n",
    "\n",
    "# Normally where to do this? (in which function?)\n",
    "with open(\"../gradio_config.yaml\", \"r\") as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "mongo_client = MongoClient(config[\"database\"][\"uri\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_llm_score(output):\n",
    "    text = output.parts[0].text.replace(\"```\", '').replace('json','')\n",
    "    result = json.loads(text)\n",
    "    return result\n",
    "\n",
    "def clean_output(output):\n",
    "    try:\n",
    "        updated_timeline = json.loads(output)\n",
    "        return updated_timeline\n",
    "    except JSONDecodeError:\n",
    "        #try 1: Ensuring that the string ends with just the open and close lists brackets\n",
    "        try:\n",
    "            new_output = re.search(r'\\[[^\\]]*\\]', output).group(0)\n",
    "        except AttributeError:\n",
    "            new_output = re.search(r'\\{.*?\\}', output, re.DOTALL).group(0)  \n",
    "        updated_timeline = json.loads(new_output)\n",
    "        return updated_timeline\n",
    "    \n",
    "def extract_json_from_string(string):\n",
    "    # Use a regular expression to find the content within the first and last square brackets\n",
    "    match = re.search(r'\\[.*\\]', string, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        json_content = match.group(0)\n",
    "        try:\n",
    "            # Load the extracted content into a JSON object\n",
    "            json_data = json.loads(json_content)\n",
    "            return json_data\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"Failed to decode JSON:\", e)\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No valid JSON content found.\")\n",
    "        return None\n",
    "\n",
    "def clean_sort_timeline(timelines, df_retrieve):  \n",
    "    generated_timeline = []\n",
    "    for idx, line in timelines.items():\n",
    "        indiv_timeline = clean_output(line)\n",
    "        if type(indiv_timeline) == list:\n",
    "            for el in indiv_timeline:\n",
    "                generated_timeline.append(el)\n",
    "        else:\n",
    "            generated_timeline.append(indiv_timeline)\n",
    "    unsorted_timeline = []\n",
    "\n",
    "    for event in generated_timeline:\n",
    "        article_index = event[\"Article\"] - 1\n",
    "        event[\"Article_id\"] = df_retrieve.iloc[article_index].id\n",
    "    for event in generated_timeline:\n",
    "        del event[\"Article\"]\n",
    "        unsorted_timeline.append(event)  \n",
    "        \n",
    "    timeline = sorted(unsorted_timeline, key=lambda x:x['Date'])\n",
    "    timeline = [event for event in timeline if event['Date'].lower()!= 'nan']\n",
    "    for event in timeline:\n",
    "        date = event['Date']\n",
    "        if date.endswith('-XX-XX'):\n",
    "            event['Date'] = date[:4]\n",
    "        elif date.endswith('-XX'):\n",
    "            event['Date'] = date[:7]\n",
    "    return timeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_timeline_date(date_str):\n",
    "    formats = ['%Y', '%Y-%m-%d', '%Y-%m']\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str, fmt)\n",
    "            if fmt == '%Y':\n",
    "                return date_obj.strftime('%Y')\n",
    "            elif fmt == '%Y-%m-%d':\n",
    "                return date_obj.strftime('%d %B %Y')\n",
    "            elif fmt == '%Y-%m':\n",
    "                return date_obj.strftime('%B %Y')\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def split_batches(timeline, max_batch_size=30):\n",
    "    n = len(timeline)\n",
    "    if n <= max_batch_size:\n",
    "        return [timeline]\n",
    "    \n",
    "    num_batches = n // max_batch_size\n",
    "    remainder = n % max_batch_size\n",
    "    \n",
    "    if remainder > 0 and remainder < max_batch_size // 2:\n",
    "        num_batches -= 1\n",
    "        remainder += max_batch_size\n",
    "\n",
    "    batches = []\n",
    "    start = 0\n",
    "    for i in range(num_batches):\n",
    "        end = start + max_batch_size\n",
    "        batches.append(timeline[start:end])\n",
    "        start = end\n",
    "    \n",
    "    if remainder > 0:\n",
    "        batches.append(timeline[start:start + remainder])\n",
    "    return batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_df_embeddings(df_train, df_test):\n",
    "    print(\"Processing embedding data and scaling data...\\n\")\n",
    "    # Deserializing the embeddings\n",
    "    body_embeddings_train = np.array(df_train['embeddings'].apply(ast.literal_eval).tolist())\n",
    "    title_embeddings_train = np.array(df_train['Title_embeddings'].apply(ast.literal_eval).tolist())\n",
    "    tags_embeddings_train = np.array(df_train['tags_embeddings'].apply(ast.literal_eval).tolist())\n",
    "\n",
    "    body_embeddings_test = np.array(df_test['embeddings'].apply(ast.literal_eval).tolist())\n",
    "    title_embeddings_test = np.array(df_test['Title_embeddings'].apply(ast.literal_eval).tolist())\n",
    "    tags_embeddings_test = np.array(df_test['tags_embeddings'].apply(ast.literal_eval).tolist())\n",
    "\n",
    "    # Combine embeddings\n",
    "    all_embeddings_train = np.concatenate((body_embeddings_train, title_embeddings_train, tags_embeddings_train), axis=1)\n",
    "    all_embeddings_test = np.concatenate((body_embeddings_test, title_embeddings_test, tags_embeddings_test), axis=1)\n",
    "\n",
    "    # Standardize embeddings\n",
    "    scaler = StandardScaler()\n",
    "    train_embeddings = scaler.fit_transform(all_embeddings_train)\n",
    "    test_embeddings = scaler.transform(all_embeddings_test)\n",
    "    return train_embeddings,  test_embeddings\n",
    "\n",
    "def get_variance_performance(train_embeddings):\n",
    "# Experiment for this variance range of 94% to 97%\n",
    "    print(\"Finding best Model parameters...\\n\")\n",
    "    variance_range = list(np.arange(0.92, 0.95, 0.01))\n",
    "    variance_dic = {}\n",
    "\n",
    "    for variance in variance_range:\n",
    "        pca = PCA(n_components=variance)\n",
    "        train_pca_embeddings = pca.fit_transform(train_embeddings)\n",
    "        \n",
    "        # Range of max_d values to try, for this dataset we use 65\n",
    "        max_d_values = np.arange(45, 65)\n",
    "        \n",
    "        # List to store silhouette scores\n",
    "        silhouette_scores_train = []\n",
    "\n",
    "        # Perform hierarchical clustering\n",
    "        Z = linkage(train_pca_embeddings, method='ward')\n",
    "\n",
    "        for max_d in max_d_values:\n",
    "            clusters_train = fcluster(Z, max_d, criterion='distance')\n",
    "            \n",
    "            # Calculate silhouette score only if there are at least 2 unique clusters and fewer than the number of samples\n",
    "            if 1 < len(set(clusters_train)) < len(train_pca_embeddings):\n",
    "                score_train = silhouette_score(train_pca_embeddings, clusters_train)\n",
    "            else:\n",
    "                score_train = -1  # Assign a score of -1 if less than 2 unique clusters or too many clusters\n",
    "            \n",
    "            silhouette_scores_train.append(score_train)\n",
    "\n",
    "        # Determine the best max_d\n",
    "        best_max_d_train = max_d_values[np.argmax(silhouette_scores_train)]\n",
    "        variance_dic[variance] = {\n",
    "            'max_d_train': best_max_d_train,\n",
    "            'best_train_silhouette': max(silhouette_scores_train)\n",
    "        }\n",
    "    return variance_dic\n",
    "\n",
    "def get_best_variance(perf_results):\n",
    "    highest_train_sil = 0\n",
    "    best_variance_s = []\n",
    "    for variance, scores in perf_results.items():\n",
    "        if scores['best_train_silhouette'] > highest_train_sil:\n",
    "            highest_train_sil = scores['best_train_silhouette']\n",
    "            best_variance_s = [variance]  \n",
    "        elif scores['best_train_silhouette'] == highest_train_sil:\n",
    "            best_variance_s.append(variance)  \n",
    "    \n",
    "    final_best_max_d = perf_results[best_variance_s[0]]['max_d_train']\n",
    "    print(f\"Best variance for this dataset is {round(best_variance_s[0], 2)} and the best maximum distance is {final_best_max_d}\\n\")\n",
    "    return round(best_variance_s[0], 2), final_best_max_d\n",
    "\n",
    "def predict_cluster(test_embedding, train_embeddings, clusters):\n",
    "        distances = np.linalg.norm(train_embeddings - test_embedding, axis=1)\n",
    "        return clusters[np.argmin(distances)]\n",
    "\n",
    "def get_cluster_labels(best_variance, best_max_d, train_embeddings, test_embeddings, df_train, df_test):\n",
    "    # Perform PCA\n",
    "    print(f\"Training new Hierarchical Clustering model with best variance: {best_variance} and max_d: {best_max_d}\\n\")\n",
    "    pca = PCA(n_components=best_variance)\n",
    "    pca_train_embeddings = pca.fit_transform(train_embeddings)\n",
    "    pca_test_embeddings = pca.transform(test_embeddings)\n",
    "\n",
    "    Z = linkage(pca_train_embeddings, method='ward', metric='euclidean')\n",
    "    clusters_train = fcluster(Z, best_max_d, criterion='distance')\n",
    "    # Predict clusters for test data using the nearest cluster center\n",
    "\n",
    "    test_clusters = [predict_cluster(te, pca_train_embeddings, clusters_train) for te in pca_test_embeddings]\n",
    "\n",
    "    df_train['Cluster_labels'] = clusters_train\n",
    "    df_test['Cluster_labels'] = test_clusters\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Create a dictionary to store the results\n",
    "    cluster_dict = {}\n",
    "\n",
    "    # Populate the dictionary with cluster contents for each test point\n",
    "    for i, (test_point, test_cluster) in enumerate(zip(df_test.itertuples(), test_clusters)):\n",
    "        cluster_contents = []\n",
    "        \n",
    "        cluster_indices = np.where(clusters_train == test_cluster)[0]\n",
    "        cluster_df = df_train.iloc[cluster_indices]\n",
    "        \n",
    "        cluster_dict = {\n",
    "            \"Test point\": {'id': test_point.id,\n",
    "                        \"Title\": test_point.Title, \n",
    "                        \"Tags\": test_point.tags},\n",
    "            \"Cluster\": test_cluster,\n",
    "            \"Cluster contents\": cluster_contents\n",
    "        }\n",
    "        \n",
    "        for _, row in cluster_df.iterrows():\n",
    "            cluster_contents.append({\"id\": row['id'], \n",
    "                                    \"Title\": row['Title'],\n",
    "                                    \"Tags\": row['tags'], \n",
    "                                    })\n",
    "\n",
    "    print(f\"Cluster Label {test_cluster} is chosen\\n\")\n",
    "    input_list = \"\"\n",
    "    input_list += f\"Test Artice Chosen: (Title: {cluster_dict['Test point']['Title']}\\nTags: {cluster_dict['Test point']['Tags']}):\\n\"\n",
    "    for _, row in cluster_df.iterrows():\n",
    "        input_list += f\"Article id: {row['id']}, Title: {row['Title']}, Tags: {row['tags']}]\\n\"\n",
    "    return input_list, df_train, df_test\n",
    "\n",
    "def generate_clusters(df_train, df_test):\n",
    "    train_embeddings, test_embeddings = scale_df_embeddings(df_train, df_test)\n",
    "    variance_perf = get_variance_performance(train_embeddings)\n",
    "    best_variance, best_max_d = get_best_variance(variance_perf)\n",
    "    relevant_articles, df_train, df_test = get_cluster_labels(best_variance, best_max_d, train_embeddings, test_embeddings, df_train, df_test)\n",
    "    return relevant_articles, df_train, df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_generate_timeline(test_data):\n",
    "    print(\"Evaluating necessity of Timeline for this aricle.\\n\")\n",
    "    llm = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "    class Event(BaseModel):\n",
    "        score: int = Field(description=\"The need for this article to have a timeline\")\n",
    "        Reason: str = Field(description = \"The main reason for your choice why a timeline is needed or why it is not needed\")\n",
    "            \n",
    "    output_parser = JsonOutputParser(pydantic_object=Event)\n",
    "\n",
    "        # See the prompt template you created for formatting\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    # Define the template\n",
    "    template = '''\n",
    "    You are a highly intelligent AI tasked with analyzing articles to determine whether generating a timeline of events leading up to the key event in the article would be beneficial. \n",
    "    Consider the following factors to make your decision:\n",
    "    1. **Significance of the Event**:\n",
    "       - Does the event have a significant impact on a large number of people, industries, or countries?\n",
    "       - Are the potential long-term consequences of the event important?\n",
    "\n",
    "    2. **Controversy or Debate**:\n",
    "       - Is the event highly controversial or has it sparked significant debate?\n",
    "       - Has the event garnered significant media attention and public interest?\n",
    "\n",
    "    3. **Complexity**:\n",
    "       - Does the event involve multiple factors, stakeholders, or causes that make it complex?\n",
    "       - Does the event have deep historical roots or is it the culmination of long-term developments?\n",
    "\n",
    "    4. **Personal Relevance**:\n",
    "       - Does the event directly affect the reader or their community?\n",
    "       - Is the event of particular interest to the reader due to economic implications, political affiliations, or social issues?\n",
    "\n",
    "    5. Educational Purposes:\n",
    "       - Would a timeline provide valuable learning or research information?\n",
    "\n",
    "    Here is the information for the article:\n",
    "    Title:{title}\n",
    "    Text: {text}\n",
    "    \n",
    "\n",
    "    Based on the factors above, decide whether generating a timeline of events leading up to the key event in this article would be beneficial. \n",
    "    Your answer will include the need for this article to have a timeline with a score 1 - 5, 1 means unnecessary, 5 means necessary. It will also include the main reason for your choice.\n",
    "    {format_instructions}    \n",
    "    ANSWER:\n",
    "    '''\n",
    "\n",
    "    # Create the prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"text\", \"title\"],\n",
    "        partial_variables={\"format_instructions\": format_instructions},\n",
    "        template=template,\n",
    "    )\n",
    "\n",
    "        # Define the headline\n",
    "    headline = test_data[\"Title\"]\n",
    "    body = test_data[\"Text\"]\n",
    "\n",
    "        # Format the prompt\n",
    "    final_prompt = prompt.format(title=headline, text=body)\n",
    "\n",
    "        # Generate content using the generative model\n",
    "    response = llm.generate_content(\n",
    "            final_prompt,\n",
    "            safety_settings={\n",
    "                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "            }\n",
    "        )\n",
    "    final_response = clean_llm_score(response)\n",
    "    # If LLM approves\n",
    "    if final_response['score'] >=3:\n",
    "        print(\"Timeline is necessary for this chosen article.\\n\")\n",
    "        return True, None\n",
    "    else:\n",
    "        print(\"A timeline for this article is not required. \\n\")\n",
    "        for part in final_response['Reason'].replace(\". \", \".\").split(\". \"):\n",
    "            print(f\"{part}\\n\")\n",
    "        print(\"Hence I gave this a required timeline score of \" + str(final_response['score']))\n",
    "        output_error = \"A timeline for this article is not required. \\n\" \\\n",
    "                    + \"\\n\" +final_response['Reason'] + \"\\n\"+ \"\\nHence this timeline received a necessity score of \" \\\n",
    "                    + str(final_response['score'])\n",
    "        return False, output_error\n",
    "\n",
    "def get_article_dict(input_list, df_train, df_test):\n",
    "    llm = genai.GenerativeModel(\"gemini-1.5-flash-latest\")\n",
    "\n",
    "    # Initialize the generative model\n",
    "    class Event(BaseModel):\n",
    "        Article_id: list = Field(description=\"Article ids that are most relevant for the generation of the timeline\")\n",
    "            \n",
    "\n",
    "    output_parser = JsonOutputParser(pydantic_object=Event)\n",
    "\n",
    "    # See the prompt template you created for formatting\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    template = '''\n",
    "    Task Description: Given the following test article, and the relevant tags of that article, and the contents of articles similar to it.\n",
    "    You will only select the articles that are closest in similarity to the test article, \\\n",
    "    for which i will be able to leverage on to build a timeline upon. \n",
    "    Return the article ids for the chosen articles. \n",
    "    Ensure that the chosen articles are relevant in terms of geographical location, main topic and whether or not they are talking about the same event or topic.\n",
    "    {text}\n",
    "\n",
    "    {format_instructions}\n",
    "    Check and ensure again that the output follows the format instructions above very strictly. \n",
    "    '''\n",
    "\n",
    "    # Create the prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        partial_variables={\"format_instructions\": format_instructions},\n",
    "        template=template,\n",
    "    )\n",
    "\n",
    "    final_prompt = prompt.format(text=input_list)\n",
    "    response = llm.generate_content(\n",
    "            final_prompt,\n",
    "            safety_settings={\n",
    "                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "            }\n",
    "        )\n",
    "    new_output = re.search(r'\\[[^\\]]*\\]', response.parts[0].text).group(0)\n",
    "    article_keys =  json.loads(new_output)\n",
    "    if not article_keys:\n",
    "        print(\"No useful similar articles found in database for timeline generation.\\n\")\n",
    "        sys.exit()\n",
    "    \n",
    "    similar_articles_dict = {}\n",
    "    \n",
    "    # Iterate over each test article in the filtered df_test\n",
    "    for index, test_row in df_test.iterrows():\n",
    "        test_cluster_label = test_row['Cluster_labels']\n",
    "        \n",
    "        # Filter df_train for the same cluster label\n",
    "        df_train_cluster = df_train[df_train['Cluster_labels'] == test_cluster_label]\n",
    "        \n",
    "        # Find similar articles in df_train\n",
    "        similar_indexes = []\n",
    "        for train_index, train_row in df_train_cluster.iterrows():\n",
    "            if train_row['id'] in article_keys:\n",
    "                similar_indexes.append(train_index)\n",
    "        \n",
    "        # Store the result in the dictionary if there are at least 2 supporting articles\n",
    "        if len(similar_indexes) >= 2:\n",
    "            similar_articles_dict = {\n",
    "                'Title': test_row['Title'],\n",
    "                'indexes': similar_indexes,\n",
    "                'Text': test_row['Text']\n",
    "            }\n",
    "    if not similar_articles_dict:\n",
    "        print(\"There are insufficient relevant articles to construct a meaningful timeline. ... Exiting execution now\\n\")\n",
    "        return \"generate_similar_error\"\n",
    "    else:\n",
    "        # Print results \n",
    "        print(\"-\"*80 + \"\\n\")\n",
    "        print(f\"Test Article Title: << {similar_articles_dict['Title']}>>\\n\")\n",
    "        print(\"Supporting Article Titles:\")\n",
    "        for idx in similar_articles_dict['indexes']:\n",
    "            print(f\" - {df_train.loc[idx, 'Title']}\")\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        return similar_articles_dict\n",
    "        \n",
    "def generate_and_sort_timeline(similar_articles_dict, df_train, df_test):\n",
    "    llm = genai.GenerativeModel('gemini-1.5-flash-latest' )\n",
    "    \n",
    "    class Event(BaseModel):\n",
    "        Date: str = Field(description=\"The date of the event in YYYY-MM-DD format\")\n",
    "        Event: str = Field(description=\"A detailed description of the important event\")\n",
    "        Article: int = Field(description=\"The article number from which the event was extracted\")\n",
    "\n",
    "    output_parser = JsonOutputParser(pydantic_object=Event)\n",
    "\n",
    "    # See the prompt template you created for formatting\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    template = '''\n",
    "    Given an article, containing a publication date, title, and content, your task is to construct a detailed timeline of events leading up to the main event described in the article.\n",
    "    Begin by thoroughly analyzing the title, content, and publication date of the article to understand the main event in the article. \n",
    "    the dates are represented in YYYY-MM-DD format. Identify events, context, and any time references such as \"last week,\" \"last month,\" or specific dates. \n",
    "    The article could contain more or one key events. \n",
    "    If the article does not provide a publication date or any events leading up to the main event, return NAN in the Date field, and 0 i the Article Field\n",
    "\n",
    "    Construct the Timeline:\n",
    "    Chronological Order: Organize the events chronologically, using the publication dates and time references within the articles.\n",
    "    Detailed Descriptions: Provide detailed descriptions of each event, explaining how it relates to the main event of the first article.\n",
    "    Contextual Links: Use information from the articles to link events together logically and coherently.\n",
    "    Handle Ambiguities: If an article uses ambiguous time references, infer the date based on the publication date of the article and provide a clear rationale for your inference.\n",
    "\n",
    "    Contextual Links:\n",
    "    External Influences: Mention any external influences (e.g., global conflicts, economic trends, scientific discoveries) that might have indirectly affected the events.\n",
    "    Internal Issues: Highlight any internal issues or developments (e.g., political changes, organizational restructuring, societal movements) within the entities involved that might have impacted the events.\n",
    "    Efforts for Improvement: Note any indications of efforts to improve the situation (e.g., policy changes, strategic initiatives, collaborative projects) despite existing challenges.\n",
    "\n",
    "    Be as thorough and precise as possible, ensuring the timeline accurately reflects the sequence and context of events leading to the main event.\n",
    "\n",
    "    Article:\n",
    "    {text}\n",
    "\n",
    "    {format_instructions}\n",
    "    Check and ensure again that the output follows the format instructions above very strictly. \n",
    "    '''\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        partial_variables={\"format_instructions\": format_instructions},\n",
    "        template=template\n",
    "    )\n",
    "    \n",
    "    def generate_individual_timeline(date_text_triples):\n",
    "        s =  f'Article {date_text_triples[0]}: Publication date: {date_text_triples[1]} Article Text: {date_text_triples[2]}'\n",
    "        final_prompt = prompt.format(text=s)\n",
    "        response = llm.generate_content(final_prompt,\n",
    "                                        safety_settings={\n",
    "                                            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, \n",
    "                                            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "                                            })\n",
    "        # Check if Model returns correct format \n",
    "        if '[' in response.parts[0].text or '{' in response.parts[0].text:\n",
    "            result = response.parts[0].text\n",
    "        else:\n",
    "            retry_response = llm.generate_content(final_prompt,\n",
    "                                        safety_settings={\n",
    "                                            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, \n",
    "                                            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "                                            })\n",
    "            try:\n",
    "                result = retry_response.parts[0].text\n",
    "            except ValueError:\n",
    "                print(\"ERROR: There were issues with the generation of the timeline. The timeline could not be generated\")\n",
    "                return\n",
    "        return result\n",
    "    \n",
    "    def process_articles(df_train):\n",
    "        df_retrieve = df_train.loc[similar_articles_dict['indexes']]\n",
    "        df_retrieve = pd.concat([df_retrieve, df_test], axis=0).iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "        # Prepare texts and publication dates\n",
    "        indiv_numbers = list(range(1,len(df_retrieve)+1))\n",
    "        indiv_text = df_retrieve['combined'].tolist()\n",
    "        indiv_dates = df_retrieve['Publication_date'].tolist()\n",
    "        date_text_triples = list(zip(indiv_numbers, indiv_text, indiv_dates))\n",
    "\n",
    "        dict_of_timelines = {}\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=len(date_text_triples)) as executor:\n",
    "            futures = {executor.submit(generate_individual_timeline, date_text_triple): date_text_triple for date_text_triple in date_text_triples}\n",
    "            i = 0\n",
    "            for future in as_completed(futures):\n",
    "                dict_of_timelines[i] = future.result()\n",
    "                i += 1\n",
    "        return dict_of_timelines, df_retrieve\n",
    "    \n",
    "    timeline_dic, df_retrieve = process_articles(df_train)\n",
    "    \n",
    "    print(\"The first timeline has been generated\\n\")\n",
    "    generated_timeline = []\n",
    "    for idx, line in timeline_dic.items():\n",
    "        indiv_timeline = clean_output(line)\n",
    "        if type(indiv_timeline) == list:\n",
    "            for el in indiv_timeline:\n",
    "                generated_timeline.append(el)\n",
    "        else:\n",
    "            generated_timeline.append(indiv_timeline)\n",
    "    \n",
    "    unsorted_timeline = []\n",
    "    for event in generated_timeline:\n",
    "        article_index = event[\"Article\"] - 1\n",
    "        event[\"Article_id\"] = df_retrieve.iloc[article_index].id\n",
    "    for event in generated_timeline:\n",
    "        del event[\"Article\"]\n",
    "        unsorted_timeline.append(event)  \n",
    "        \n",
    "    timeline = sorted(unsorted_timeline, key=lambda x:x['Date'])\n",
    "    finished_timeline = [event for event in timeline if event['Date'].lower()!= 'nan']\n",
    "    for i in range(len(generated_timeline)):\n",
    "        date = generated_timeline[i]['Date']\n",
    "        if date.endswith('-XX-XX') or date.endswith('00-00'):\n",
    "            generated_timeline[i]['Date'] = date[:4]\n",
    "        elif date.endswith('-XX') or date.endswith('00'):\n",
    "            generated_timeline[i]['Date'] = date[:7]\n",
    "    return finished_timeline\n",
    "\n",
    "def enhance_timeline(timeline):\n",
    "    print(\"\\nProceeding to enhance the timeline...\\n\")\n",
    "    llm = genai.GenerativeModel(model_name='gemini-1.5-flash-latest')\n",
    "\n",
    "    class Event(BaseModel):\n",
    "            Date: str = Field(description=\"The date of the event in YYYY-MM-DD format\")\n",
    "            Event: str = Field(description=\"A detailed description of the event\")\n",
    "            Contextual_Annotation: str = Field(description=\"Contextual anecdotes of the event.\")\n",
    "            Article_id: list = Field(description=\"The article id(s) from which the event was extracted\")\n",
    "\n",
    "    parser = JsonOutputParser(pydantic_object=Event)\n",
    "\n",
    "    template = '''\n",
    "        You are given a timeline of events, your task is to enhance this timeline by improving its clarity and contextual information.\n",
    "        IF the same event occurs on the exact same date, merge these events to avoid redundancy, and add the article ids to a list. \n",
    "        Add contextual annotations by providing brief annotations for major events to give additional context and improve understanding.\n",
    "        Only retain important information that would be value-add when the general public reads the information.\n",
    "\n",
    "        Initial Timeline:\n",
    "        {text}\n",
    "\n",
    "        {format_instructions}\n",
    "        Ensure that the format follows the example output format strictly before returning the output.\n",
    "        '''\n",
    "    prompt = PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=template\n",
    "        )\n",
    "            \n",
    "    def generate_enhanced(batch):\n",
    "        batch_timeline_text = json.dumps(batch)\n",
    "        final_prompt = prompt.format(text=batch_timeline_text, format_instructions=parser.get_format_instructions())\n",
    "        response = llm.generate_content(final_prompt,\n",
    "            safety_settings={\n",
    "                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, \n",
    "                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "            }\n",
    "        )\n",
    "        data = extract_json_from_string(response.parts[0].text)\n",
    "        return data\n",
    "\n",
    "    def process_articles(timeline):\n",
    "        results = []\n",
    "        batches = split_batches(timeline, max_batch_size=30)\n",
    "        num_batches = len(batches)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=num_batches) as executor:\n",
    "            print(\"Processing batches simultaneously now...\\n\")\n",
    "            futures = {executor.submit(generate_enhanced, batch): batch for batch in batches}\n",
    "            for future in as_completed(futures):\n",
    "                indiv_batch = future.result()\n",
    "                for event in indiv_batch:\n",
    "                    results.append(event)\n",
    "        return results\n",
    "\n",
    "    full_enhanced = process_articles(timeline)\n",
    "    sorted_timeline = sorted(full_enhanced, key=lambda x:x['Date'])\n",
    "    print(\"Finished enhancing the timeline\\n\")\n",
    "    return sorted_timeline\n",
    "\n",
    "def save_enhanced_timeline(enhanced_timeline):\n",
    "    \"\"\"\n",
    "    Save the enhanced timeline to a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    enhanced_timeline (list): The enhanced timeline data.\n",
    "    output_path (str): The file path where the JSON will be saved.\n",
    "    \"\"\"\n",
    "    \n",
    "    def edit_timeline(timeline):\n",
    "        for event in timeline:\n",
    "            new_date = format_timeline_date(event['Date'])\n",
    "            event['Date'] = new_date\n",
    "            # Check if Contextual Annotation empty    \n",
    "            if not event['Contextual_Annotation']:\n",
    "                event['Contextual_Annotation'] = \"NONE\"\n",
    "        return timeline\n",
    "\n",
    "    edited_timeline = edit_timeline(enhanced_timeline)\n",
    "    json_data = json.dumps(edited_timeline, indent=4, ensure_ascii=False)\n",
    "    return json_data\n",
    "\n",
    "def generate_save_timeline(relevant_articles, df_train, df_test):\n",
    "    similar_articles = get_article_dict(relevant_articles, df_train, df_test)\n",
    "    if similar_articles == \"generate_similar_error\":\n",
    "        return \"Error02\"\n",
    "    generated_timeline = generate_and_sort_timeline(similar_articles, df_train, df_test)\n",
    "    final_timeline = enhance_timeline(generated_timeline)\n",
    "    final_timeline = save_enhanced_timeline(final_timeline)\n",
    "    return final_timeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_hierarchical(test_article, df_train):\n",
    "    #check if the test point is worth generating a timeline. \n",
    "    to_generate, reason01 = to_generate_timeline(test_article)\n",
    "    if to_generate:\n",
    "        df_test = pd.DataFrame(test_article)\n",
    "        relevant_articles, df_train, df_test = generate_clusters(df_train, df_test)\n",
    "        final_timeline = generate_save_timeline(relevant_articles, df_train, df_test)\n",
    "        if final_timeline==\"Error02\":\n",
    "            reason02 = \"There are insufficient relevant articles to construct a meaningful timeline. \"\n",
    "            return \"generate_similar_articles_error\", reason02\n",
    "        return final_timeline, None\n",
    "    else:\n",
    "        return \"to_generate_error\", reason01    \n",
    "\n",
    "def load_mongo():\n",
    "    print(\"Fetching article data from MongoDB...\\n\")\n",
    "    # Connect to the MongoDB client\n",
    "    try:\n",
    "        db = mongo_client[config[\"database\"][\"name\"]]\n",
    "        train_docs = db[config[\"database\"][\"train_collection\"]].find()\n",
    "        print(\"Data successfully fetched from MongoDB\\n\")\n",
    "    except Exception as error: \n",
    "        print(\"Unable to fetch data from MongoDB. Check your connection the database...\\n\")\n",
    "        print(f\"ERROR: {error}\\n\")\n",
    "        sys.exit()\n",
    "    return train_docs\n",
    "\n",
    "def gradio_generate_timeline(test_articles_json, index):\n",
    "    print(\"Starting Timeline Generation\\n\")\n",
    "    \n",
    "    # Function to load JSON formatted test articles:\n",
    "    def load_test_articles():\n",
    "        with open(test_articles_json, \"r\", encoding='utf-8') as fin:\n",
    "            test_database = json.load(fin)\n",
    "            print(\"Test Database loaded\\n\")\n",
    "        return test_database\n",
    "    \n",
    "    train_database = load_mongo()\n",
    "    test_database = load_test_articles()\n",
    "\n",
    "    # Select the test article based on the given index\n",
    "    test_article = test_database[index-1]\n",
    "    \n",
    "    df_train = pd.DataFrame(train_database)\n",
    "    \n",
    "    # Validate the index\n",
    "    if index < 0 or index >= len(test_database):\n",
    "        return {\"error\": \"Index out of range\"}\n",
    "\n",
    "    test_article_id = test_article['id']\n",
    "    \n",
    "    # Run this after gradio workflow tested\n",
    "    timeline, fail_reason = main_hierarchical(test_article, df_train)\n",
    "    \n",
    "    # Pull database\n",
    "    db = mongo_client[config[\"database\"][\"name\"]]\n",
    "    \n",
    "    # Get collection from database\n",
    "    gen_timeline_documents = db[config[\"database\"][\"timelines_collection\"]]\n",
    "            \n",
    "    # If timeline should not be generated\n",
    "    if timeline == \"to_generate_error\" or timeline == \"generate_similar_articles_error\":\n",
    "        \n",
    "        # Timeline instance to return for error message\n",
    "        timeline_return = {\"Article_id\": test_article_id, \"error\": fail_reason}\n",
    "        \n",
    "        # Timeline instance to export to MongoDB\n",
    "        timeline_export = {\"Article_id\": test_article_id, \"Timeline\": \"null\"}\n",
    "        try:\n",
    "            # Insert result into collection\n",
    "            gen_timeline_documents.insert_one(timeline_export)\n",
    "            print(\"Data successfully saved to MongoDB\")\n",
    "        except Exception as error:\n",
    "            print(f\"Unable to save timeline to database. Check your connection the database...\\nERROR: {error}\\n\")\n",
    "            sys.exit()\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        # Convert the timeline to JSON\n",
    "        timeline_json = json.dumps(timeline)\n",
    "        timeline_return = {\"Article_id\": test_article_id, \"Timeline\": timeline_json}\n",
    "        timeline_export = timeline_return\n",
    "        \n",
    "        # Send the timeline data to MongoDB\n",
    "        try:\n",
    "            # Insert result into collection\n",
    "            gen_timeline_documents.insert_one(timeline_export)\n",
    "            print(\"Data successfully saved to MongoDB\")\n",
    "        except Exception as error:\n",
    "            print(f\"Unable to save timeline to database. Check your connection the database...\\nERROR: {error}\\n\")\n",
    "            sys.exit()\n",
    "    return timeline_return\n",
    "\n",
    "def display_timeline(timeline_str):\n",
    "    print(\"Displaying timeline on Gradio Interface \\n\")\n",
    "    timeline_list = json.loads(timeline_str)\n",
    "    display_list= timeline_list[:3]\n",
    "    html_content = \"First 3 events in the timeline:\\n<div style='padding: 10px;'>\"\n",
    "    for event in display_list:\n",
    "        html_content += f\"<h3>{event['Date']}</h3>\"\n",
    "        html_content += f\"<p><strong>Event:</strong> {event['Event']}</p>\"\n",
    "        html_content += f\"<p><strong>Contextual Annotation:</strong> {event['Contextual_Annotation']}</p>\"\n",
    "        html_content += \"<p><strong>Article IDs:</strong> \" + \", \".join(event['Article_id']) + \"</p>\"\n",
    "        html_content += \"<hr>\"\n",
    "    html_content += \"</div>\"\n",
    "    return html_content\n",
    "\n",
    "def user_download_timeline(timeline, article_id):\n",
    "    timeline_export = {\"Article_id\": article_id, \"Timeline\": timeline}\n",
    "    return json.dumps(timeline_export, indent=4)\n",
    "\n",
    "def display_gradio():\n",
    "    with gr.Blocks(title=\"Article Timeline Generator\", theme='snehilsanyal/scikit-learn') as gradio_timeline:\n",
    "        gr.Markdown(\"\"\"\n",
    "            <h1 style='text-align: center;'>\n",
    "            Timeline Generator\n",
    "            </h1>\n",
    "            <hr>\n",
    "            <h3>\n",
    "            Upload the JSON database and choose an article index to generate a timeline.\n",
    "            </h3>\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    input_test_db = gr.File(label=\"Upload the JSON Database\")\n",
    "                    input_test_index = gr.Number(label=\"Test Article Index. Choose an index from 1-7\", value=0)\n",
    "                    hidden_article_id = gr.Textbox(visible=False)\n",
    "\n",
    "                    with gr.Row():\n",
    "                        clear_button = gr.Button(\"Reset index\")\n",
    "                        generate_button = gr.Button(\"Generate Timeline\")\n",
    "                    output_timeline = gr.JSON(label=\"Generated Timeline in JSON format\")\n",
    "                    gr.Markdown('''\n",
    "                                If error message not shon past the 7 second mark, Timeline is necessary for the chosen article. \n",
    "                                ''')\n",
    "                    output_error = gr.Textbox(label=\"Error Message:\", visible=False)  # Initialize as not visible\n",
    "                \n",
    "                with gr.Column():\n",
    "                    show_timeline_button = gr.Button(\"Show Generated Timeline\")\n",
    "                    output_timeline_HTML = gr.HTML()\n",
    "                    download_button = gr.DownloadButton(\"Download Full Timeline\", visible=True)\n",
    "        \n",
    "        clear_button.click(lambda: 0, None, input_test_index)\n",
    "        output_error.visible = True\n",
    "        \n",
    "        def handle_generate_timeline(test_articles_json, index):\n",
    "                result = gradio_generate_timeline(test_articles_json, index)\n",
    "                article_id = result['Article_id']\n",
    "                if \"error\" in result:\n",
    "                    timeline_error = result[\"error\"]\n",
    "                    return timeline_error, None, article_id, \"\"\n",
    "                else:\n",
    "                    timeline = result['timeline']\n",
    "                    return \"NIL\", timeline, article_id, \"\"\n",
    "\n",
    "        generate_button.click(\n",
    "                handle_generate_timeline,\n",
    "                inputs=[input_test_db, input_test_index],\n",
    "                outputs=[output_error, output_timeline, hidden_article_id]\n",
    "            )\n",
    "\n",
    "        show_timeline_button.click(\n",
    "                display_timeline,\n",
    "                inputs=output_timeline,\n",
    "                outputs=output_timeline_HTML\n",
    "            )\n",
    "        download_button.click(\n",
    "            user_download_timeline,\n",
    "            inputs=[output_timeline, hidden_article_id],\n",
    "            outputs=download_button  # This provides the file for download\n",
    "        )\n",
    "        \n",
    "    gradio_timeline.launch(inbrowser=True, debug=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_gradio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import re\n",
    "from json import JSONDecodeError\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# Import libraries for working with language models and Google Gemini\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "GEMINI_KEY = os.environ.get('GEMINI_KEY')\n",
    "genai.configure(api_key=GEMINI_KEY)\n",
    "\n",
    "# Normally where to do this? (in which function?)\n",
    "with open(\"../gradio_config.yaml\", \"r\") as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "# Initialise mongo client.\n",
    "mongo_client = MongoClient(config[\"database\"][\"uri\"])\n",
    "\n",
    "\n",
    "def clean_llm_score(output):\n",
    "    text = output.parts[0].text.replace(\"```\", '').replace('json','')\n",
    "    result = json.loads(text)\n",
    "    return result\n",
    "\n",
    "def clean_output(output):\n",
    "    try:\n",
    "        updated_timeline = json.loads(output)\n",
    "        return updated_timeline\n",
    "    except JSONDecodeError:\n",
    "        #try 1: Ensuring that the string ends with just the open and close lists brackets\n",
    "        try:\n",
    "            new_output = re.search(r'\\[[^\\]]*\\]', output).group(0)\n",
    "        except AttributeError:\n",
    "            new_output = re.search(r'\\{.*?\\}', output, re.DOTALL).group(0)  \n",
    "        updated_timeline = json.loads(new_output)\n",
    "        return updated_timeline\n",
    "    \n",
    "def extract_json_from_string(string):\n",
    "    # Use a regular expression to find the content within the first and last square brackets\n",
    "    match = re.search(r'\\[.*\\]', string, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        json_content = match.group(0)\n",
    "        try:\n",
    "            # Load the extracted content into a JSON object\n",
    "            json_data = json.loads(json_content)\n",
    "            return json_data\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"Failed to decode JSON:\", e)\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No valid JSON content found.\")\n",
    "        return None\n",
    "\n",
    "def clean_sort_timeline(timelines, df_retrieve):  \n",
    "    generated_timeline = []\n",
    "    for idx, line in timelines.items():\n",
    "        indiv_timeline = clean_output(line)\n",
    "        if type(indiv_timeline) == list:\n",
    "            for el in indiv_timeline:\n",
    "                generated_timeline.append(el)\n",
    "        else:\n",
    "            generated_timeline.append(indiv_timeline)\n",
    "    unsorted_timeline = []\n",
    "\n",
    "    for event in generated_timeline:\n",
    "        article_index = event[\"Article\"] - 1\n",
    "        event[\"Article_id\"] = df_retrieve.iloc[article_index].id\n",
    "    for event in generated_timeline:\n",
    "        del event[\"Article\"]\n",
    "        unsorted_timeline.append(event)  \n",
    "        \n",
    "    timeline = sorted(unsorted_timeline, key=lambda x:x['Date'])\n",
    "    timeline = [event for event in timeline if event['Date'].lower()!= 'nan']\n",
    "    for event in timeline:\n",
    "        date = event['Date']\n",
    "        if date.endswith('-XX-XX'):\n",
    "            event['Date'] = date[:4]\n",
    "        elif date.endswith('-XX'):\n",
    "            event['Date'] = date[:7]\n",
    "    return timeline\n",
    "\n",
    "\n",
    "\n",
    "def format_timeline_date(date_str):\n",
    "    formats = ['%Y', '%Y-%m-%d', '%Y-%m']\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str, fmt)\n",
    "            if fmt == '%Y':\n",
    "                return date_obj.strftime('%Y')\n",
    "            elif fmt == '%Y-%m-%d':\n",
    "                return date_obj.strftime('%d %B %Y')\n",
    "            elif fmt == '%Y-%m':\n",
    "                return date_obj.strftime('%B %Y')\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def split_batches(timeline, max_batch_size=30):\n",
    "    n = len(timeline)\n",
    "    if n <= max_batch_size:\n",
    "        return [timeline]\n",
    "    \n",
    "    num_batches = n // max_batch_size\n",
    "    remainder = n % max_batch_size\n",
    "    \n",
    "    if remainder > 0 and remainder < max_batch_size // 2:\n",
    "        num_batches -= 1\n",
    "        remainder += max_batch_size\n",
    "\n",
    "    batches = []\n",
    "    start = 0\n",
    "    for i in range(num_batches):\n",
    "        end = start + max_batch_size\n",
    "        batches.append(timeline[start:end])\n",
    "        start = end\n",
    "    \n",
    "    if remainder > 0:\n",
    "        batches.append(timeline[start:start + remainder])\n",
    "    return batches\n",
    "\n",
    "\n",
    "\n",
    "def scale_df_embeddings(df_train, df_test):\n",
    "    print(\"Processing embedding data and scaling data...\\n\")\n",
    "    # Deserializing the embeddings\n",
    "    # body_embeddings_train = np.array(df_train['embeddings'].apply(ast.literal_eval).tolist())\n",
    "    # title_embeddings_train = np.array(df_train['Title_embeddings'].apply(ast.literal_eval).tolist())\n",
    "    # tags_embeddings_train = np.array(df_train['tags_embeddings'].apply(ast.literal_eval).tolist())\n",
    "\n",
    "    # body_embeddings_test = np.array(df_test['embeddings'].apply(ast.literal_eval).tolist())\n",
    "    # title_embeddings_test = np.array(df_test['Title_embeddings'].apply(ast.literal_eval).tolist())\n",
    "    # tags_embeddings_test = np.array(df_test['tags_embeddings'].apply(ast.literal_eval).tolist())\n",
    "    \n",
    "    body_embeddings_train = np.array(df_train['Body_embeddings'].apply(ast.literal_eval).tolist())\n",
    "    title_embeddings_train = np.array(df_train['Title_embeddings'].apply(ast.literal_eval).tolist())\n",
    "    tags_embeddings_train = np.array(df_train['MPNET_tags_embeddings'].apply(ast.literal_eval).tolist())\n",
    "\n",
    "    body_embeddings_test = np.array(df_test['Body_embeddings'].apply(ast.literal_eval).tolist())\n",
    "    title_embeddings_test = np.array(df_test['Title_embeddings'].apply(ast.literal_eval).tolist())\n",
    "    tags_embeddings_test = np.array(df_test['MPNET_tags_embeddings'].apply(ast.literal_eval).tolist())\n",
    "\n",
    "    # Combine embeddings\n",
    "    all_embeddings_train = np.concatenate((body_embeddings_train, title_embeddings_train, tags_embeddings_train), axis=1)\n",
    "    all_embeddings_test = np.concatenate((body_embeddings_test, title_embeddings_test, tags_embeddings_test), axis=1)\n",
    "\n",
    "    # Standardize embeddings\n",
    "    scaler = StandardScaler()\n",
    "    train_embeddings = scaler.fit_transform(all_embeddings_train)\n",
    "    test_embeddings = scaler.transform(all_embeddings_test)\n",
    "    return train_embeddings,  test_embeddings\n",
    "\n",
    "def get_variance_performance(train_embeddings):\n",
    "# Experiment for this variance range of 92% to 95%\n",
    "    print(\"Finding best Model parameters...\\n\")\n",
    "    variance_range = list(np.arange(0.92, 0.95, 0.01))\n",
    "    variance_dic = {}\n",
    "\n",
    "    for variance in variance_range:\n",
    "        pca = PCA(n_components=variance)\n",
    "        train_pca_embeddings = pca.fit_transform(train_embeddings)\n",
    "        \n",
    "        # Range of max_d values to try, for this dataset we use 65\n",
    "        max_d_values = np.arange(45, 65)\n",
    "        \n",
    "        # List to store silhouette scores\n",
    "        silhouette_scores_train = []\n",
    "\n",
    "        # Perform hierarchical clustering\n",
    "        Z = linkage(train_pca_embeddings, method='ward')\n",
    "\n",
    "        for max_d in max_d_values:\n",
    "            clusters_train = fcluster(Z, max_d, criterion='distance')\n",
    "            \n",
    "            # Calculate silhouette score only if there are at least 2 unique clusters and fewer than the number of samples\n",
    "            if 1 < len(set(clusters_train)) < len(train_pca_embeddings):\n",
    "                score_train = silhouette_score(train_pca_embeddings, clusters_train)\n",
    "            else:\n",
    "                score_train = -1  # Assign a score of -1 if less than 2 unique clusters or too many clusters\n",
    "            \n",
    "            silhouette_scores_train.append(score_train)\n",
    "\n",
    "        # Determine the best max_d\n",
    "        best_max_d_train = max_d_values[np.argmax(silhouette_scores_train)]\n",
    "        variance_dic[variance] = {\n",
    "            'max_d_train': best_max_d_train,\n",
    "            'best_train_silhouette': max(silhouette_scores_train)\n",
    "        }\n",
    "    return variance_dic\n",
    "\n",
    "def get_best_variance(perf_results):\n",
    "    highest_train_sil = 0\n",
    "    best_variance_s = []\n",
    "    for variance, scores in perf_results.items():\n",
    "        if scores['best_train_silhouette'] > highest_train_sil:\n",
    "            highest_train_sil = scores['best_train_silhouette']\n",
    "            best_variance_s = [variance]  \n",
    "        elif scores['best_train_silhouette'] == highest_train_sil:\n",
    "            best_variance_s.append(variance)  \n",
    "    \n",
    "    final_best_max_d = perf_results[best_variance_s[0]]['max_d_train']\n",
    "    print(f\"Best variance for this dataset is {round(best_variance_s[0], 2)} and the best maximum distance is {final_best_max_d}\\n\")\n",
    "    return round(best_variance_s[0], 2), final_best_max_d\n",
    "\n",
    "def predict_cluster(test_embedding, train_embeddings, clusters):\n",
    "        distances = np.linalg.norm(train_embeddings - test_embedding, axis=1)\n",
    "        return clusters[np.argmin(distances)]\n",
    "\n",
    "def get_cluster_labels(best_variance, best_max_d, train_embeddings, test_embeddings, df_train, df_test):\n",
    "    # Perform PCA\n",
    "    print(f\"Training new Hierarchical Clustering model with best variance: {best_variance} and max_d: {best_max_d}\\n\")\n",
    "    pca = PCA(n_components=best_variance)\n",
    "    pca_train_embeddings = pca.fit_transform(train_embeddings)\n",
    "    pca_test_embeddings = pca.transform(test_embeddings)\n",
    "\n",
    "    Z = linkage(pca_train_embeddings, method='ward', metric='euclidean')\n",
    "    clusters_train = fcluster(Z, best_max_d, criterion='distance')\n",
    "    # Predict clusters for test data using the nearest cluster center\n",
    "\n",
    "    test_clusters = [predict_cluster(te, pca_train_embeddings, clusters_train) for te in pca_test_embeddings]\n",
    "\n",
    "    df_train['Cluster_labels'] = clusters_train\n",
    "    df_test['Cluster_labels'] = test_clusters\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Create a dictionary to store the results\n",
    "    cluster_dict = {}\n",
    "\n",
    "    # Populate the dictionary with cluster contents for each test point\n",
    "    for i, (test_point, test_cluster) in enumerate(zip(df_test.itertuples(), test_clusters)):\n",
    "        cluster_contents = []\n",
    "        \n",
    "        cluster_indices = np.where(clusters_train == test_cluster)[0]\n",
    "        cluster_df = df_train.iloc[cluster_indices]\n",
    "        \n",
    "        cluster_dict = {\n",
    "            \"Test point\": {'id': test_point.st_id,\n",
    "                        \"Title\": test_point.Title, \n",
    "                        \"Tags\": test_point.tags},\n",
    "            \"Cluster\": test_cluster,\n",
    "            \"Cluster contents\": cluster_contents\n",
    "        }\n",
    "        \n",
    "        for _, row in cluster_df.iterrows():\n",
    "            cluster_contents.append({\"id\": row['st_id'], \n",
    "                                    \"Title\": row['Title'],\n",
    "                                    \"Tags\": row['tags'], \n",
    "                                    })\n",
    "\n",
    "    print(f\"Cluster Label {test_cluster} is chosen\\n\")\n",
    "    input_list = \"\"\n",
    "    input_list += f\"Test Artice Chosen: (Title: {cluster_dict['Test point']['Title']}\\nTags: {cluster_dict['Test point']['Tags']}):\\n\"\n",
    "    for _, row in cluster_df.iterrows():\n",
    "        input_list += f\"Article id: {row['st_id']}, Title: {row['Title']}, Tags: {row['tags']}]\\n\"\n",
    "    return input_list, df_train, df_test\n",
    "\n",
    "def generate_clusters(df_train, df_test):\n",
    "    train_embeddings, test_embeddings = scale_df_embeddings(df_train, df_test)\n",
    "    variance_perf = get_variance_performance(train_embeddings)\n",
    "    best_variance, best_max_d = get_best_variance(variance_perf)\n",
    "    relevant_articles, df_train, df_test = get_cluster_labels(best_variance, best_max_d, train_embeddings, test_embeddings, df_train, df_test)\n",
    "    return relevant_articles, df_train, df_test\n",
    "\n",
    "\n",
    "\n",
    "def to_generate_timeline(test_data):\n",
    "    print(\"Evaluating necessity of Timeline for this aricle.\\n\")\n",
    "    llm = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "    class Event(BaseModel):\n",
    "        score: int = Field(description=\"The need for this article to have a timeline\")\n",
    "        Reason: str = Field(description = \"The main reason for your choice why a timeline is needed or why it is not needed\")\n",
    "            \n",
    "    output_parser = JsonOutputParser(pydantic_object=Event)\n",
    "\n",
    "        # See the prompt template you created for formatting\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    # Define the template\n",
    "    template = '''\n",
    "    You are a highly intelligent AI tasked with analyzing articles to determine whether generating a timeline of events leading up to the key event in the article would be beneficial. \n",
    "    Consider the following factors to make your decision:\n",
    "    1. **Significance of the Event**:\n",
    "       - Does the event have a significant impact on a large number of people, industries, or countries?\n",
    "       - Are the potential long-term consequences of the event important?\n",
    "\n",
    "    2. **Controversy or Debate**:\n",
    "       - Is the event highly controversial or has it sparked significant debate?\n",
    "       - Has the event garnered significant media attention and public interest?\n",
    "\n",
    "    3. **Complexity**:\n",
    "       - Does the event involve multiple factors, stakeholders, or causes that make it complex?\n",
    "       - Does the event have deep historical roots or is it the culmination of long-term developments?\n",
    "\n",
    "    4. **Personal Relevance**:\n",
    "       - Does the event directly affect the reader or their community?\n",
    "       - Is the event of particular interest to the reader due to economic implications, political affiliations, or social issues?\n",
    "\n",
    "    5. Educational Purposes:\n",
    "       - Would a timeline provide valuable learning or research information?\n",
    "\n",
    "    Here is the information for the article:\n",
    "    Title:{title}\n",
    "    Text: {text}\n",
    "    \n",
    "\n",
    "    Based on the factors above, decide whether generating a timeline of events leading up to the key event in this article would be beneficial. \n",
    "    Your answer will include the need for this article to have a timeline with a score 1 - 5, 1 means unnecessary, 5 means necessary. It will also include the main reason for your choice.\n",
    "    {format_instructions}    \n",
    "    ANSWER:\n",
    "    '''\n",
    "\n",
    "    # Create the prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"text\", \"title\"],\n",
    "        partial_variables={\"format_instructions\": format_instructions},\n",
    "        template=template,\n",
    "    )\n",
    "\n",
    "        # Define the headline\n",
    "    headline = test_data[\"Title\"]\n",
    "    body = test_data[\"Text\"]\n",
    "\n",
    "        # Format the prompt\n",
    "    final_prompt = prompt.format(title=headline, text=body)\n",
    "\n",
    "        # Generate content using the generative model\n",
    "    response = llm.generate_content(\n",
    "            final_prompt,\n",
    "            safety_settings={\n",
    "                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "            }\n",
    "        )\n",
    "    final_response = clean_llm_score(response)\n",
    "    # If LLM approves\n",
    "    if final_response['score'] >=3:\n",
    "        print(\"Timeline is necessary for this chosen article.\\n\")\n",
    "        return True, None\n",
    "    else:\n",
    "        print(\"A timeline for this article is not required. \\n\")\n",
    "        for part in final_response['Reason'].replace(\". \", \".\").split(\". \"):\n",
    "            print(f\"{part}\\n\")\n",
    "        print(\"Hence I gave this a required timeline score of \" + str(final_response['score']))\n",
    "        output_error = \"A timeline for this article is not required. \\n\" \\\n",
    "                    + \"\\n\" +final_response['Reason'] + \"\\n\"+ \"\\nHence this timeline received a necessity score of \" \\\n",
    "                    + str(final_response['score'])\n",
    "        return False, output_error\n",
    "\n",
    "def get_article_dict(input_list, df_train, df_test):\n",
    "    llm = genai.GenerativeModel(\"gemini-1.5-flash-latest\")\n",
    "\n",
    "    # Initialize the generative model\n",
    "    class Event(BaseModel):\n",
    "        Article_id: list = Field(description=\"Article ids that are most relevant for the generation of the timeline\")\n",
    "            \n",
    "\n",
    "    output_parser = JsonOutputParser(pydantic_object=Event)\n",
    "\n",
    "    # See the prompt template you created for formatting\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    template = '''\n",
    "    Task Description: Given the following test article, and the relevant tags of that article, and the contents of articles similar to it.\n",
    "    You will only select the articles that are closest in similarity to the test article, \\\n",
    "    for which i will be able to leverage on to build a timeline upon. \n",
    "    Return the article ids for the chosen articles. \n",
    "    Ensure that the chosen articles are relevant in terms of geographical location, main topic and whether or not they are talking about the same event or topic.\n",
    "    {text}\n",
    "\n",
    "    {format_instructions}\n",
    "    Check and ensure again that the output follows the format instructions above very strictly. \n",
    "    '''\n",
    "\n",
    "    # Create the prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        partial_variables={\"format_instructions\": format_instructions},\n",
    "        template=template,\n",
    "    )\n",
    "\n",
    "    final_prompt = prompt.format(text=input_list)\n",
    "    response = llm.generate_content(\n",
    "            final_prompt,\n",
    "            safety_settings={\n",
    "                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "            }\n",
    "        )\n",
    "    new_output = re.search(r'\\[[^\\]]*\\]', response.parts[0].text).group(0)\n",
    "    article_keys =  json.loads(new_output)\n",
    "    if not article_keys:\n",
    "        print(\"No useful similar articles found in database for timeline generation.\\n\")\n",
    "        sys.exit()\n",
    "    \n",
    "    similar_articles_dict = {}\n",
    "    \n",
    "    # Iterate over each test article in the filtered df_test\n",
    "    for index, test_row in df_test.iterrows():\n",
    "        test_cluster_label = test_row['Cluster_labels']\n",
    "        \n",
    "        # Filter df_train for the same cluster label\n",
    "        df_train_cluster = df_train[df_train['Cluster_labels'] == test_cluster_label]\n",
    "        \n",
    "        # Find similar articles in df_train\n",
    "        similar_indexes = []\n",
    "        for train_index, train_row in df_train_cluster.iterrows():\n",
    "            if train_row['st_id'] in article_keys:\n",
    "                similar_indexes.append(train_index)\n",
    "        \n",
    "        # Store the result in the dictionary if there are at least 2 supporting articles\n",
    "        if len(similar_indexes) >= 2:\n",
    "            similar_articles_dict = {\n",
    "                'Title': test_row['Title'],\n",
    "                'indexes': similar_indexes,\n",
    "                'Text': test_row['Text']\n",
    "            }\n",
    "    if not similar_articles_dict:\n",
    "        print(\"There are insufficient relevant articles to construct a meaningful timeline. ... Exiting execution now\\n\")\n",
    "        return \"generate_similar_error\"\n",
    "    else:\n",
    "        # Print results \n",
    "        print(\"-\"*80 + \"\\n\")\n",
    "        print(f\"Test Article Title: << {similar_articles_dict['Title']}>>\\n\")\n",
    "        print(\"Supporting Article Titles:\")\n",
    "        for idx in similar_articles_dict['indexes']:\n",
    "            print(f\" - {df_train.loc[idx, 'Title']}\")\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        return similar_articles_dict\n",
    "        \n",
    "def generate_and_sort_timeline(similar_articles_dict, df_train, df_test):\n",
    "    llm = genai.GenerativeModel('gemini-1.5-flash-latest' )\n",
    "    \n",
    "    class Event(BaseModel):\n",
    "        Date: str = Field(description=\"The date of the event in YYYY-MM-DD format\")\n",
    "        Event: str = Field(description=\"A detailed description of the important event\")\n",
    "        Article: int = Field(description=\"The article number from which the event was extracted\")\n",
    "\n",
    "    output_parser = JsonOutputParser(pydantic_object=Event)\n",
    "\n",
    "    # See the prompt template you created for formatting\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    template = '''\n",
    "    Given an article, containing a publication date, title, and content, your task is to construct a detailed timeline of events leading up to the main event described in the article.\n",
    "    Begin by thoroughly analyzing the title, content, and publication date of the article to understand the main event in the article. \n",
    "    the dates are represented in YYYY-MM-DD format. Identify events, context, and any time references such as \"last week,\" \"last month,\" or specific dates. \n",
    "    The article could contain more or one key events. \n",
    "    If the article does not provide a publication date or any events leading up to the main event, return NAN in the Date field, and 0 i the Article Field\n",
    "\n",
    "    Construct the Timeline:\n",
    "    Chronological Order: Organize the events chronologically, using the publication dates and time references within the articles.\n",
    "    Detailed Descriptions: Provide detailed descriptions of each event, explaining how it relates to the main event of the first article.\n",
    "    Contextual Links: Use information from the articles to link events together logically and coherently.\n",
    "    Handle Ambiguities: If an article uses ambiguous time references, infer the date based on the publication date of the article and provide a clear rationale for your inference.\n",
    "\n",
    "    Contextual Links:\n",
    "    External Influences: Mention any external influences (e.g., global conflicts, economic trends, scientific discoveries) that might have indirectly affected the events.\n",
    "    Internal Issues: Highlight any internal issues or developments (e.g., political changes, organizational restructuring, societal movements) within the entities involved that might have impacted the events.\n",
    "    Efforts for Improvement: Note any indications of efforts to improve the situation (e.g., policy changes, strategic initiatives, collaborative projects) despite existing challenges.\n",
    "\n",
    "    Be as thorough and precise as possible, ensuring the timeline accurately reflects the sequence and context of events leading to the main event.\n",
    "\n",
    "    Article:\n",
    "    {text}\n",
    "\n",
    "    {format_instructions}\n",
    "    Check and ensure again that the output follows the format instructions above very strictly. \n",
    "    '''\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        partial_variables={\"format_instructions\": format_instructions},\n",
    "        template=template\n",
    "    )\n",
    "    \n",
    "    def generate_individual_timeline(date_text_triples):\n",
    "        s =  f'Article {date_text_triples[0]}: Publication date: {date_text_triples[1]} Article Text: {date_text_triples[2]}'\n",
    "        final_prompt = prompt.format(text=s)\n",
    "        response = llm.generate_content(final_prompt,\n",
    "                                        safety_settings={\n",
    "                                            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, \n",
    "                                            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "                                            })\n",
    "        # Check if Model returns correct format \n",
    "        if '[' in response.parts[0].text or '{' in response.parts[0].text:\n",
    "            result = response.parts[0].text\n",
    "        else:\n",
    "            retry_response = llm.generate_content(final_prompt,\n",
    "                                        safety_settings={\n",
    "                                            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, \n",
    "                                            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "                                            })\n",
    "            try:\n",
    "                result = retry_response.parts[0].text\n",
    "            except ValueError:\n",
    "                print(\"ERROR: There were issues with the generation of the timeline. The timeline could not be generated\")\n",
    "                return\n",
    "        return result\n",
    "    \n",
    "    def process_articles(df_train):\n",
    "        df_retrieve = df_train.loc[similar_articles_dict['indexes']]\n",
    "        df_retrieve = pd.concat([df_retrieve, df_test], axis=0).iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "        # Prepare texts and publication dates\n",
    "        indiv_numbers = list(range(1,len(df_retrieve)+1))\n",
    "        indiv_text = df_retrieve['combined'].tolist()\n",
    "        indiv_dates = df_retrieve['Publication_date'].tolist()\n",
    "        date_text_triples = list(zip(indiv_numbers, indiv_text, indiv_dates))\n",
    "\n",
    "        dict_of_timelines = {}\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=len(date_text_triples)) as executor:\n",
    "            futures = {executor.submit(generate_individual_timeline, date_text_triple): date_text_triple for date_text_triple in date_text_triples}\n",
    "            i = 0\n",
    "            for future in as_completed(futures):\n",
    "                dict_of_timelines[i] = future.result()\n",
    "                i += 1\n",
    "        return dict_of_timelines, df_retrieve\n",
    "    \n",
    "    timeline_dic, df_retrieve = process_articles(df_train)\n",
    "    \n",
    "    print(\"The first timeline has been generated\\n\")\n",
    "    generated_timeline = []\n",
    "    for idx, line in timeline_dic.items():\n",
    "        indiv_timeline = clean_output(line)\n",
    "        if type(indiv_timeline) == list:\n",
    "            for el in indiv_timeline:\n",
    "                generated_timeline.append(el)\n",
    "        else:\n",
    "            generated_timeline.append(indiv_timeline)\n",
    "    \n",
    "    unsorted_timeline = []\n",
    "    for event in generated_timeline:\n",
    "        article_index = event[\"Article\"] - 1\n",
    "        event[\"Article_id\"] = df_retrieve.iloc[article_index].id\n",
    "    for event in generated_timeline:\n",
    "        del event[\"Article\"]\n",
    "        unsorted_timeline.append(event)  \n",
    "        \n",
    "    timeline = sorted(unsorted_timeline, key=lambda x:x['Date'])\n",
    "    finished_timeline = [event for event in timeline if event['Date'].lower()!= 'nan']\n",
    "    for i in range(len(generated_timeline)):\n",
    "        date = generated_timeline[i]['Date']\n",
    "        if date.endswith('-XX-XX') or date.endswith('00-00'):\n",
    "            generated_timeline[i]['Date'] = date[:4]\n",
    "        elif date.endswith('-XX') or date.endswith('00'):\n",
    "            generated_timeline[i]['Date'] = date[:7]\n",
    "    return finished_timeline\n",
    "\n",
    "def enhance_timeline(timeline):\n",
    "    print(\"\\nProceeding to enhance the timeline...\\n\")\n",
    "    llm = genai.GenerativeModel(model_name='gemini-1.5-flash-latest')\n",
    "\n",
    "    class Event(BaseModel):\n",
    "            Date: str = Field(description=\"The date of the event in YYYY-MM-DD format\")\n",
    "            Event: str = Field(description=\"A detailed description of the event\")\n",
    "            Contextual_Annotation: str = Field(description=\"Contextual anecdotes of the event.\")\n",
    "            Article_id: list = Field(description=\"The article id(s) from which the event was extracted\")\n",
    "\n",
    "    parser = JsonOutputParser(pydantic_object=Event)\n",
    "\n",
    "    template = '''\n",
    "        You are given a timeline of events, your task is to enhance this timeline by improving its clarity and contextual information.\n",
    "        IF the same event occurs on the exact same date, merge these events to avoid redundancy, and add the article ids to a list. \n",
    "        Add contextual annotations by providing brief annotations for major events to give additional context and improve understanding.\n",
    "        Only retain important information that would be value-add when the general public reads the information.\n",
    "\n",
    "        Initial Timeline:\n",
    "        {text}\n",
    "\n",
    "        {format_instructions}\n",
    "        Ensure that the format follows the example output format strictly before returning the output.\n",
    "        '''\n",
    "    prompt = PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=template\n",
    "        )\n",
    "            \n",
    "    def generate_enhanced(batch):\n",
    "        batch_timeline_text = json.dumps(batch)\n",
    "        final_prompt = prompt.format(text=batch_timeline_text, format_instructions=parser.get_format_instructions())\n",
    "        response = llm.generate_content(final_prompt,\n",
    "            safety_settings={\n",
    "                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, \n",
    "                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "            }\n",
    "        )\n",
    "        data = extract_json_from_string(response.parts[0].text)\n",
    "        return data\n",
    "\n",
    "    def process_articles(timeline):\n",
    "        results = []\n",
    "        batches = split_batches(timeline, max_batch_size=30)\n",
    "        num_batches = len(batches)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=num_batches) as executor:\n",
    "            print(\"Processing batches simultaneously now...\\n\")\n",
    "            futures = {executor.submit(generate_enhanced, batch): batch for batch in batches}\n",
    "            for future in as_completed(futures):\n",
    "                indiv_batch = future.result()\n",
    "                for event in indiv_batch:\n",
    "                    results.append(event)\n",
    "        return results\n",
    "\n",
    "    full_enhanced = process_articles(timeline)\n",
    "    sorted_timeline = sorted(full_enhanced, key=lambda x:x['Date'])\n",
    "    print(\"Finished enhancing the timeline\\n\")\n",
    "    return sorted_timeline\n",
    "\n",
    "def save_enhanced_timeline(enhanced_timeline):\n",
    "    \"\"\"\n",
    "    Save the enhanced timeline to a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    enhanced_timeline (list): The enhanced timeline data.\n",
    "    output_path (str): The file path where the JSON will be saved.\n",
    "    \"\"\"\n",
    "    \n",
    "    def edit_timeline(timeline):\n",
    "        for event in timeline:\n",
    "            new_date = format_timeline_date(event['Date'])\n",
    "            event['Date'] = new_date\n",
    "            # Check if Contextual Annotation empty    \n",
    "            if not event['Contextual_Annotation']:\n",
    "                event['Contextual_Annotation'] = \"NONE\"\n",
    "        return timeline\n",
    "\n",
    "    edited_timeline = edit_timeline(enhanced_timeline)\n",
    "    json_data = json.dumps(edited_timeline, indent=4, ensure_ascii=False)\n",
    "    return json_data\n",
    "\n",
    "def generate_save_timeline(relevant_articles, df_train, df_test):\n",
    "    similar_articles = get_article_dict(relevant_articles, df_train, df_test)\n",
    "    if similar_articles == \"generate_similar_error\":\n",
    "        return \"Error02\"\n",
    "    generated_timeline = generate_and_sort_timeline(similar_articles, df_train, df_test)\n",
    "    final_timeline = enhance_timeline(generated_timeline)\n",
    "    final_timeline = save_enhanced_timeline(final_timeline)\n",
    "    return final_timeline\n",
    "\n",
    "\n",
    "\n",
    "def main_hierarchical(test_article, df_train):\n",
    "    #check if the test point is worth generating a timeline. \n",
    "    to_generate, reason01 = to_generate_timeline(test_article)\n",
    "    if to_generate:\n",
    "        df_test = pd.DataFrame([test_article])\n",
    "        relevant_articles, df_train, df_test = generate_clusters(df_train, df_test)\n",
    "        final_timeline = generate_save_timeline(relevant_articles, df_train, df_test)\n",
    "        if final_timeline==\"Error02\":\n",
    "            reason02 = \"There are insufficient relevant articles to construct a meaningful timeline. \"\n",
    "            return \"generate_similar_articles_error\", reason02\n",
    "        return final_timeline, None\n",
    "    else:\n",
    "        return \"to_generate_error\", reason01    \n",
    "\n",
    "def load_mongo_train():\n",
    "    print(\"Fetching article data from MongoDB...\\n\")\n",
    "    # Connect to the MongoDB client\n",
    "    try:\n",
    "        db = mongo_client[config[\"database\"][\"name\"]]\n",
    "        train_docs = db[config[\"database\"][\"train_collection\"]].find()\n",
    "        print(\"Data successfully fetched from MongoDB\\n\")\n",
    "    except Exception as error: \n",
    "        print(f\"Unable to fetch data from MongoDB. Check your connection the database...\\n\")\n",
    "        print(f\"ERROR: {error}\\n\")\n",
    "        sys.exit()\n",
    "    return train_docs\n",
    "\n",
    "def load_mongo_test():\n",
    "    print(\"Fetching article data from MongoDB...\\n\")\n",
    "    # Connect to the MongoDB client\n",
    "    try:\n",
    "        db = mongo_client[config[\"database\"][\"name\"]]\n",
    "        train_docs = db[config[\"database\"][\"test_collection\"]].find()\n",
    "        print(\"Data successfully fetched from MongoDB\\n\")\n",
    "    except Exception as error: \n",
    "        print(f\"Unable to fetch data from MongoDB. Check your connection the database...\\n\")\n",
    "        print(f\"ERROR: {error}\\n\")\n",
    "        sys.exit()\n",
    "    return train_docs\n",
    "\n",
    "\n",
    "\n",
    "def gradio_generate_timeline(test_articles_json, index):\n",
    "    print(\"Starting Timeline Generation\\n\")\n",
    "    \n",
    "    # Function to load JSON formatted test articles:\n",
    "    def load_test_articles():\n",
    "        with open(\"../data/test_data/test_articles.json\", \"r\", encoding='utf-8') as fin:\n",
    "            test_database = json.load(fin)\n",
    "            print(\"Test Database loaded\\n\")\n",
    "        return test_database\n",
    "    \n",
    "    train_database = load_mongo_train()\n",
    "    test_database = load_mongo_test()\n",
    "\n",
    "    # Select the test article based on the given index\n",
    "    test_article = test_database[index-1]\n",
    "    \n",
    "    df_train = pd.DataFrame(train_database)\n",
    "    \n",
    "    # Validate the index\n",
    "    if index < 0 or index >= len(test_database):\n",
    "        return {\"error\": \"Index out of range\"}\n",
    "\n",
    "    test_article_id = test_article['st_id']\n",
    "    \n",
    "    # Run this after gradio workflow tested\n",
    "    timeline, fail_reason = main_hierarchical(test_article, df_train)\n",
    "  \n",
    "    \n",
    "    # Pull database\n",
    "    db = mongo_client[config[\"database\"][\"name\"]]\n",
    "    \n",
    "    # Get collection from database\n",
    "    gen_timeline_documents = db[config[\"database\"][\"timelines_collection\"]]\n",
    "            \n",
    "    # If timeline should not be generated\n",
    "    if timeline == \"to_generate_error\" or timeline == \"generate_similar_articles_error\":\n",
    "        \n",
    "        # Timeline instance to return for error message\n",
    "        timeline_return = {\"Article_id\": test_article_id, \"error\": fail_reason}\n",
    "        \n",
    "        # Timeline instance to export to MongoDB\n",
    "        timeline_export = {\"Article_id\": test_article_id, \"Timeline\": \"null\"}\n",
    "        try:\n",
    "            # Insert result into collection\n",
    "            gen_timeline_documents.insert_one(timeline_export)\n",
    "            print(\"Data successfully saved to MongoDB\")\n",
    "        except Exception as error:\n",
    "            print(f\"Unable to save timeline to database. Check your connection the database...\\nERROR: {error}\\n\")\n",
    "            sys.exit()\n",
    "            \n",
    "    else:\n",
    "        # Convert the timeline to JSON\n",
    "        timeline_json = json.dumps(timeline)\n",
    "        timeline_return = {\"Article_id\": test_article_id, \"Timeline\": timeline_json}\n",
    "        timeline_export = timeline_return\n",
    "        \n",
    "        # Send the timeline data to MongoDB\n",
    "        try:\n",
    "            # Insert result into collection\n",
    "            gen_timeline_documents.insert_one(timeline_export)\n",
    "            print(\"Data successfully saved to MongoDB\")\n",
    "        except Exception as error:\n",
    "            print(f\"Unable to save timeline to database. Check your connection the database...\\nERROR: {error}\\n\")\n",
    "            sys.exit()\n",
    "    return timeline_return\n",
    "\n",
    "def display_timeline(timeline_str):\n",
    "    print(\"Displaying timeline on Gradio Interface \\n\")\n",
    "    timeline_list = json.loads(timeline_str)\n",
    "    display_list= timeline_list[:3]\n",
    "    html_content = \"First 3 events in the timeline:\\n<div style='padding: 10px;'>\"\n",
    "    for event in display_list:\n",
    "        html_content += f\"<h3>{event['Date']}</h3>\"\n",
    "        html_content += f\"<p><strong>Event:</strong> {event['Event']}</p>\"\n",
    "        html_content += f\"<p><strong>Contextual Annotation:</strong> {event['Contextual_Annotation']}</p>\"\n",
    "        html_content += \"<p><strong>Article IDs:</strong> \" + \", \".join(event['Article_id']) + \"</p>\"\n",
    "        html_content += \"<hr>\"\n",
    "    html_content += \"</div>\"\n",
    "    return html_content\n",
    "\n",
    "def user_download_timeline(timeline, article_id):\n",
    "    timeline_export = {\"Article_id\": article_id, \"Timeline\": timeline}\n",
    "    return json.dumps(timeline_export, indent=4)\n",
    "\n",
    "def display_gradio():\n",
    "    with gr.Blocks(title=\"Article Timeline Generator\", theme='snehilsanyal/scikit-learn') as gradio_timeline:\n",
    "        gr.Markdown(\"\"\"\n",
    "            <h1 style='text-align: center;'>\n",
    "            Timeline Generator\n",
    "            </h1>\n",
    "            <hr>\n",
    "            <h3>\n",
    "            Upload the JSON database and choose an article index to generate a timeline.\n",
    "            </h3>\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    input_test_db = gr.File(label=\"Upload the JSON Database\")\n",
    "                    input_test_index = gr.Number(label=\"Test Article Index. Choose an index from 1-8 (Number of test articles)\", value=0)\n",
    "                    hidden_article_id = gr.Textbox(visible=False)\n",
    "\n",
    "                    with gr.Row():\n",
    "                        clear_button = gr.Button(\"Reset index\")\n",
    "                        generate_button = gr.Button(\"Generate Timeline\")\n",
    "                    output_timeline = gr.JSON(label=\"Generated Timeline in JSON format\")\n",
    "                    gr.Markdown('''\n",
    "                                If error message not shon past the 7 second mark, Timeline is necessary for the chosen article. \n",
    "                                ''')\n",
    "                    output_error = gr.Textbox(label=\"Error Message:\", visible=False)  # Initialize as not visible\n",
    "                \n",
    "                with gr.Column():\n",
    "                    show_timeline_button = gr.Button(\"Show Generated Timeline\")\n",
    "                    output_timeline_HTML = gr.HTML()\n",
    "                    download_button = gr.DownloadButton(\"Download Full Timeline\", visible=True)\n",
    "        \n",
    "        clear_button.click(lambda: 0, None, input_test_index)\n",
    "        output_error.visible = True\n",
    "        \n",
    "        def handle_generate_timeline(test_articles_json, index):\n",
    "                result = gradio_generate_timeline(test_articles_json, index)\n",
    "                article_id = result['Article_id']\n",
    "                if \"error\" in result:\n",
    "                    timeline_error = result[\"error\"]\n",
    "                    return timeline_error, None, article_id, \"\"\n",
    "                else:\n",
    "                    timeline = result['Timeline']\n",
    "                    return \"NIL\", timeline, article_id, \"\"\n",
    "\n",
    "        generate_button.click(\n",
    "                handle_generate_timeline,\n",
    "                inputs=[input_test_db, input_test_index],\n",
    "                outputs=[output_error, output_timeline, hidden_article_id]\n",
    "            )\n",
    "\n",
    "        show_timeline_button.click(\n",
    "                display_timeline,\n",
    "                inputs=output_timeline,\n",
    "                outputs=output_timeline_HTML\n",
    "            )\n",
    "        download_button.click(\n",
    "            user_download_timeline,\n",
    "            inputs=[output_timeline, hidden_article_id],\n",
    "            outputs=download_button  # This provides the file for download\n",
    "        )\n",
    "        \n",
    "    gradio_timeline.launch(inbrowser=True, debug=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Database loaded\n",
      "\n",
      "Test Database loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 4\n",
    "def load_test_articles():\n",
    "        with open(\"../data/test_data/test.json\", \"r\", encoding='utf-8') as fin:\n",
    "            test_database = json.load(fin)\n",
    "            print(\"Test Database loaded\\n\")\n",
    "        return test_database\n",
    "def load_train_articles():\n",
    "        with open(\"../data/test_data/train.json\", \"r\", encoding='utf-8') as fin:\n",
    "            train_database = json.load(fin)\n",
    "            print(\"Test Database loaded\\n\")\n",
    "        return train_database\n",
    "    \n",
    "train_database = load_train_articles()\n",
    "test_database = load_test_articles()\n",
    "\n",
    "# Select the test article based on the given index\n",
    "test_article = test_database[index-1]\n",
    "\n",
    "df_train = pd.DataFrame(train_database)\n",
    "df_test = pd.DataFrame([test_article])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing embedding data and scaling data...\n",
      "\n",
      "Finding best Model parameters...\n",
      "\n",
      "Best variance for this dataset is 0.92 and the best maximum distance is 54\n",
      "\n",
      "Training new Hierarchical Clustering model with best variance: 0.92 and max_d: 54\n",
      "\n",
      "Cluster Label 122 is chosen\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Test Artice Chosen: (Title: Australia pick up the pace before India showdown\\nTags: ['Australia', 'India', 'Cricket', 'World Cup Semifinals', 'Pace Attack', 'Mitchell Starc']):\\nArticle id: st_1162194, Title: India focused on World Cup progress and not history ahead of semis, Tags: ['India', 'World Cup', 'Semifinals', 'New Zealand', 'Pressure Management', 'History']]\\nArticle id: st_1152341, Title: Winning World Cup in India would trump 2019 title: Morgan, Tags: ['England', 'Cricket', 'World Cup', 'India', 'Challenge', 'Preparation']]\\nArticle id: st_1162906, Title: Cricket-World Cup 2023 final: India v Australia, Tags: ['Cricket', 'ICC World Cup', 'India', 'Australia', 'Tournament', 'Final']]\\nArticle id: st_1163459, Title: Australia underline big-match credentials with stunning India takedown, Tags: ['Australia', 'Cricket', 'World Cup', 'India', 'Travis Head', 'Pat Cummins']]\\nArticle id: st_1162908, Title: Resurgent Australia stand between unblemished India and World Cup glory, Tags: ['India', 'Australia', 'World Cup Final', 'Cricket', 'Ahmedabad', 'Narendra Modi Stadium']]\\nArticle id: st_1152128, Title: With licence to thrill, England look to deny India home World Cup win, Tags: ['Cricket', 'World Cup', 'England', 'India', 'Australia', 'New Zealand']]\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Processing embedding data and scaling data...\\n\")\n",
    "# Deserializing the embeddings\n",
    "body_embeddings_train = np.array(df_train['embeddings'].apply(ast.literal_eval).tolist())\n",
    "title_embeddings_train = np.array(df_train['Title_embeddings'].apply(ast.literal_eval).tolist())\n",
    "tags_embeddings_train = np.array(df_train['tags_embeddings'].apply(ast.literal_eval).tolist())\n",
    "\n",
    "body_embeddings_test = np.array(df_test['embeddings'].apply(ast.literal_eval).tolist())\n",
    "title_embeddings_test = np.array(df_test['Title_embeddings'].apply(ast.literal_eval).tolist())\n",
    "tags_embeddings_test = np.array(df_test['tags_embeddings'].apply(ast.literal_eval).tolist())\n",
    "\n",
    "# body_embeddings_train = np.array(df_train['Body_embeddings'].apply(ast.literal_eval).tolist())\n",
    "# title_embeddings_train = np.array(df_train['Title_embeddings'].apply(ast.literal_eval).tolist())\n",
    "# tags_embeddings_train = np.array(df_train['MPNET_tags_embeddings'].apply(ast.literal_eval).tolist())\n",
    "\n",
    "# body_embeddings_test = np.array(df_test['Body_embeddings'].apply(ast.literal_eval).tolist())\n",
    "# title_embeddings_test = np.array(df_test['Title_embeddings'].apply(ast.literal_eval).tolist())\n",
    "# tags_embeddings_test = np.array(df_test['MPNET_tags_embeddings'].apply(ast.literal_eval).tolist())\n",
    "\n",
    "# Combine embeddings\n",
    "all_embeddings_train = np.concatenate((body_embeddings_train, title_embeddings_train, tags_embeddings_train), axis=1)\n",
    "all_embeddings_test = np.concatenate((body_embeddings_test, title_embeddings_test, tags_embeddings_test), axis=1)\n",
    "\n",
    "# Standardize embeddings\n",
    "scaler = StandardScaler()\n",
    "train_embeddings = scaler.fit_transform(all_embeddings_train)\n",
    "test_embeddings = scaler.transform(all_embeddings_test)\n",
    "#return train_embeddings,  test_embeddings\n",
    "\n",
    "# def get_variance_performance(train_embeddings):\n",
    "# Experiment for this variance range of 92% to 95%\n",
    "print(\"Finding best Model parameters...\\n\")\n",
    "variance_range = list(np.arange(0.92, 0.95, 0.01))\n",
    "variance_dic = {}\n",
    "\n",
    "for variance in variance_range:\n",
    "        pca = PCA(n_components=variance)\n",
    "        train_pca_embeddings = pca.fit_transform(train_embeddings)\n",
    "        \n",
    "        # Range of max_d values to try, for this dataset we use 65\n",
    "        max_d_values = np.arange(45, 65)\n",
    "        \n",
    "        # List to store silhouette scores\n",
    "        silhouette_scores_train = []\n",
    "\n",
    "        # Perform hierarchical clustering\n",
    "        Z = linkage(train_pca_embeddings, method='ward')\n",
    "\n",
    "        for max_d in max_d_values:\n",
    "            clusters_train = fcluster(Z, max_d, criterion='distance')\n",
    "            \n",
    "            # Calculate silhouette score only if there are at least 2 unique clusters and fewer than the number of samples\n",
    "            if 1 < len(set(clusters_train)) < len(train_pca_embeddings):\n",
    "                score_train = silhouette_score(train_pca_embeddings, clusters_train)\n",
    "            else:\n",
    "                score_train = -1  # Assign a score of -1 if less than 2 unique clusters or too many clusters\n",
    "            \n",
    "            silhouette_scores_train.append(score_train)\n",
    "\n",
    "        # Determine the best max_d\n",
    "        best_max_d_train = max_d_values[np.argmax(silhouette_scores_train)]\n",
    "        variance_dic[variance] = {\n",
    "            'max_d_train': best_max_d_train,\n",
    "            'best_train_silhouette': max(silhouette_scores_train)\n",
    "        }\n",
    "# return variance_dic\n",
    "perf_results = variance_dic\n",
    "# def get_best_variance(perf_results):\n",
    "highest_train_sil = 0\n",
    "best_variance_s = []\n",
    "for variance, scores in perf_results.items():\n",
    "        if scores['best_train_silhouette'] > highest_train_sil:\n",
    "            highest_train_sil = scores['best_train_silhouette']\n",
    "            best_variance_s = [variance]  \n",
    "        elif scores['best_train_silhouette'] == highest_train_sil:\n",
    "            best_variance_s.append(variance)  \n",
    "    \n",
    "final_best_max_d = perf_results[best_variance_s[0]]['max_d_train']\n",
    "print(f\"Best variance for this dataset is {round(best_variance_s[0], 2)} and the best maximum distance is {final_best_max_d}\\n\")\n",
    "# return round(best_variance_s[0], 2), final_best_max_d\n",
    "\n",
    "# def predict_cluster(test_embedding, train_embeddings, clusters):\n",
    "distances = np.linalg.norm(train_embeddings - test_embeddings, axis=1)\n",
    "# return clusters[np.argmin(distances)]\n",
    "best_variance, best_max_d = round(best_variance_s[0], 2), final_best_max_d\n",
    "# def get_cluster_labels(best_variance, best_max_d, train_embeddings, test_embeddings, df_train, df_test):\n",
    "# Perform PCA\n",
    "print(f\"Training new Hierarchical Clustering model with best variance: {best_variance} and max_d: {best_max_d}\\n\")\n",
    "pca = PCA(n_components=best_variance)\n",
    "pca_train_embeddings = pca.fit_transform(train_embeddings)\n",
    "pca_test_embeddings = pca.transform(test_embeddings)\n",
    "\n",
    "Z = linkage(pca_train_embeddings, method='ward', metric='euclidean')\n",
    "clusters_train = fcluster(Z, best_max_d, criterion='distance')\n",
    "# Predict clusters for test data using the nearest cluster center\n",
    "\n",
    "test_clusters = [predict_cluster(te, pca_train_embeddings, clusters_train) for te in pca_test_embeddings]\n",
    "\n",
    "df_train['Cluster_labels'] = clusters_train\n",
    "df_test['Cluster_labels'] = test_clusters\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "cluster_dict = {}\n",
    "\n",
    "# Populate the dictionary with cluster contents for each test point\n",
    "for i, (test_point, test_cluster) in enumerate(zip(df_test.itertuples(), test_clusters)):\n",
    "        cluster_contents = []\n",
    "        \n",
    "        cluster_indices = np.where(clusters_train == test_cluster)[0]\n",
    "        cluster_df = df_train.iloc[cluster_indices]\n",
    "        \n",
    "        cluster_dict = {\n",
    "            \"Test point\": {'id': test_point.st_id,\n",
    "                        \"Title\": test_point.Title, \n",
    "                        \"Tags\": test_point.tags},\n",
    "            \"Cluster\": test_cluster,\n",
    "            \"Cluster contents\": cluster_contents\n",
    "        }\n",
    "        \n",
    "        for _, row in cluster_df.iterrows():\n",
    "            cluster_contents.append({\"id\": row['st_id'], \n",
    "                                    \"Title\": row['Title'],\n",
    "                                    \"Tags\": row['tags'], \n",
    "                                    })\n",
    "\n",
    "print(f\"Cluster Label {test_cluster} is chosen\\n\")\n",
    "input_list = \"\"\n",
    "input_list += f\"Test Artice Chosen: (Title: {cluster_dict['Test point']['Title']}\\nTags: {cluster_dict['Test point']['Tags']}):\\n\"\n",
    "for _, row in cluster_df.iterrows():\n",
    "        input_list += f\"Article id: {row['st_id']}, Title: {row['Title']}, Tags: {row['tags']}]\\n\"\n",
    "# return input_list, df_train, df_test\n",
    "input_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "502\n"
     ]
    }
   ],
   "source": [
    "current = df_train.drop('_id', axis=1)\n",
    "print(len(current))\n",
    "final_df = df_train.drop_duplicates(subset='Text', keep='last')\n",
    "print(len(final_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Timeline Generation\n",
      "\n",
      "Fetching article data from MongoDB...\n",
      "\n",
      "Data successfully fetched from MongoDB\n",
      "\n",
      "Test Database loaded\n",
      "\n",
      "Evaluating necessity of Timeline for this aricle.\n",
      "\n",
      "Timeline is necessary for this chosen article.\n",
      "\n",
      "Processing embedding data and scaling data...\n",
      "\n",
      "Finding best Model parameters...\n",
      "\n",
      "Best variance for this dataset is 0.94 and the best maximum distance is 45\n",
      "\n",
      "Training new Hierarchical Clustering model with best variance: 0.94 and max_d: 45\n",
      "\n",
      "Cluster Label 356 is chosen\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Test Article Title: << Not a short-lived nuisance: Some clarity about the haze  >>\n",
      "\n",
      "Supporting Article Titles:\n",
      " - Not a short-lived nuisance: Some clarity about the haze  \n",
      " - Not a short-lived nuisance: Some clarity about the haze  \n",
      " - Not a short-lived nuisance: Some clarity about the haze  \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first timeline has been generated\n",
      "\n",
      "\n",
      "Proceeding to enhance the timeline...\n",
      "\n",
      "Processing batches simultaneously now...\n",
      "\n",
      "Finished enhancing the timeline\n",
      "\n",
      "Data successfully saved to MongoDB\n"
     ]
    }
   ],
   "source": [
    "display_gradio()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
